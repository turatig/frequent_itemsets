{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "frequent_itemsets.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/turatig/frequent_itemsets/blob/master/frequent_itemsets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J00vwdMle-Nx"
      },
      "source": [
        "**MARKET BASKET ANALYSIS NOTEBOOK**"
      ],
      "id": "J00vwdMle-Nx"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOVdeW-efZ3z"
      },
      "source": [
        "Dowload and preprocess dataset"
      ],
      "id": "LOVdeW-efZ3z"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22ba4125",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3345c20f-90d8-405e-bdd3-43966b83ba6b"
      },
      "source": [
        "!pip install kaggle\n",
        "!pip install pandas\n",
        "\n",
        "import os,sys,time,zipfile,json,re\n",
        "import functools as ft\n",
        "import itertools as it\n",
        "from datetime import datetime as dt\n",
        "\n",
        "os.environ['KAGGLE_USERNAME']='giacomoturati1'\n",
        "os.environ['KAGGLE_KEY']='7d34a1aefc3558065164b70c24ce27ed'\n",
        "\n",
        "from kaggle.api.kaggle_api_extended import KaggleApi\n",
        "\n",
        "def get_dataset():\n",
        "\n",
        "  #execute only if the dataset was not already downloaded\n",
        "  if 'old-newspaper.tsv' not in os.listdir():\n",
        "    api=KaggleApi()\n",
        "    api.authenticate()\n",
        "\n",
        "    api.dataset_download_file('alvations/old-newspapers','old-newspaper.tsv')\n",
        "\n",
        "    with zipfile.ZipFile('old-newspaper.tsv.zip','r') as _zip:\n",
        "      _zip.extractall()\n",
        "\n",
        "#languages: subset of languages to be considered during the market-basket analysis\n",
        "def read_dataset_from_disk(languages=None,max_basket=None):\n",
        "  count=0\n",
        "  with open('old-newspaper.tsv','r') as f:\n",
        "    #skip header line\n",
        "    next(f)\n",
        "    for line in f:\n",
        "      l=line.split('\\t')\n",
        "      if languages is not None and l[0] not in languages: continue\n",
        "\n",
        "      #get a list of words as basket skipping any sequence of non-alphabetical characters\n",
        "      basket=re.split(r'[^a-zA-Z]+',l[3])\n",
        "      #remove any empty string\n",
        "      basket=[word.lower() for word in basket if word!='']\n",
        "      count+=1\n",
        "      yield basket\n",
        "\n",
        "      if max_basket is not None and count>=max_basket: break\n",
        "      \n",
        "    f.close()\n",
        "\n",
        "#create json wich contains an array of baskets (lists of words)\n",
        "def create_test_json_dataset(languages=None,max_basket=None):\n",
        "  baskets=[]\n",
        "\n",
        "  #execute only if the dataset was not already created\n",
        "  for line in read_dataset_from_disk(languages,max_basket):\n",
        "    baskets.append(line)\n",
        "\n",
        "  filename=ft.reduce(lambda i,j:i+'_'+j,languages).lower() if languages is not None else 'all_languages' \n",
        "  filename+=str(len(baskets))+'.json'\n",
        "\n",
        "  with open(filename,'w') as f:\n",
        "    f.write(json.dumps(baskets,indent='\\t'))\n",
        "    f.close()\n",
        "  \n",
        "  \n",
        "\n",
        "#yield lists of words (baskets) from json file created with create_test_json_dataset \n",
        "def iter_baskets_from_json(filename):\n",
        "  theres_next=True\n",
        "  basket=[]\n",
        "\n",
        "  with open(filename,'r') as f:\n",
        "    #skip first square braket\n",
        "    next(f)\n",
        "    for line in f:\n",
        "\n",
        "      m=re.search(r'[a-zA-Z]+|\\[|\\]',line)\n",
        "      line=line[m.start():m.end()]\n",
        "\n",
        "      if line=='[': \n",
        "        basket=[]\n",
        "        theres_next=True\n",
        "\n",
        "      elif line==']':\n",
        "        if theres_next:\n",
        "          theres_next=False\n",
        "          yield basket\n",
        "\n",
        "      else: basket.append(line)\n",
        "    \n",
        "    f.close()\n",
        "    \n",
        "\n",
        "\n",
        "get_dataset()\n",
        "create_test_json_dataset(['Italian'],300)\n",
        "\n",
        "\n"
      ],
      "id": "22ba4125",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.12)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (5.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2020.12.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.41.1)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (2.10)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.1.5)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from pandas) (1.19.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ixo8_SLkdPOd"
      },
      "source": [
        "A-priori algorithm implementation"
      ],
      "id": "Ixo8_SLkdPOd"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-0BQUe4z29F"
      },
      "source": [
        "#decorator to log time execution of function/method\n",
        "def timeit(f):\n",
        "  def _wrap(*args,**kwargs):\n",
        "    start=time.time()\n",
        "    res=f(*args,**kwargs)\n",
        "    stop=time.time()\n",
        "    print('Function {0} executed in {1} seconds'.format(f.__name__,stop-start))\n",
        "    return res\n",
        "  return _wrap\n",
        "\n",
        "def sizeof_GB(obj): return \"%f\"%(sys.getsizeof(obj)/1000000000)\n",
        "\n",
        "class Apriori:\n",
        "\n",
        "  #it: iterable of baskets\n",
        "  def __init__(self,it,*it_args,**it_kwargs):\n",
        "\n",
        "    #basket file is supposed to be one of the previously defined iterables\n",
        "    self._basket_file_it=it\n",
        "    self._basket_file_args=it_args\n",
        "    self._basket_file_kwargs=it_kwargs\n",
        "    #support threshold: this value is set the first time the basket file is passed\n",
        "    self._s=0\n",
        "    #the k-1-th element is the set of frequent itemsets made of k elements\n",
        "    self._frequent_itemsets=[]\n",
        "    self._log=True\n",
        "  \n",
        "  #reset support threshold and frequent itemsets list and compute frequent singletons\n",
        "  @timeit\n",
        "  def _algo_init(self,s_thresh=None):\n",
        "\n",
        "    self._frequent_itemsets=[]\n",
        "    singletons=dict()\n",
        "    baskets_count=0\n",
        "\n",
        "    for basket in self._basket_file_it(*self._basket_file_args,**self._basket_file_kwargs):\n",
        "      baskets_count+=1\n",
        "      for word in basket:\n",
        "        if (word,) not in singletons.keys(): singletons[(word,)]=1\n",
        "        else: singletons[(word,)]+=1\n",
        "\n",
        "    if s_thresh is None:\n",
        "      #set threshold to 1% of the number of baskets analyzed\n",
        "      self._s=baskets_count//100\n",
        "    else: self._s=s_thresh\n",
        "    \n",
        "    self._frequent_itemsets.append(self._filter_ck(singletons))\n",
        "\n",
        "    if self._log:\n",
        "      print('Total number of singletons: {0}\\nSize in GB: {1}'.\\\n",
        "            format(len(singletons),sizeof_GB(singletons)))\n",
        "\n",
        "  #build candidate sets and count their occurence in baskets\n",
        "  @timeit  \n",
        "  def _get_ck(self,k):\n",
        "\n",
        "    candidates=dict()\n",
        "    for basket in self._basket_file_it(*self._basket_file_args,**self._basket_file_kwargs):\n",
        "      #filter unfrequent singletons\n",
        "      basket=[word for word in basket if (word,) in self._frequent_itemsets[0]]\n",
        "      for kuple in it.combinations(basket,r=k):\n",
        "        kuple=tuple(sorted(kuple))\n",
        "        #exploit the monotonicity property: kuple is a possible itemset -> all immediate subsets are frequent itemsets.\n",
        "        if all([tuple(sorted(el)) in self._frequent_itemsets[k-2] for el in it.combinations(kuple,r=k-1)]):\n",
        "          if kuple not in candidates.keys(): candidates[kuple]=1\n",
        "          else: candidates[kuple]+=1\n",
        "    \n",
        "    if self._log:\n",
        "      print('Total number of candidate {0}-itemsets: {1}\\nSize in GB: {2}'.\\\n",
        "            format(k,len(candidates),sizeof_GB(candidates)))\n",
        "\n",
        "    return candidates\n",
        "\n",
        "  @timeit\n",
        "  def _filter_ck(self,candidates):\n",
        "    return {k:v for k,v in candidates.items() if v>=self._s}\n",
        "\n",
        "\n",
        "\n",
        "  def compute(self,max_k=None,s_thresh=None):\n",
        "    self._algo_init(s_thresh)\n",
        "    k=2\n",
        "\n",
        "    if self._log:\n",
        "      print('Number of frequent singletons: {0}\\nSize in GB: {1}'.\\\n",
        "            format(len(self._frequent_itemsets[-1]),sizeof_GB(self._frequent_itemsets[-1])))\n",
        "      print('-'*30)\n",
        "      print('\\n')\n",
        "\n",
        "    #stop when no more frequent itemsets is found or k>max_k\n",
        "    while self._frequent_itemsets[-1] and k<=max_k:\n",
        "      self._frequent_itemsets.append(self._filter_ck(self._get_ck(k)))\n",
        "      k+=1\n",
        "\n",
        "      if self._log:\n",
        "        print('Number of frequent {0}-itemsets: {1}\\nSize in GB: {2}'.\\\n",
        "              format(k-1,len(self._frequent_itemsets[-1]),sizeof_GB(self._frequent_itemsets[-1])))\n",
        "        print('-'*30)\n",
        "        print('\\n')\n",
        "    \n",
        "    if not self._frequent_itemsets[:-1]:\n",
        "      #remove the last element if empty\n",
        "      self._frequent_itemsets=self._frequent_itemsets[:-1]\n",
        "\n",
        "  def dump_result(self):\n",
        "    def remap(dic):\n",
        "      return {str(k):v for k,v in dic.items()}\n",
        "\n",
        "    header_info={'iterable':self._basket_file_it.__name__,'iterable_args':self._basket_file_args,\\\n",
        "                 'iterable_kwargs':self._basket_file_kwargs,'support_threshold':self._s}\n",
        "\n",
        "    filename='apriori_market_basket_analysis_'+str(dt.today())[:10]+'_'+str(dt.today())[11:]+'.json'\n",
        "    with open(filename,'w') as f:\n",
        "      f.write(json.dumps([header_info]+[remap(dic) for dic in self._frequent_itemsets],indent='\\t'))\n",
        "      f.close()\n",
        "\n",
        "\n"
      ],
      "id": "P-0BQUe4z29F",
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mtL-veBk1moc",
        "outputId": "f2fc3ce0-6edf-40fa-906b-bfff5949c2bb"
      },
      "source": [
        "algo=Apriori(iter_baskets_from_json,'italian300.json')\n",
        "algo.compute(3)\n",
        "algo.dump_result()\n"
      ],
      "id": "mtL-veBk1moc",
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Function _filter_ck executed in 0.0007615089416503906 seconds\n",
            "Total number of singletons: 6439\n",
            "Size in GB: 0.000295\n",
            "Function _algo_init executed in 0.04676508903503418 seconds\n",
            "Number of frequent singletons: 1140\n",
            "Size in GB: 0.000037\n",
            "------------------------------\n",
            "\n",
            "\n",
            "Total number of candidate 2-itemsets: 118747\n",
            "Size in GB: 0.005243\n",
            "Function _get_ck executed in 1.5234949588775635 seconds\n",
            "Function _filter_ck executed in 0.027146100997924805 seconds\n",
            "Number of frequent 2-itemsets: 37906\n",
            "Size in GB: 0.001311\n",
            "------------------------------\n",
            "\n",
            "\n",
            "Total number of candidate 3-itemsets: 713202\n",
            "Size in GB: 0.041943\n",
            "Function _get_ck executed in 57.88164567947388 seconds\n",
            "Function _filter_ck executed in 0.23906159400939941 seconds\n",
            "Number of frequent 3-itemsets: 621306\n",
            "Size in GB: 0.020972\n",
            "------------------------------\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}