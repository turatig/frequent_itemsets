{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "frequent_itemsets.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/turatig/frequent_itemsets/blob/master/frequent_itemsets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J00vwdMle-Nx"
      },
      "source": [
        "**MARKET BASKET ANALYSIS NOTEBOOK**"
      ],
      "id": "J00vwdMle-Nx"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOVdeW-efZ3z"
      },
      "source": [
        "Dowload and preprocess dataset"
      ],
      "id": "LOVdeW-efZ3z"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22ba4125",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41e0d4d0-1b32-42fe-f796-f7a71a8ed6a6"
      },
      "source": [
        "!pip install kaggle\n",
        "\n",
        "\n",
        "import os,sys,time,zipfile,json,re\n",
        "import functools as ft\n",
        "import itertools as it\n",
        "from datetime import datetime as dt\n",
        "\n",
        "os.environ['KAGGLE_USERNAME']='giacomoturati1'\n",
        "os.environ['KAGGLE_KEY']='7d34a1aefc3558065164b70c24ce27ed'\n",
        "\n",
        "from kaggle.api.kaggle_api_extended import KaggleApi\n",
        "\n",
        "def get_dataset():\n",
        "\n",
        "  #execute only if the dataset was not already downloaded\n",
        "  if 'old-newspaper.tsv' not in os.listdir():\n",
        "    api=KaggleApi()\n",
        "    api.authenticate()\n",
        "\n",
        "    api.dataset_download_file('alvations/old-newspapers','old-newspaper.tsv')\n",
        "\n",
        "    with zipfile.ZipFile('old-newspaper.tsv.zip','r') as _zip:\n",
        "      _zip.extractall()\n",
        "\n",
        "#Yield baskets (list of lists) reading from tsv\n",
        "#languages: subset of languages to be considered during the market-basket analysis\n",
        "def read_dataset_from_tsv(languages=None,max_basket=-1):\n",
        "  count=0\n",
        "  with open('old-newspaper.tsv','r') as f:\n",
        "    #skip header line\n",
        "    next(f)\n",
        "    for line in f:\n",
        "      l=line.split('\\t')\n",
        "      if languages is not None and l[0] not in languages: continue\n",
        "\n",
        "      #get a list of words as basket skipping any sequence of non-alphabetical characters\n",
        "      basket=re.split(r'[^a-zA-Z]+',l[3])\n",
        "      #remove any empty string\n",
        "      basket=[word.lower() for word in basket if word!='']\n",
        "      count+=1\n",
        "      yield basket\n",
        "\n",
        "      if max_basket>0 and count>=max_basket: break\n",
        "      \n",
        "    f.close()\n",
        "\n",
        "#Create json wich contains an array of baskets (lists of words)\n",
        "def create_test_json_dataset(languages=None,max_basket=None):\n",
        "  baskets=[]\n",
        "\n",
        "  #execute only if the dataset was not already created\n",
        "  for line in read_dataset_from_tsv(languages,max_basket):\n",
        "    baskets.append(line)\n",
        "\n",
        "  filename=ft.reduce(lambda i,j:i+'_'+j,languages).lower() if languages is not None else 'all_languages' \n",
        "  filename+=str(len(baskets))+'.json'\n",
        "\n",
        "  with open(filename,'w') as f:\n",
        "    f.write(json.dumps(baskets,indent='\\t'))\n",
        "    f.close()\n",
        "  \n",
        "  \n",
        "\n",
        "#Yield baskets from json structures as array of arrays\n",
        "def iter_baskets_from_json(filename,max_basket=-1):\n",
        "  theres_next=True\n",
        "  basket=[]\n",
        "  count=0\n",
        "\n",
        "  with open(filename,'r') as f:\n",
        "    #skip first square braket\n",
        "    next(f)\n",
        "    for line in f:\n",
        "\n",
        "      m=re.search(r'[a-zA-Z]+|\\[|\\]',line)\n",
        "      line=line[m.start():m.end()]\n",
        "\n",
        "      if line=='[': \n",
        "        basket=[]\n",
        "        theres_next=True\n",
        "\n",
        "      elif line==']':\n",
        "        if theres_next:\n",
        "          theres_next=False\n",
        "          yield basket\n",
        "          count+=1\n",
        "          if max_basket>0 and count>=max_basket:break\n",
        "\n",
        "      else: basket.append(line)\n",
        "    \n",
        "    f.close()\n",
        "\n",
        "#Just to iterate agnostically over basket file\n",
        "def iter_baskets(basket_file,max_basket=-1):\n",
        "  if callable(basket_file):\n",
        "    for basket in basket_file(max_basket): yield basket\n",
        "  else:\n",
        "    count=0\n",
        "    for basket in basket_file:\n",
        "      yield basket\n",
        "      count+=1\n",
        "      if count>=max_basket: break \n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "get_dataset()\n",
        "create_test_json_dataset(['Italian'],300)\n",
        "\n",
        "\n"
      ],
      "id": "22ba4125",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.12)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (5.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2020.12.5)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.1)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.41.1)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (2.10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TqnHG1Eppe9F"
      },
      "source": [
        "Utilities to log the execution"
      ],
      "id": "TqnHG1Eppe9F"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDz4lLPYfJMc"
      },
      "source": [
        "def sizeof_GB(obj): return \"%f\"%(sys.getsizeof(obj)/1000000000)\n",
        "\n",
        "#decorator to log time execution of function/method\n",
        "def time_it(f):\n",
        "  def _wrap(*args,**kwargs):\n",
        "    start=time.time()\n",
        "    res=f(*args,**kwargs)\n",
        "    stop=time.time()\n",
        "    \n",
        "    print('\\n'+'-'*30)\n",
        "    print('Function {0} executed in {1} seconds'.format(f.__name__,stop-start))\n",
        "    print('-'*30+'\\n')\n",
        "    return res\n",
        "  return _wrap\n",
        "\n",
        "#decorator to log the memory space used before and after a candidate itemset filtering operation\n",
        "def log_filter(f):\n",
        "  def _wrap(*args,**kwargs):\n",
        "    if len(args)>1:\n",
        "      print('Total number of candidate {0}-itemsets: {1}\\nSize in GB: {2}'.\\\n",
        "            format(len(list(args[1].keys())[0]),len(args[1]),sizeof_GB(args[1])))\n",
        "      \n",
        "    #argument was given by key=value\n",
        "    else:\n",
        "      print('Total number of candidate {0}-itemsets: {1}\\nSize in GB: {2}'.\\\n",
        "            format(len(list(kwargs['candidates'].keys())[0]),len(kwargs['candidates']),sizeof_GB(kwargs['candidates'])))\n",
        "    \n",
        "    res=f(*args,**kwargs)\n",
        "\n",
        "    if res:\n",
        "      print('Number of frequent {0}-itemsets: {1}\\nSize in GB: {2}'.\\\n",
        "              format(len(list(res.keys())[0]),len(res),sizeof_GB(res)))\n",
        "    else:\n",
        "      print('Number of frequent {0}-itemsets: {1}\\nSize in GB: {2}'.\\\n",
        "              format(0,0,0))\n",
        "    print('-'*30+'\\n')\n",
        "    return res\n",
        "\n",
        "  return _wrap"
      ],
      "id": "wDz4lLPYfJMc",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ixo8_SLkdPOd"
      },
      "source": [
        "A-priori algorithm implementation"
      ],
      "id": "Ixo8_SLkdPOd"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-0BQUe4z29F"
      },
      "source": [
        "class Apriori:\n",
        "\n",
        "  \"\"\"\n",
        "    basket_file_it: iterable of baskets\n",
        "    s:support_threshold\n",
        "    max_basket:max_number of baskets to be analyzed\n",
        "    basket_count: total number of baskets\n",
        "    frequent_itemsets:the k-1-th element is the set of frequent itemsets made of k elements\n",
        "  \"\"\"\n",
        "  def __init__(self,basket_file_it,s=0,max_basket=-1):\n",
        "    self._basket_file_it=basket_file_it\n",
        "    self._s=s\n",
        "    self._max_basket=max_basket\n",
        "    self._basket_count=0\n",
        "    self._frequent_itemsets=[]\n",
        "  \n",
        "  \"\"\"\n",
        "  Private:\n",
        "    Increment singletons set counter\n",
        "  \"\"\"\n",
        "  def _up_singles_count(self,basket):\n",
        "    for word in basket:\n",
        "        if (word,) not in self._frequent_itemsets[0].keys(): self._frequent_itemsets[0][(word,)]=1\n",
        "        else: self._frequent_itemsets[0][(word,)]+=1\n",
        "  \n",
        "  \"\"\"\n",
        "    First pass: initialize frequent singletons set \n",
        "  \"\"\"\n",
        "  @time_it\n",
        "  def _algo_init(self):\n",
        "    self._frequent_itemsets=[dict()]\n",
        "    baskets_count=0\n",
        "\n",
        "    for basket in iter_baskets(self._basket_file_it,self._max_basket):\n",
        "      baskets_count+=1\n",
        "      self._up_singles_count(basket)\n",
        "\n",
        "    self._basket_count=baskets_count\n",
        "    #set the threshold to 1% of the basket count\n",
        "    if self._s<=0: self._s=self._basket_count//100\n",
        "    self._frequent_itemsets[0]=self._filter_ck(self._frequent_itemsets[0])\n",
        "\n",
        "  \"\"\"\n",
        "    Check monotonicity property\n",
        "    kuple is a possible itemset -> all immediate subsets are frequent itemsets.\n",
        "  \"\"\" \n",
        "  def _check_mono_prop(self,kuple,k):\n",
        "    return all([tuple(sorted(el)) in self._frequent_itemsets[k-2] for el in it.combinations(kuple,r=k-1)])\n",
        "\n",
        "  \"\"\"\n",
        "    Return candidate itemsets during an iteration step\n",
        "  \"\"\"\n",
        "  @time_it  \n",
        "  def _get_ck(self,k):\n",
        "\n",
        "    candidates=dict()\n",
        "    for basket in iter_baskets(self._basket_file_it,self._max_basket):\n",
        "      #filter unfrequent singletons\n",
        "      basket=[word for word in basket if (word,) in self._frequent_itemsets[0]]\n",
        "\n",
        "      for kuple in it.combinations(basket,r=k):\n",
        "        #sort tuple in order to avoid duplication of the same itemset considered in different order\n",
        "        kuple=tuple(sorted(kuple))\n",
        "\n",
        "        if self._check_mono_prop(kuple,k):\n",
        "          if kuple not in candidates.keys(): candidates[kuple]=1\n",
        "          else: candidates[kuple]+=1\n",
        "\n",
        "    return candidates\n",
        "\n",
        "  \"\"\"\n",
        "    Filter candidate set of itemsets according to suppport threshold \n",
        "  \"\"\"\n",
        "  @log_filter\n",
        "  @time_it\n",
        "  def _filter_ck(self,candidates):\n",
        "    return {k:v for k,v in candidates.items() if v>=self._s}\n",
        "\n",
        "  \"\"\"\n",
        "  Public:\n",
        "    Compute the algorithm\n",
        "  \"\"\"\n",
        "  def compute(self,max_k=-1):\n",
        "    self._algo_init()\n",
        "    k=2\n",
        "\n",
        "    #stop when no more frequent itemsets are found or k>max_k\n",
        "    while self._frequent_itemsets[-1] and (max_k<0 or k<=max_k):\n",
        "      self._frequent_itemsets.append(self._filter_ck(self._get_ck(k)))\n",
        "      k+=1\n",
        "    \n",
        "    if not self._frequent_itemsets[:-1]:\n",
        "      #remove the last element if empty\n",
        "      self._frequent_itemsets=self._frequent_itemsets[:-1]\n",
        "\n",
        "  \"\"\"\n",
        "    Dump the current state of the algorithm on json file\n",
        "  \"\"\"\n",
        "  def dump_result(self):\n",
        "    def remap(dic):\n",
        "      return {str(k):v for k,v in dic.items()}\n",
        "\n",
        "    header_info={'iterable':str(self._basket_file_it),'support_threshold':self._s,'total_n_baskets':self._basket_count}\n",
        "\n",
        "    filename='{0}_market_basket_analysis_'.format(self.__class__.__name__)\\\n",
        "              +str(dt.today())[:10]+'_'+str(dt.today())[11:]+'.json'\n",
        "              \n",
        "    with open(filename,'w') as f:\n",
        "      f.write(json.dumps([header_info]+[remap(dic) for dic in self._frequent_itemsets],indent='\\t'))\n",
        "      f.close()\n",
        "    \n",
        "  \"\"\"\n",
        "    Functional interface to compute the Apriori algorithm\n",
        "  \"\"\"\n",
        "  @staticmethod\n",
        "  def mb_analysis(basket_file_it,s=0,max_basket=-1,max_k=-1,log=False):\n",
        "    algo=Apriori(basket_file_it,s,max_basket)\n",
        "    algo.compute(max_k)\n",
        "    if log: algo.dump_result()\n",
        "\n",
        "    return algo._frequent_itemsets\n",
        "\n"
      ],
      "id": "P-0BQUe4z29F",
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "mtL-veBk1moc",
        "outputId": "8c2e5a70-2ca3-4f7d-f045-ffcf7dd69325"
      },
      "source": [
        "from functools import partial\n",
        "\n",
        "basket_file=partial(iter_baskets_from_json,'italian300.json')\n",
        "Apriori.mb_analysis(basket_file)\n"
      ],
      "id": "mtL-veBk1moc",
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of candidate 1-itemsets: 6439\n",
            "Size in GB: 0.000295\n",
            "\n",
            "------------------------------\n",
            "Function _filter_ck executed in 0.0007603168487548828 seconds\n",
            "------------------------------\n",
            "\n",
            "Number of frequent 1-itemsets: 1140\n",
            "Size in GB: 0.000037\n",
            "------------------------------\n",
            "\n",
            "\n",
            "------------------------------\n",
            "Function _algo_init executed in 0.054376840591430664 seconds\n",
            "------------------------------\n",
            "\n",
            "\n",
            "------------------------------\n",
            "Function _get_ck executed in 1.6819777488708496 seconds\n",
            "------------------------------\n",
            "\n",
            "Total number of candidate 2-itemsets: 118747\n",
            "Size in GB: 0.005243\n",
            "\n",
            "------------------------------\n",
            "Function _filter_ck executed in 0.0189669132232666 seconds\n",
            "------------------------------\n",
            "\n",
            "Number of frequent 2-itemsets: 37906\n",
            "Size in GB: 0.001311\n",
            "------------------------------\n",
            "\n",
            "\n",
            "------------------------------\n",
            "Function _get_ck executed in 62.7361958026886 seconds\n",
            "------------------------------\n",
            "\n",
            "Total number of candidate 3-itemsets: 713202\n",
            "Size in GB: 0.041943\n",
            "\n",
            "------------------------------\n",
            "Function _filter_ck executed in 0.22894644737243652 seconds\n",
            "------------------------------\n",
            "\n",
            "Number of frequent 3-itemsets: 621306\n",
            "Size in GB: 0.020972\n",
            "------------------------------\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-1aa37b482f3f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mbasket_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter_baskets_from_json\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'italian300.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mApriori\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmb_analysis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbasket_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-36-fe69376930c1>\u001b[0m in \u001b[0;36mmb_analysis\u001b[0;34m(basket_file_it, s, max_basket, max_k, log)\u001b[0m\n\u001b[1;32m    115\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mmb_analysis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbasket_file_it\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_basket\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0malgo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mApriori\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbasket_file_it\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_basket\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m     \u001b[0malgo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_k\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlog\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0malgo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-36-fe69376930c1>\u001b[0m in \u001b[0;36mcompute\u001b[0;34m(self, max_k)\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;31m#stop when no more frequent itemsets are found or k>max_k\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_frequent_itemsets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_k\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m<=\u001b[0m\u001b[0mmax_k\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_frequent_itemsets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filter_ck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_ck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m       \u001b[0mk\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-8a1417191929>\u001b[0m in \u001b[0;36m_wrap\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_wrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mres\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-36-fe69376930c1>\u001b[0m in \u001b[0;36m_get_ck\u001b[0;34m(self, k)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_mono_prop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkuple\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m           \u001b[0;32mif\u001b[0m \u001b[0mkuple\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcandidates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcandidates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkuple\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m           \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcandidates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkuple\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPimayhEg_Ue"
      },
      "source": [
        "PCY implementation"
      ],
      "id": "wPimayhEg_Ue"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8YqFThkXhL4l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8111e60-9ed2-4200-eb13-9a164df257a1"
      },
      "source": [
        "!pip install bitmap\n",
        "from bitmap import BitMap\n",
        "\n",
        "#map a tuple to a bucket of a table of size=s\n",
        "def hash_tuple(t,s): return hash(t)%s\n"
      ],
      "id": "8YqFThkXhL4l",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting bitmap\n",
            "  Downloading https://files.pythonhosted.org/packages/3a/18/0cf0116c3faf5023a6e971549055d42907020bca37320b3ada380e07f3ff/bitmap-0.0.7-py3-none-any.whl\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from bitmap) (0.16.0)\n",
            "Installing collected packages: bitmap\n",
            "Successfully installed bitmap-0.0.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJotqhVRg-sG"
      },
      "source": [
        "class PCY(Apriori):\n",
        "  \"\"\"\n",
        "    bm_size: BitMap of frequent buckets count\n",
        "    bm: BitMap of frequent buckets\n",
        "  \"\"\"\n",
        "  def __init__(self,basket_file_it,s=0,max_basket=-1,bm_size=256):\n",
        "    super().__init__(basket_file_it,s,max_basket)\n",
        "    self._bm_size=bm_size\n",
        "    self._bm=BitMap(self._bm_size)\n",
        "  \n",
        "  \"\"\"\n",
        "    Algorithm initialization modified: initialize bucket table\n",
        "  \"\"\"\n",
        "  @time_it\n",
        "  def _algo_init(self):\n",
        "    self._frequent_itemsets=[dict()]\n",
        "    self._bm=BitMap(self._bm_size)\n",
        "\n",
        "    baskets_count=0\n",
        "    frequent_buckets=[0 for i in range(self._bm_size)]\n",
        "\n",
        "    for basket in iter_baskets(self._basket_file_it,self._max_basket):\n",
        "      baskets_count+=1\n",
        "      self._up_singles_count(basket)\n",
        "\n",
        "      #PCY variant: during the first pass hash couples to bucket\n",
        "      for couple in it.combinations(basket,r=2):\n",
        "        frequent_buckets[hash_tuple(tuple(sorted(couple)),self._bm_size)]+=1\n",
        "    \n",
        "\n",
        "    self._basket_count=baskets_count\n",
        "    if self._s<=0: self._s=self._basket_count//100\n",
        "\n",
        "    self._frequent_itemsets[0]=self._filter_ck(self._frequent_itemsets[0])\n",
        "\n",
        "    #set bit of frequent buckets in the bitmap\n",
        "    for i in range(len(frequent_buckets)):\n",
        "      if frequent_buckets[i]>=self._s: self._bm.set(i)\n",
        "  \n",
        "  \"\"\"\n",
        "    Algorithm iteration modified: consider couple that hash to a frequent bucket\n",
        "  \"\"\"\n",
        "  @time_it\n",
        "  def _get_ck(self,k):\n",
        "\n",
        "    candidates=dict()\n",
        "    for basket in iter_baskets(self._basket_file_it,self._max_basket):\n",
        "      #filter unfrequent singletons\n",
        "      basket=[word for word in basket if (word,) in self._frequent_itemsets[0]]\n",
        "      for kuple in it.combinations(basket,r=k):\n",
        "        #sort tuple in order to avoid duplication of the same itemset considered in different order\n",
        "        kuple=tuple(sorted(kuple))\n",
        "\n",
        "        #PCY variant: added constraint for couple -> must hash to a frequent bucket\n",
        "        if self._check_mono_prop(kuple,k) and (k!=2 or self._bm[hash_tuple(kuple,self._bm_size)]):\n",
        "          if kuple not in candidates.keys(): candidates[kuple]=1\n",
        "          else: candidates[kuple]+=1\n",
        "\n",
        "    return candidates\n"
      ],
      "id": "rJotqhVRg-sG",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vymqKnQ1tGb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31755eaa-8c9b-4271-da96-f893ba7c1423"
      },
      "source": [
        "algo=PCY(basket_file)\n",
        "algo.compute(3)"
      ],
      "id": "3vymqKnQ1tGb",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of candidate 1-itemsets: 6439\n",
            "Size in GB: 0.000295\n",
            "\n",
            "------------------------------\n",
            "Function _filter_ck executed in 0.000873565673828125 seconds\n",
            "------------------------------\n",
            "\n",
            "Number of frequent 1-itemsets: 1140\n",
            "Size in GB: 0.000037\n",
            "------------------------------\n",
            "\n",
            "\n",
            "------------------------------\n",
            "Function _algo_init executed in 0.895531415939331 seconds\n",
            "------------------------------\n",
            "\n",
            "\n",
            "------------------------------\n",
            "Function _get_ck executed in 2.1663713455200195 seconds\n",
            "------------------------------\n",
            "\n",
            "Total number of candidate 2-itemsets: 118747\n",
            "Size in GB: 0.005243\n",
            "\n",
            "------------------------------\n",
            "Function _filter_ck executed in 0.018021583557128906 seconds\n",
            "------------------------------\n",
            "\n",
            "Number of frequent 2-itemsets: 37906\n",
            "Size in GB: 0.001311\n",
            "------------------------------\n",
            "\n",
            "\n",
            "------------------------------\n",
            "Function _get_ck executed in 63.767906188964844 seconds\n",
            "------------------------------\n",
            "\n",
            "Total number of candidate 3-itemsets: 713202\n",
            "Size in GB: 0.041943\n",
            "\n",
            "------------------------------\n",
            "Function _filter_ck executed in 0.2209937572479248 seconds\n",
            "------------------------------\n",
            "\n",
            "Number of frequent 3-itemsets: 621306\n",
            "Size in GB: 0.020972\n",
            "------------------------------\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iaHPR8RcpoeX"
      },
      "source": [
        "Spark installation"
      ],
      "id": "iaHPR8RcpoeX"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bvp4Dio7ptUr"
      },
      "source": [
        "!sudo apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://www-eu.apache.org/dist/spark/spark-2.4.8/spark-2.4.8-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.8-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark"
      ],
      "id": "Bvp4Dio7ptUr",
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJeCUMC2qCiP"
      },
      "source": [
        "SON algorithm implementation"
      ],
      "id": "aJeCUMC2qCiP"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SckgldjIqF4V"
      },
      "source": [
        "import findspark\n",
        "\n",
        "os.environ['JAVA_HOME']='/usr/lib/jvm/java-8-openjdk-amd64'\n",
        "os.environ['SPARK_HOME']='spark-2.4.8-bin-hadoop2.7'\n",
        "\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "import pyspark\n"
      ],
      "id": "SckgldjIqF4V",
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QpZPFU73YplA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8ce74a9b-0e83-4a5b-d19b-31ce6457eb16"
      },
      "source": [
        "spark=SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "sc=spark.sparkContext\n",
        "basket_rdd=sc.parallelize(basket_file())\n",
        "basket_rdd.mapPartitions(lambda chunk: Apriori.mb_analysis(chunk)).collect()\n"
      ],
      "id": "QpZPFU73YplA",
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Py4JJavaError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-99c6226620c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mbasket_rdd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallelize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbasket_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mbasket_rdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mApriori\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmb_analysis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/spark-2.4.8-bin-hadoop2.7/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    814\u001b[0m         \"\"\"\n\u001b[1;32m    815\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 816\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    817\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mspark-2.4.8-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-2.4.8-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mspark-2.4.8-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 3.0 failed 1 times, most recent failure: Lost task 1.0 in stage 3.0 (TID 7, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/content/spark-2.4.8-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/content/spark-2.4.8-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"spark-2.4.8-bin-hadoop2.7/python/pyspark/rdd.py\", line 352, in func\n    return f(iterator)\n  File \"<ipython-input-40-99c6226620c5>\", line 4, in <lambda>\n  File \"<ipython-input-36-fe69376930c1>\", line 117, in mb_analysis\n  File \"<ipython-input-36-fe69376930c1>\", line 89, in compute\n  File \"<ipython-input-12-8a1417191929>\", line 20, in _wrap\nIndexError: list index out of range\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2107)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2107)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:411)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:417)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1925)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1913)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1912)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1912)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:948)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:948)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:948)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2146)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2095)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2084)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:759)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2067)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2088)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2107)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2132)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:990)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:989)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/content/spark-2.4.8-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/content/spark-2.4.8-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"spark-2.4.8-bin-hadoop2.7/python/pyspark/rdd.py\", line 352, in func\n    return f(iterator)\n  File \"<ipython-input-40-99c6226620c5>\", line 4, in <lambda>\n  File \"<ipython-input-36-fe69376930c1>\", line 117, in mb_analysis\n  File \"<ipython-input-36-fe69376930c1>\", line 89, in compute\n  File \"<ipython-input-12-8a1417191929>\", line 20, in _wrap\nIndexError: list index out of range\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2107)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2107)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:411)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:417)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
          ]
        }
      ]
    }
  ]
}