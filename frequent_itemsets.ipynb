{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "frequent_itemsets.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/turatig/frequent_itemsets/blob/master/frequent_itemsets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J00vwdMle-Nx"
      },
      "source": [
        "**MARKET BASKET ANALYSIS NOTEBOOK**"
      ],
      "id": "J00vwdMle-Nx"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOVdeW-efZ3z"
      },
      "source": [
        "Dowload and preprocess dataset"
      ],
      "id": "LOVdeW-efZ3z"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22ba4125",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": "OK"
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 360
        },
        "outputId": "083007a0-152e-498a-8ddb-b5d078f2e33b"
      },
      "source": [
        "!pip install kaggle\n",
        "\n",
        "\n",
        "import os,sys,time,zipfile,json,re\n",
        "import functools as ft\n",
        "import itertools as it\n",
        "from datetime import datetime as dt\n",
        "from random import uniform,shuffle\n",
        "from nltk.corpus import stopwords\n",
        "import nltk \n",
        "nltk.download('stopwords')\n",
        "\n",
        "#add kaggle crediantials json\n",
        "from google.colab import files\n",
        "\n",
        "files.upload()\n",
        "os.environ['KAGGLE_CONFIG_DIR']='.'\n",
        "\n",
        "from kaggle.api.kaggle_api_extended import KaggleApi\n",
        "\n",
        "def get_dataset():\n",
        "\n",
        "  #execute only if the dataset was not already downloaded\n",
        "  if 'old-newspaper.tsv' not in os.listdir():\n",
        "    api=KaggleApi()\n",
        "    api.authenticate()\n",
        "\n",
        "    api.dataset_download_file('alvations/old-newspapers','old-newspaper.tsv')\n",
        "\n",
        "    with zipfile.ZipFile('old-newspaper.tsv.zip','r') as _zip:\n",
        "      _zip.extractall()\n",
        "\n",
        "#Yield baskets (list of lists) reading from tsv\n",
        "#languages: subset of languages to be considered during the market-basket analysis\n",
        "def iter_baskets_from_tsv(languages=None,max_basket=-1,skip=0):\n",
        "  count=0\n",
        "  with open('old-newspaper.tsv','r') as f:\n",
        "    #skip header line\n",
        "    next(f)\n",
        "    for line in f:\n",
        "      l=line.split('\\t')\n",
        "      if languages is not None and l[0] not in languages: continue\n",
        "\n",
        "      #get a list of words as basket skipping any sequence of non-alphabetical characters\n",
        "      basket=re.split(r'[^a-zA-Z]+',l[3])\n",
        "      #remove any empty string\n",
        "      basket=[word.lower() for word in basket if word!='']\n",
        "      if basket:\n",
        "        count+=1\n",
        "        if count>skip: yield basket\n",
        "        if max_basket>0 and count-skip>=max_basket: break \n",
        "      \n",
        "    f.close()\n",
        "\n",
        "#Create txt dataset. Structure: every line (basket) is a sequence of words (items) separated from commas\n",
        "def create_txt_dataset(languages=None,max_basket=-1,skip=0,remove_stopwords=False,max_items_per_basket=-1):\n",
        "  baskets=[]\n",
        "  stopw=set()\n",
        "  #compute the mean of the number of items per basket\n",
        "  n_items=0\n",
        "\n",
        "  if remove_stopwords and languages:\n",
        "    for lang in languages: stopw|=set(stopwords.words(lang.lower()))\n",
        "\n",
        "  #execute only if the dataset was not already created\n",
        "  for line in iter_baskets_from_tsv(languages,max_basket,skip=0):\n",
        "    if stopw: line=[word for word in line if word not in stopw]\n",
        "    if line:\n",
        "      if max_items_per_basket>0:\n",
        "        #avoid bias on order during the analysis\n",
        "        shuffle(line)\n",
        "        line=line[:max_items_per_basket]\n",
        "      line=set(line)\n",
        "      baskets.append(ft.reduce(lambda w1,w2: w1+','+w2,line))\n",
        "      n_items+=len(line)\n",
        "\n",
        "  filename=ft.reduce(lambda i,j:i+'_'+j,languages).lower() if languages is not None else 'all_languages' \n",
        "  filename+=str(len(baskets))+'.txt'\n",
        "\n",
        "  with open(filename,'w') as f:\n",
        "    for basket in baskets:\n",
        "      f.write(basket+'\\n')\n",
        "\n",
        "    f.close()\n",
        "\n",
        "  print(\"Dataset {0} was created with {1} baskets with {2} items on average\".format(filename,len(baskets),n_items//len(baskets)))\n",
        "  print(\"-\"*30)\n",
        "\n",
        "#Yield baskets from txt files where every line is a basket and every basket is a sequence item1,item2...\n",
        "def iter_baskets_from_txt(filename,max_basket=-1,skip=0):\n",
        "  theres_next=True\n",
        "  basket=[]\n",
        "  count=0\n",
        "\n",
        "  with open(filename,'r') as f:\n",
        "    eol=''\n",
        "    for line in f:\n",
        "      line=line.split(',')\n",
        "      count+=1\n",
        "      #trim \\n\n",
        "      if count>skip: yield line[:-1]+[line[-1][:-1]]\n",
        "      if max_basket>0 and count-skip>=max_basket: break \n",
        "    \n",
        "    f.close()\n",
        "\n",
        "#Scan the basket file and extract a basket with fixed probability p\n",
        "def get_rand_sample(basket_file,p):\n",
        "  basket_count=0\n",
        "  sample=[]\n",
        "  all_items=set()\n",
        "\n",
        "  for basket in basket_file:\n",
        "    basket_count+=1\n",
        "    if uniform(0,1)<=p:\n",
        "      sample.append(basket)\n",
        "\n",
        "    all_items|=set(basket)\n",
        "  \n",
        "  return sample,basket_count,all_items\n",
        "\n",
        "\n",
        "\n",
        "get_dataset()"
      ],
      "id": "22ba4125",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.41.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (5.0.2)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2020.12.5)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (2.10)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-47288b68-e729-4fb5-af7a-5931201eef2d\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-47288b68-e729-4fb5-af7a-5931201eef2d\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n",
            "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 ./kaggle.json'\n",
            "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 ./kaggle.json'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TqnHG1Eppe9F"
      },
      "source": [
        "Utilities to log the execution"
      ],
      "id": "TqnHG1Eppe9F"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDz4lLPYfJMc"
      },
      "source": [
        "def sizeof_GB(obj): return \"%f\"%(sys.getsizeof(obj)/1000000000)\n",
        "\n",
        "#decorator to log time execution of function/method\n",
        "def time_it(f):\n",
        "  def _wrap(*args,**kwargs):\n",
        "    start=time.time()\n",
        "    res=f(*args,**kwargs)\n",
        "    stop=time.time()\n",
        "    \n",
        "    print('\\n'+'-'*30)\n",
        "    print('Function {0} executed in {1} seconds'.format(f.__name__,stop-start))\n",
        "    print('-'*30+'\\n')\n",
        "    return res\n",
        "  return _wrap\n",
        "\n",
        "#decorator to log the memory space used before and after a candidate itemset filtering operation\n",
        "def log_filter(f):\n",
        "  def _wrap(*args,**kwargs):\n",
        "    if len(args)>0:\n",
        "      if args[0]:\n",
        "        print('Total number of candidate {0}-itemsets: {1}\\nSize in GB: {2}'.\\\n",
        "            format(len(list(args[0].keys())[0]),len(args[0]),sizeof_GB(args[0])))\n",
        "      else:\n",
        "        print('No more candidates were found')\n",
        "      \n",
        "    #argument was given by key=value\n",
        "    else:\n",
        "      if kwargs['candidates']:\n",
        "        print('Total number of candidate {0}-itemsets: {1}\\nSize in GB: {2}'.\\\n",
        "            format(len(list(kwargs['candidates'].keys())[0]),len(kwargs['candidates']),sizeof_GB(kwargs['candidates'])))\n",
        "      else:\n",
        "        print('No more candidates were found')\n",
        "    \n",
        "    res=f(*args,**kwargs)\n",
        "\n",
        "    if res:\n",
        "      print('Number of frequent {0}-itemsets: {1}\\nSize in GB: {2}'.\\\n",
        "              format(len(list(res.keys())[0]),len(res),sizeof_GB(res)))\n",
        "    else:\n",
        "      print('No frequent itemset were found')\n",
        "    print('-'*30+'\\n')\n",
        "    return res\n",
        "\n",
        "  return _wrap\n",
        "\n",
        "#Dump on json file the result of an algorithm run\n",
        "def dump_result(algo,s,basket_count,freq_it_sets):\n",
        "  def remap(dic):\n",
        "    return {str(k):v for k,v in dic.items()}\n",
        "\n",
        "  header_info={'support_threshold':s,'total_n_baskets':basket_count}\n",
        "\n",
        "  filename=algo+'_market_basket_analysis_'+str(dt.today())[:10]+'_'+str(dt.today())[11:]+'.json'\n",
        "              \n",
        "  with open(filename,'w') as f:\n",
        "    f.write(json.dumps({'header':header_info,'frequent itemsets':remap(freq_it_sets)},indent='\\t'))\n",
        "    f.close()\n",
        "\n",
        "\"\"\"\n",
        "  Utility to clean output of analysis. Take [{freq_0_it_set},{freq_1_it_set}...] and return {freq_non_empty_it_set}\n",
        "\"\"\"\n",
        "def clean_output(freq_it_sets):\n",
        "  if not freq_it_sets[-1]:\n",
        "    #remove the last element if empty\n",
        "    freq_it_sets=freq_it_sets[:-1]\n",
        "  #remove empty itemset set\n",
        "  freq_it_sets=freq_it_sets[1:]\n",
        "  fitsets=dict()\n",
        "\n",
        "  #create a single dict with frequent itemsets and their occurences\n",
        "  for fis in freq_it_sets:\n",
        "    for k,v in fis.items():\n",
        "      fitsets[k]=v\n",
        "\n",
        "  return fitsets"
      ],
      "id": "wDz4lLPYfJMc",
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ixo8_SLkdPOd"
      },
      "source": [
        "A-priori algorithm implementation"
      ],
      "id": "Ixo8_SLkdPOd"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYkGZQBq4t9f"
      },
      "source": [
        "\"\"\"\n",
        "  Filter candidate set of itemsets according to suppport threshold \n",
        "\"\"\"\n",
        "@log_filter\n",
        "def filter_ck(candidates,s):\n",
        "  return {k:v for k,v in candidates.items() if v>=s}\n",
        "\n",
        "\"\"\"\n",
        "  Discard unfrequent singletons from a basket\n",
        "\"\"\"\n",
        "def freq_sing(freq_it_sets,basket):\n",
        "  return [word for word in basket if (word,) in freq_it_sets[1]]\n",
        "\n",
        "\"\"\"\n",
        "  Check monotonicity property\n",
        "  kuple is a possible k-itemset -> all immediate subsets (k-1 itemsets) are frequent itemsets.\n",
        "\"\"\" \n",
        "def check_mono_prop(kuple,k,freq_it_sets):\n",
        "  return all([tuple(sorted(el)) in freq_it_sets[k-1] for el in it.combinations(kuple,r=k-1)])\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "  Return candidate k-itemsets found after a basket_file pass\n",
        "\"\"\"\n",
        "def get_ck(basket_file,k,freq_it_sets):\n",
        "  candidates=dict()\n",
        "\n",
        "  for basket in basket_file:\n",
        "    basket=set(freq_sing(freq_it_sets,basket))\n",
        "              \n",
        "    for kuple in it.combinations(basket,r=k):\n",
        "        #sort tuple in order to avoid duplication caused by taking the same itemset ordered in a different way\n",
        "        kuple=tuple(sorted(kuple))\n",
        "\n",
        "        if check_mono_prop(kuple,k,freq_it_sets):\n",
        "          if kuple not in candidates.keys(): candidates[kuple]=1\n",
        "          else: candidates[kuple]+=1\n",
        "\n",
        "  return candidates\n",
        "\n",
        "\"\"\"\n",
        "  First pass of apriori. Build set of candidates singletons\n",
        "\"\"\"\n",
        "def first_pass(basket_file):\n",
        "  basket_count=0\n",
        "  candidates=dict()\n",
        "\n",
        "  for basket in basket_file:\n",
        "    basket_count+=1\n",
        "\n",
        "    for item in set(basket):\n",
        "      if (item,) not in candidates.keys(): candidates[(item,)]=1\n",
        "      else: candidates[(item,)]+=1\n",
        "  \n",
        "  return candidates,basket_count\n",
        "\n",
        "\"\"\"\n",
        "  Second pass of apriori. Build set of candidates couples\n",
        "\"\"\"\n",
        "def second_pass(basket_file,freq_it_sets):\n",
        "  basket_count=0\n",
        "  candidates=dict()\n",
        "\n",
        "  for basket in basket_file:\n",
        "    basket=set(freq_sing(freq_it_sets,basket))\n",
        "\n",
        "    for couple in it.combinations(basket,r=2):\n",
        "      couple=tuple(sorted(couple))\n",
        "      if couple not in candidates.keys(): candidates[couple]=1\n",
        "      else: candidates[couple]+=1\n",
        "  \n",
        "  return candidates\n",
        "\n",
        "\"\"\"\n",
        "  Loop for counting k-itemsets with k>2\n",
        "\"\"\"\n",
        "def main_loop(basket_file,freq_it_sets,s,max_k=-1):\n",
        "  k=3\n",
        "  #stop when no more frequent itemsets are found or k>max_k\n",
        "  while freq_it_sets[-1] and (max_k<0 or k<=max_k):\n",
        "\n",
        "    #duplicate generator for multiple iterations\n",
        "    basket_file,_bf=it.tee(basket_file,2)\n",
        "    ck=get_ck(basket_file,k,freq_it_sets)\n",
        "    freq_it_sets.append(filter_ck(ck,s))\n",
        "    k+=1\n",
        "    basket_file=_bf\n",
        "\n",
        "\"\"\"\n",
        "  Apriori algorithm iteration\n",
        "\"\"\"\n",
        "@time_it\n",
        "def apriori(basket_file,s=-1,max_k=-1,log=False):\n",
        "\n",
        "    freq_it_sets=[{tuple():1}]\n",
        "    basket_file,_bf=it.tee(basket_file,2)\n",
        "    ck,basket_count=first_pass(_bf)\n",
        "    \n",
        "    #set threshold to the 1% of the total number of baskets\n",
        "    if s<0: s=basket_count//100\n",
        "    freq_it_sets.append(filter_ck(ck,s))\n",
        "\n",
        "    basket_file,_bf=it.tee(basket_file,2)\n",
        "    freq_it_sets.append(filter_ck(second_pass(_bf,freq_it_sets),s))\n",
        "\n",
        "    main_loop(basket_file,freq_it_sets,s,max_k)\n",
        "    freq_it_sets=clean_output(freq_it_sets)\n",
        "    if log:dump_result(\"apriori\",s,basket_count,freq_it_sets)\n",
        "\n",
        "    return freq_it_sets"
      ],
      "id": "bYkGZQBq4t9f",
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPimayhEg_Ue"
      },
      "source": [
        "PCY implementation"
      ],
      "id": "wPimayhEg_Ue"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8YqFThkXhL4l"
      },
      "source": [
        "!pip install -q bitmap\n",
        "from bitmap import BitMap\n",
        "\n",
        "#map a tuple to a bucket of a table of size=s\n",
        "def hash_tuple(t,s): return hash(t)%s\n",
        "def set_all(bm):\n",
        "  for i in bm.size():\n",
        "    bm.set(i)\n"
      ],
      "id": "8YqFThkXhL4l",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJotqhVRg-sG"
      },
      "source": [
        "\n",
        "\"\"\"\n",
        "  PCY first pass. Count items' occurence and build buckets table\n",
        "\"\"\"\n",
        "def pcy_first_pass(basket_file,bm):\n",
        "  basket_count=0\n",
        "  candidates=dict()\n",
        "  buckets=[0 for i in range(bm.size())]\n",
        "\n",
        "  for basket in basket_file:\n",
        "    basket_count+=1\n",
        "\n",
        "    for item in set(basket):\n",
        "      if (item,) not in candidates.keys(): candidates[(item,)]=1\n",
        "      else: candidates[(item,)]+=1\n",
        "\n",
        "    #PCY variant: during the first pass hash couples to buckets\n",
        "    for couple in it.combinations(set(basket),r=2):\n",
        "      buckets[hash_tuple(tuple(sorted(couple)),bm.size())]+=1\n",
        "  \n",
        "  return candidates,basket_count,buckets\n",
        "\n",
        "\n",
        "def pcy_second_pass(basket_file,freq_it_sets,bm):\n",
        "  basket_count=0\n",
        "  candidates=dict()\n",
        "\n",
        "  for basket in basket_file:\n",
        "    basket=set(freq_sing(freq_it_sets,basket))\n",
        "\n",
        "    for couple in it.combinations(basket,r=2):\n",
        "      couple=tuple(sorted(couple))\n",
        "      #PCY variant: added constraint for couple -> must hash to a frequent bucket\n",
        "      if bm[hash_tuple(couple,bm.size())]:\n",
        "        if couple not in candidates.keys(): candidates[couple]=1\n",
        "        else: candidates[couple]+=1\n",
        "  \n",
        "  return candidates\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "  PCY algorithm iteration\n",
        "\"\"\"\n",
        "@time_it\n",
        "def pcy(basket_file,s=-1,max_k=-1,bm_size=256,log=False):\n",
        "    freq_it_sets=[{tuple():1}]\n",
        "    bm=BitMap(bm_size)\n",
        "\n",
        "    freq_it_sets=[{tuple():1}]\n",
        "    basket_file,_bf=it.tee(basket_file,2)\n",
        "    ck,basket_count,buckets=pcy_first_pass(_bf,bm)\n",
        "    #set threshold to the 1% of the total number of baskets\n",
        "    if s<0: s=basket_count//100\n",
        "    freq_it_sets.append(filter_ck(ck,s))\n",
        "    \n",
        "    #PCY variant:set bit of frequent buckets in the bitmap for couples\n",
        "    for i in range(len(buckets)):\n",
        "      if buckets[i]>=s: bm.set(i)\n",
        "\n",
        "    basket_file,_bf=it.tee(basket_file,2)\n",
        "    freq_it_sets.append(filter_ck(pcy_second_pass(_bf,freq_it_sets,bm),s))\n",
        "\n",
        "    main_loop(basket_file,freq_it_sets,s,max_k)\n",
        "    freq_it_sets=clean_output(freq_it_sets)\n",
        "    if log:dump_result(\"apriori\",s,basket_count,freq_it_sets)\n",
        "\n",
        "    return freq_it_sets"
      ],
      "id": "rJotqhVRg-sG",
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfCOEYFIht9c"
      },
      "source": [
        "Toivonen algorithm implementation"
      ],
      "id": "qfCOEYFIht9c"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3ZvvKieh0WL"
      },
      "source": [
        "from math import ceil,floor\n",
        "\n",
        "\"\"\"\n",
        "  Utility to build negative border efficiently.\n",
        "  Return all k-itemsets made from k-1 frequent itemsets found in sample (only for k>=2)\n",
        "\"\"\"\n",
        "def nb_candidates(freq_in_sample,k):\n",
        "  immediate_subs=[itemset for itemset in freq_in_sample if len(itemset)==k-1]\n",
        "  self_join=[set(kuple[0])|set(kuple[1]) for kuple in it.product(immediate_subs,immediate_subs)]\n",
        "\n",
        "  return {tuple(sorted(itemset)) for itemset in self_join if len(itemset)==k}\n",
        "\n",
        "\"\"\"\n",
        "  Function that builds the negative border\n",
        "\"\"\"\n",
        "def build_neg_border(freq_in_sample,all_items):\n",
        "  neg_border=set()\n",
        "  max_k=len(max(freq_in_sample,key=lambda el:len(el)))+1 if freq_in_sample else 1\n",
        "\n",
        "  for k in range(1,max_k+1):\n",
        "    #add singletons not found in sample\n",
        "    if k==1:\n",
        "      for singleton in all_items:\n",
        "        if (singleton,) not in freq_in_sample:\n",
        "          neg_border|={(singleton,)}\n",
        "    else:\n",
        "      for itemset in nb_candidates(freq_in_sample,k):\n",
        "        if itemset not in freq_in_sample:\n",
        "          neg_border|={itemset}\n",
        "\n",
        "\n",
        "  return neg_border\n",
        "\n",
        "\n",
        "@time_it\n",
        "def toivonen(basket_file,s=-1,p=0.25,scaling=0.9,max_k=-1,log=False):\n",
        "  negative_border=set()\n",
        "  #freq_it_sets:the k-1-th element is the set of frequent itemsets made of k elements\n",
        "  freq_it_sets=dict()\n",
        "  basket_file,_bf=it.tee(basket_file,2)\n",
        "\n",
        "  sample,basket_count,all_items=get_rand_sample(_bf,p)\n",
        "  if s<0: s=basket_count//100\n",
        "  \n",
        "  ps=floor(scaling*p*s)\n",
        "\n",
        "  #keeping only itemsets and not their counters\n",
        "  freq_in_sample={k for k in apriori(sample,s=ps,max_k=max_k).keys()}\n",
        "\n",
        "  neg_border=build_neg_border(freq_in_sample,all_items)\n",
        "\n",
        "  max_k=len(max(neg_border,key=lambda el:len(el))) if max_k<0 else max_k\n",
        "\n",
        "  #add all the items not present in the sample to build the complete negative border\n",
        "  ck=freq_in_sample | neg_border\n",
        "\n",
        "  #total pass of toivonen algorithm\n",
        "  for basket in basket_file:\n",
        "    basket=set(basket)\n",
        "    for k in range(1,max_k+1):\n",
        "      for kuple in it.combinations(basket,r=k):\n",
        "        kuple=tuple(sorted(kuple))\n",
        "\n",
        "        if kuple in ck:\n",
        "          if kuple not in freq_it_sets:\n",
        "            freq_it_sets[kuple]=1\n",
        "          else:\n",
        "            freq_it_sets[kuple]+=1\n",
        "\n",
        "  #filter according to threshold\n",
        "  freq_it_sets=filter_ck(freq_it_sets,s)\n",
        "  #check that no elements of the negative border are frequent in the sample\n",
        "  if not [itemset for itemset in neg_border if itemset in freq_it_sets]:\n",
        "    if log:dump_result(\"toivonen\",s,basket_count,freq_it_sets)\n",
        "    return freq_it_sets\n",
        "  else:\n",
        "    return None\n",
        "\n"
      ],
      "id": "b3ZvvKieh0WL",
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOAV6G-5LO2X"
      },
      "source": [
        "Validate implementation by comparing it with apyori's implementation on a small size dataset"
      ],
      "id": "BOAV6G-5LO2X"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtL-veBk1moc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a4bab77-fb0c-46c8-d3f3-b28d57a84b64"
      },
      "source": [
        "!pip install apyori\n",
        "import apyori as ap\n",
        "from functools import partial\n",
        "from random import randint\n",
        "\n",
        "create_txt_dataset(['Italian'],300)\n",
        "\n",
        "s_thresh=randint(30,100)\n",
        "apriori_res=apriori(iter_baskets_from_txt('italian300.txt'),s=s_thresh,max_k=3)\n",
        "pcy_res=pcy(iter_baskets_from_txt('italian300.txt'),s=s_thresh,max_k=3,bm_size=2048)\n",
        "toivonen_res=toivonen(iter_baskets_from_txt('italian300.txt'),s=s_thresh,p=0.4,scaling=0.8,max_k=3)\n",
        "\n",
        "bf=[i for i in iter_baskets_from_txt('italian300.txt')]\n",
        "basket_count=len(bf)\n",
        "\n",
        "start=time.time()\n",
        "apyori_res=list(ap.apriori(bf,min_support=s_thresh/basket_count,max_length=3))\n",
        "stop=time.time()\n",
        "\n",
        "print('\\n'+'-'*30)\n",
        "print('Function apyori.apriori executed in {0} seconds'.format(stop-start))\n",
        "print('-'*30+'\\n')\n",
        "\n",
        "#TEST\n",
        "def test(target,tested,basket_count):\n",
        "  failed=False\n",
        "\n",
        "  if len(target)!=len(tested): failed=True\n",
        "  test_itemsets={tuple(sorted(i.items)):i.support for i in target}\n",
        "\n",
        "  for k,v in tested.items():\n",
        "    if k not in test_itemsets:\n",
        "      failed=True\n",
        "      break\n",
        "    #apyori lib compute support as itemset_count/basket_count\n",
        "    elif test_itemsets[k]!=v/basket_count:\n",
        "      failed=True\n",
        "      break\n",
        "\n",
        "  if failed:\n",
        "    print(\"-\"*30+\"TEST FAILED\"+\"-\"*30)\n",
        "  else:\n",
        "    print(\"-\"*30+\"TEST PASSED\"+\"-\"*30)\n",
        "\n",
        "test(apyori_res,apriori_res,basket_count)\n",
        "test(apyori_res,pcy_res,basket_count)\n",
        "if toivonen_res is not None:\n",
        "  test(apyori_res,toivonen_res,basket_count)\n"
      ],
      "id": "mtL-veBk1moc",
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: apyori in /usr/local/lib/python3.7/dist-packages (1.1.2)\n",
            "Dataset italian300.txt was created with 300 baskets with 54 items on average\n",
            "------------------------------\n",
            "Total number of candidate 1-itemsets: 6439\n",
            "Size in GB: 0.000295\n",
            "Number of frequent 1-itemsets: 23\n",
            "Size in GB: 0.000001\n",
            "------------------------------\n",
            "\n",
            "Total number of candidate 2-itemsets: 253\n",
            "Size in GB: 0.000009\n",
            "Number of frequent 2-itemsets: 120\n",
            "Size in GB: 0.000005\n",
            "------------------------------\n",
            "\n",
            "Total number of candidate 3-itemsets: 367\n",
            "Size in GB: 0.000019\n",
            "Number of frequent 3-itemsets: 220\n",
            "Size in GB: 0.000009\n",
            "------------------------------\n",
            "\n",
            "\n",
            "------------------------------\n",
            "Function apriori executed in 0.28304028511047363 seconds\n",
            "------------------------------\n",
            "\n",
            "Total number of candidate 1-itemsets: 6439\n",
            "Size in GB: 0.000295\n",
            "Number of frequent 1-itemsets: 23\n",
            "Size in GB: 0.000001\n",
            "------------------------------\n",
            "\n",
            "Total number of candidate 2-itemsets: 253\n",
            "Size in GB: 0.000009\n",
            "Number of frequent 2-itemsets: 120\n",
            "Size in GB: 0.000005\n",
            "------------------------------\n",
            "\n",
            "Total number of candidate 3-itemsets: 367\n",
            "Size in GB: 0.000019\n",
            "Number of frequent 3-itemsets: 220\n",
            "Size in GB: 0.000009\n",
            "------------------------------\n",
            "\n",
            "\n",
            "------------------------------\n",
            "Function pcy executed in 0.9443881511688232 seconds\n",
            "------------------------------\n",
            "\n",
            "Total number of candidate 1-itemsets: 3456\n",
            "Size in GB: 0.000148\n",
            "Number of frequent 1-itemsets: 30\n",
            "Size in GB: 0.000001\n",
            "------------------------------\n",
            "\n",
            "Total number of candidate 2-itemsets: 435\n",
            "Size in GB: 0.000019\n",
            "Number of frequent 2-itemsets: 211\n",
            "Size in GB: 0.000009\n",
            "------------------------------\n",
            "\n",
            "Total number of candidate 3-itemsets: 879\n",
            "Size in GB: 0.000037\n",
            "Number of frequent 3-itemsets: 563\n",
            "Size in GB: 0.000019\n",
            "------------------------------\n",
            "\n",
            "\n",
            "------------------------------\n",
            "Function apriori executed in 0.18299317359924316 seconds\n",
            "------------------------------\n",
            "\n",
            "Total number of candidate 1-itemsets: 8876\n",
            "Size in GB: 0.000295\n",
            "Number of frequent 1-itemsets: 363\n",
            "Size in GB: 0.000019\n",
            "------------------------------\n",
            "\n",
            "\n",
            "------------------------------\n",
            "Function toivonen executed in 10.44698166847229 seconds\n",
            "------------------------------\n",
            "\n",
            "\n",
            "------------------------------\n",
            "Function apyori.apriori executed in 0.05035901069641113 seconds\n",
            "------------------------------\n",
            "\n",
            "------------------------------TEST PASSED------------------------------\n",
            "------------------------------TEST PASSED------------------------------\n",
            "------------------------------TEST PASSED------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlgMk0-1sZVi"
      },
      "source": [
        "Experiment: \n",
        "\n",
        "*   build a dataset with english newspapers (baskets)\n",
        "*   consider 3 different orders of magnitude (10^3,10^5,10^6) for the number of baskets to analyze\n",
        "*   Evaluate performance of the 3 algorithms"
      ],
      "id": "nlgMk0-1sZVi"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86af2QLusWUd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1cfd0386-920b-40b7-931f-67805aa2bda7"
      },
      "source": [
        "create_txt_dataset(['English'],1000,remove_stopwords=True,max_items_per_basket=15)\n",
        "create_txt_dataset(['English'],100000,remove_stopwords=True,max_items_per_basket=15)\n",
        "create_txt_dataset(['English'],1000000,remove_stopwords=True,max_items_per_basket=15)"
      ],
      "id": "86af2QLusWUd",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset english999.txt was created with 999 baskets with 11 items on average\n",
            "------------------------------\n",
            "Dataset english99944.txt was created with 99944 baskets with 11 items on average\n",
            "------------------------------\n",
            "Dataset english999318.txt was created with 999318 baskets with 11 items on average\n",
            "------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fvqYt_987RXv",
        "outputId": "958719de-f2f1-4eb0-cf0b-07a99412668c"
      },
      "source": [
        "basket_files=['english999.txt','english99944.txt','english999318.txt']\n",
        "\n",
        "for basket_file in basket_files:\n",
        "\n",
        "  bf=[i for i in iter_baskets_from_txt(basket_file)]\n",
        "  basket_count=len(bf)\n",
        "  s_thresh=basket_count//100\n",
        "\n",
        "  apriori_res=apriori(iter_baskets_from_txt(basket_file),s=s_thresh,max_k=2)\n",
        "  pcy_res=pcy(iter_baskets_from_txt(basket_file),s=s_thresh,bm_size=1024,max_k=2)\n",
        "  toivonen_res=toivonen(iter_baskets_from_txt(basket_file),s=s_thresh,p=0.4,scaling=0.8,max_k=2)\n",
        "\n",
        "  start=time.time()\n",
        "  apyori_res=list(ap.apriori(bf,min_support=s_thresh/basket_count,max_length=2))\n",
        "  stop=time.time()\n",
        "  print('\\n'+'-'*30)\n",
        "  print('Function apyori.apriori executed in {0} seconds'.format(stop-start))\n",
        "  print('-'*30+'\\n')\n",
        "\n",
        "  print('\\n'+'*'*30)\n",
        "  print('Test on basket file {0}'.format(basket_file))\n",
        "  print('*'*30+'\\n')\n",
        "\n",
        "  print('Apriori results')\n",
        "  test(apyori_res,apriori_res,basket_count)\n",
        "  print('PCY results')\n",
        "  test(apyori_res,pcy_res,basket_count)\n",
        "  print('Toivonen results')\n",
        "  if toivonen_res is not None:\n",
        "    test(apyori_res,toivonen_res,basket_count)\n",
        "  else:\n",
        "    print('Toivonen didn\\'t find any frequent itemset')"
      ],
      "id": "fvqYt_987RXv",
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of candidate 1-itemsets: 5560\n",
            "Size in GB: 0.000295\n",
            "Number of frequent 1-itemsets: 179\n",
            "Size in GB: 0.000009\n",
            "------------------------------\n",
            "\n",
            "Total number of candidate 2-itemsets: 3551\n",
            "Size in GB: 0.000148\n",
            "Number of frequent 2-itemsets: 10\n",
            "Size in GB: 0.000000\n",
            "------------------------------\n",
            "\n",
            "\n",
            "------------------------------\n",
            "Function apriori executed in 0.023258209228515625 seconds\n",
            "------------------------------\n",
            "\n",
            "Total number of candidate 1-itemsets: 5560\n",
            "Size in GB: 0.000295\n",
            "Number of frequent 1-itemsets: 179\n",
            "Size in GB: 0.000009\n",
            "------------------------------\n",
            "\n",
            "Total number of candidate 2-itemsets: 3551\n",
            "Size in GB: 0.000148\n",
            "Number of frequent 2-itemsets: 10\n",
            "Size in GB: 0.000000\n",
            "------------------------------\n",
            "\n",
            "\n",
            "------------------------------\n",
            "Function pcy executed in 0.11531281471252441 seconds\n",
            "------------------------------\n",
            "\n",
            "Total number of candidate 1-itemsets: 3054\n",
            "Size in GB: 0.000148\n",
            "Number of frequent 1-itemsets: 811\n",
            "Size in GB: 0.000037\n",
            "------------------------------\n",
            "\n",
            "Total number of candidate 2-itemsets: 9338\n",
            "Size in GB: 0.000295\n",
            "Number of frequent 2-itemsets: 386\n",
            "Size in GB: 0.000019\n",
            "------------------------------\n",
            "\n",
            "\n",
            "------------------------------\n",
            "Function apriori executed in 0.01750779151916504 seconds\n",
            "------------------------------\n",
            "\n",
            "Total number of candidate 1-itemsets: 20558\n",
            "Size in GB: 0.000590\n",
            "Number of frequent 1-itemsets: 189\n",
            "Size in GB: 0.000009\n",
            "------------------------------\n",
            "\n",
            "\n",
            "------------------------------\n",
            "Function toivonen executed in 3.117540121078491 seconds\n",
            "------------------------------\n",
            "\n",
            "\n",
            "------------------------------\n",
            "Function apyori.apriori executed in 0.06539654731750488 seconds\n",
            "------------------------------\n",
            "\n",
            "\n",
            "******************************\n",
            "Test on basket file english999.txt\n",
            "******************************\n",
            "\n",
            "Apriori results\n",
            "------------------------------TEST PASSED------------------------------\n",
            "PCY results\n",
            "------------------------------TEST PASSED------------------------------\n",
            "Toivonen results\n",
            "Toivonen didn't find any frequent itemset\n",
            "Total number of candidate 1-itemsets: 66798\n",
            "Size in GB: 0.002622\n",
            "Number of frequent 1-itemsets: 104\n",
            "Size in GB: 0.000005\n",
            "------------------------------\n",
            "\n",
            "Total number of candidate 2-itemsets: 5352\n",
            "Size in GB: 0.000148\n",
            "Number of frequent 2-itemsets: 1\n",
            "Size in GB: 0.000000\n",
            "------------------------------\n",
            "\n",
            "\n",
            "------------------------------\n",
            "Function apriori executed in 1.9361011981964111 seconds\n",
            "------------------------------\n",
            "\n",
            "Total number of candidate 1-itemsets: 66798\n",
            "Size in GB: 0.002622\n",
            "Number of frequent 1-itemsets: 104\n",
            "Size in GB: 0.000005\n",
            "------------------------------\n",
            "\n",
            "Total number of candidate 2-itemsets: 5352\n",
            "Size in GB: 0.000148\n",
            "Number of frequent 2-itemsets: 1\n",
            "Size in GB: 0.000000\n",
            "------------------------------\n",
            "\n",
            "\n",
            "------------------------------\n",
            "Function pcy executed in 10.23287057876587 seconds\n",
            "------------------------------\n",
            "\n",
            "Total number of candidate 1-itemsets: 43802\n",
            "Size in GB: 0.002622\n",
            "Number of frequent 1-itemsets: 168\n",
            "Size in GB: 0.000005\n",
            "------------------------------\n",
            "\n",
            "Total number of candidate 2-itemsets: 13674\n",
            "Size in GB: 0.000590\n",
            "Number of frequent 2-itemsets: 6\n",
            "Size in GB: 0.000000\n",
            "------------------------------\n",
            "\n",
            "\n",
            "------------------------------\n",
            "Function apriori executed in 0.6771116256713867 seconds\n",
            "------------------------------\n",
            "\n",
            "Total number of candidate 1-itemsets: 80791\n",
            "Size in GB: 0.002622\n",
            "Number of frequent 1-itemsets: 105\n",
            "Size in GB: 0.000005\n",
            "------------------------------\n",
            "\n",
            "\n",
            "------------------------------\n",
            "Function toivonen executed in 7.758591175079346 seconds\n",
            "------------------------------\n",
            "\n",
            "\n",
            "------------------------------\n",
            "Function apyori.apriori executed in 2.366748571395874 seconds\n",
            "------------------------------\n",
            "\n",
            "\n",
            "******************************\n",
            "Test on basket file english99944.txt\n",
            "******************************\n",
            "\n",
            "Apriori results\n",
            "------------------------------TEST PASSED------------------------------\n",
            "PCY results\n",
            "------------------------------TEST PASSED------------------------------\n",
            "Toivonen results\n",
            "------------------------------TEST PASSED------------------------------\n",
            "Total number of candidate 1-itemsets: 175614\n",
            "Size in GB: 0.010486\n",
            "Number of frequent 1-itemsets: 106\n",
            "Size in GB: 0.000005\n",
            "------------------------------\n",
            "\n",
            "Total number of candidate 2-itemsets: 5565\n",
            "Size in GB: 0.000295\n",
            "Number of frequent 2-itemsets: 1\n",
            "Size in GB: 0.000000\n",
            "------------------------------\n",
            "\n",
            "\n",
            "------------------------------\n",
            "Function apriori executed in 22.358069896697998 seconds\n",
            "------------------------------\n",
            "\n",
            "Total number of candidate 1-itemsets: 175614\n",
            "Size in GB: 0.010486\n",
            "Number of frequent 1-itemsets: 106\n",
            "Size in GB: 0.000005\n",
            "------------------------------\n",
            "\n",
            "Total number of candidate 2-itemsets: 5565\n",
            "Size in GB: 0.000295\n",
            "Number of frequent 2-itemsets: 1\n",
            "Size in GB: 0.000000\n",
            "------------------------------\n",
            "\n",
            "\n",
            "------------------------------\n",
            "Function pcy executed in 105.71686458587646 seconds\n",
            "------------------------------\n",
            "\n",
            "Total number of candidate 1-itemsets: 120623\n",
            "Size in GB: 0.005243\n",
            "Number of frequent 1-itemsets: 169\n",
            "Size in GB: 0.000005\n",
            "------------------------------\n",
            "\n",
            "Total number of candidate 2-itemsets: 14194\n",
            "Size in GB: 0.000590\n",
            "Number of frequent 2-itemsets: 5\n",
            "Size in GB: 0.000000\n",
            "------------------------------\n",
            "\n",
            "\n",
            "------------------------------\n",
            "Function apriori executed in 7.058328151702881 seconds\n",
            "------------------------------\n",
            "\n",
            "Total number of candidate 1-itemsets: 189810\n",
            "Size in GB: 0.010486\n",
            "Number of frequent 1-itemsets: 107\n",
            "Size in GB: 0.000005\n",
            "------------------------------\n",
            "\n",
            "\n",
            "------------------------------\n",
            "Function toivonen executed in 77.71448850631714 seconds\n",
            "------------------------------\n",
            "\n",
            "\n",
            "------------------------------\n",
            "Function apyori.apriori executed in 16.51896357536316 seconds\n",
            "------------------------------\n",
            "\n",
            "\n",
            "******************************\n",
            "Test on basket file english999318.txt\n",
            "******************************\n",
            "\n",
            "Apriori results\n",
            "------------------------------TEST PASSED------------------------------\n",
            "PCY results\n",
            "------------------------------TEST PASSED------------------------------\n",
            "Toivonen results\n",
            "------------------------------TEST PASSED------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}