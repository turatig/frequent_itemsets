{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Copia di frequent_itemsets.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/turatig/frequent_itemsets/blob/master/frequent_itemsets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J00vwdMle-Nx"
      },
      "source": [
        "**MARKET BASKET ANALYSIS NOTEBOOK**"
      ],
      "id": "J00vwdMle-Nx"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOVdeW-efZ3z"
      },
      "source": [
        "Dowload and preprocess dataset"
      ],
      "id": "LOVdeW-efZ3z"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22ba4125",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": "OK"
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "30bf631e-e5ef-4ee3-e3ec-5546513b1f70"
      },
      "source": [
        "!pip install kaggle\n",
        "\n",
        "\n",
        "import os,sys,time,zipfile,json,re\n",
        "import functools as ft\n",
        "import itertools as it\n",
        "from datetime import datetime as dt\n",
        "from random import uniform,shuffle\n",
        "from nltk.corpus import stopwords\n",
        "import nltk \n",
        "nltk.download('stopwords')\n",
        "\n",
        "#add kaggle crediantials json\n",
        "from google.colab import files\n",
        "\n",
        "files.upload()\n",
        "os.environ['KAGGLE_CONFIG_DIR']='.'\n",
        "\n",
        "from kaggle.api.kaggle_api_extended import KaggleApi\n",
        "\n",
        "def get_dataset():\n",
        "\n",
        "  #execute only if the dataset was not already downloaded\n",
        "  if 'old-newspaper.tsv' not in os.listdir():\n",
        "    api=KaggleApi()\n",
        "    api.authenticate()\n",
        "\n",
        "    api.dataset_download_file('alvations/old-newspapers','old-newspaper.tsv')\n",
        "\n",
        "    with zipfile.ZipFile('old-newspaper.tsv.zip','r') as _zip:\n",
        "      _zip.extractall()\n",
        "\n",
        "#Yield baskets (list of lists) reading from tsv\n",
        "#languages: subset of languages to be considered during the market-basket analysis\n",
        "def iter_baskets_from_tsv(languages=None,max_basket=-1,skip=0):\n",
        "  count=0\n",
        "  with open('old-newspaper.tsv','r') as f:\n",
        "    #skip header line\n",
        "    next(f)\n",
        "    for line in f:\n",
        "      l=line.split('\\t')\n",
        "      if languages is not None and l[0] not in languages: continue\n",
        "\n",
        "      #get a list of words as basket skipping any sequence of non-alphabetical characters\n",
        "      basket=re.split(r'[^a-zA-Z]+',l[3])\n",
        "      #remove any empty string\n",
        "      basket=[word.lower() for word in basket if word!='']\n",
        "      if basket:\n",
        "        count+=1\n",
        "        if count>skip: yield basket\n",
        "        if max_basket>0 and count-skip>=max_basket: break \n",
        "      \n",
        "    f.close()\n",
        "\n",
        "#Create txt dataset. Structure: every line (basket) is a sequence of words (items) separated from commas\n",
        "def create_txt_dataset(languages=None,max_basket=-1,skip=0,remove_stopwords=False,max_items_per_basket=-1):\n",
        "  baskets=[]\n",
        "  stopw=set()\n",
        "  #compute the mean of the number of items per basket\n",
        "  n_items=0\n",
        "\n",
        "  if remove_stopwords and languages:\n",
        "    for lang in languages: stopw|=set(stopwords.words(lang.lower()))\n",
        "\n",
        "  #execute only if the dataset was not already created\n",
        "  for line in iter_baskets_from_tsv(languages,max_basket,skip=0):\n",
        "    if stopw: line=[word for word in line if word not in stopw]\n",
        "    if line:\n",
        "      if max_items_per_basket>0:\n",
        "        #avoid bias on order during the analysis\n",
        "        shuffle(line)\n",
        "        line=line[:max_items_per_basket]\n",
        "      line=set(line)\n",
        "      baskets.append(ft.reduce(lambda w1,w2: w1+','+w2,line))\n",
        "      n_items+=len(line)\n",
        "\n",
        "  filename=ft.reduce(lambda i,j:i+'_'+j,languages).lower() if languages is not None else 'all_languages' \n",
        "  filename+=str(len(baskets))+'.txt'\n",
        "\n",
        "  with open(filename,'w') as f:\n",
        "    for basket in baskets:\n",
        "      f.write(basket+'\\n')\n",
        "\n",
        "    f.close()\n",
        "\n",
        "  print(\"Dataset {0} was created with {1} baskets with {2} items on average\".format(filename,len(baskets),n_items//len(baskets)))\n",
        "  print(\"-\"*30)\n",
        "\n",
        "#Yield baskets from txt files where every line is a basket and every basket is a sequence item1,item2...\n",
        "def iter_baskets_from_txt(filename,max_basket=-1,skip=0):\n",
        "  theres_next=True\n",
        "  basket=[]\n",
        "  count=0\n",
        "\n",
        "  with open(filename,'r') as f:\n",
        "    eol=''\n",
        "    for line in f:\n",
        "      line=line.split(',')\n",
        "      count+=1\n",
        "      #trim \\n\n",
        "      if count>skip: yield line[:-1]+[line[-1][:-1]]\n",
        "      if max_basket>0 and count-skip>=max_basket: break \n",
        "    \n",
        "    f.close()\n",
        "\n",
        "#Scan the basket file and extract a basket with fixed probability p\n",
        "def get_rand_sample(basket_file,p):\n",
        "  basket_count=0\n",
        "  sample=[]\n",
        "  all_items=set()\n",
        "\n",
        "  for basket in basket_file:\n",
        "    basket_count+=1\n",
        "    if uniform(0,1)<=p:\n",
        "      sample.append(basket)\n",
        "\n",
        "    all_items|=set(basket)\n",
        "  \n",
        "  return sample,basket_count,all_items\n",
        "\n",
        "\n",
        "\n",
        "get_dataset()"
      ],
      "id": "22ba4125",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.12)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (5.0.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2020.12.5)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (2.10)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-cf367f75-1072-48ab-bfdd-cfef7417c4f1\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-cf367f75-1072-48ab-bfdd-cfef7417c4f1\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n",
            "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 ./kaggle.json'\n",
            "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 ./kaggle.json'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TqnHG1Eppe9F"
      },
      "source": [
        "Utilities to log the execution"
      ],
      "id": "TqnHG1Eppe9F"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDz4lLPYfJMc"
      },
      "source": [
        "def sizeof_GB(obj): return \"%f\"%(sys.getsizeof(obj)/1000000000)\n",
        "\n",
        "#decorator to log time execution of function/method\n",
        "def time_it(f):\n",
        "  def _wrap(*args,**kwargs):\n",
        "    start=time.time()\n",
        "    res=f(*args,**kwargs)\n",
        "    stop=time.time()\n",
        "    \n",
        "    print('\\n'+'-'*30)\n",
        "    print('Function {0} executed in {1} seconds'.format(f.__name__,stop-start))\n",
        "    print('-'*30+'\\n')\n",
        "    return res\n",
        "  return _wrap\n",
        "\n",
        "#decorator to log the memory space used before and after a candidate itemset filtering operation\n",
        "def log_filter(f):\n",
        "  def _wrap(*args,**kwargs):\n",
        "    if len(args)>0:\n",
        "      if args[0]:\n",
        "        print('Total number of candidate {0}-itemsets: {1}\\nSize in GB: {2}'.\\\n",
        "            format(len(list(args[0].keys())[0]),len(args[0]),sizeof_GB(args[0])))\n",
        "      else:\n",
        "        print('No more candidates were found')\n",
        "      \n",
        "    #argument was given by key=value\n",
        "    else:\n",
        "      if kwargs['candidates']:\n",
        "        print('Total number of candidate {0}-itemsets: {1}\\nSize in GB: {2}'.\\\n",
        "            format(len(list(kwargs['candidates'].keys())[0]),len(kwargs['candidates']),sizeof_GB(kwargs['candidates'])))\n",
        "      else:\n",
        "        print('No more candidates were found')\n",
        "    \n",
        "    res=f(*args,**kwargs)\n",
        "\n",
        "    if res:\n",
        "      print('Number of frequent {0}-itemsets: {1}\\nSize in GB: {2}'.\\\n",
        "              format(len(list(res.keys())[0]),len(res),sizeof_GB(res)))\n",
        "    else:\n",
        "      print('No frequent itemset were found')\n",
        "    print('-'*30+'\\n')\n",
        "    return res\n",
        "\n",
        "  return _wrap\n",
        "\n",
        "#Dump on json file the result of an algorithm run\n",
        "def dump_result(algo,s,basket_count,freq_it_sets):\n",
        "  def remap(dic):\n",
        "    return {str(k):v for k,v in dic.items()}\n",
        "\n",
        "  header_info={'support_threshold':s,'total_n_baskets':basket_count}\n",
        "\n",
        "  filename=algo+'_market_basket_analysis_'+str(dt.today())[:10]+'_'+str(dt.today())[11:]+'.json'\n",
        "              \n",
        "  with open(filename,'w') as f:\n",
        "    f.write(json.dumps({'header':header_info,'frequent itemsets':remap(freq_it_sets)},indent='\\t'))\n",
        "    f.close()\n",
        "\n",
        "\"\"\"\n",
        "  Utility to clean output of analysis. Take [{freq_0_it_set},{freq_1_it_set}...] and return {freq_non_empty_it_set}\n",
        "\"\"\"\n",
        "def clean_output(freq_it_sets):\n",
        "  if not freq_it_sets[-1]:\n",
        "    #remove the last element if empty\n",
        "    freq_it_sets=freq_it_sets[:-1]\n",
        "  #remove empty itemset set\n",
        "  freq_it_sets=freq_it_sets[1:]\n",
        "  fitsets=dict()\n",
        "\n",
        "  #create a single dict with frequent itemsets and their occurences\n",
        "  for fis in freq_it_sets:\n",
        "    for k,v in fis.items():\n",
        "      fitsets[k]=v\n",
        "\n",
        "  return fitsets"
      ],
      "id": "wDz4lLPYfJMc",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ixo8_SLkdPOd"
      },
      "source": [
        "A-priori algorithm implementation"
      ],
      "id": "Ixo8_SLkdPOd"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYkGZQBq4t9f"
      },
      "source": [
        "\"\"\"\n",
        "  Filter candidate set of itemsets according to suppport threshold \n",
        "\"\"\"\n",
        "@log_filter\n",
        "def filter_ck(candidates,s):\n",
        "  return {k:v for k,v in candidates.items() if v>=s}\n",
        "\n",
        "\"\"\"\n",
        "  Discard unfrequent singletons from a basket\n",
        "\"\"\"\n",
        "def freq_sing(freq_it_sets,basket):\n",
        "  return [word for word in basket if (word,) in freq_it_sets[1]]\n",
        "\n",
        "\"\"\"\n",
        "  Check monotonicity property\n",
        "  kuple is a possible k-itemset -> all immediate subsets (k-1 itemsets) are frequent itemsets.\n",
        "\"\"\" \n",
        "def check_mono_prop(kuple,k,freq_it_sets):\n",
        "  return all([tuple(sorted(el)) in freq_it_sets[k-1] for el in it.combinations(kuple,r=k-1)])\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "  Return candidate k-itemsets found after a basket_file pass\n",
        "\"\"\"\n",
        "def get_ck(basket_file,k,freq_it_sets):\n",
        "  candidates=dict()\n",
        "\n",
        "  for basket in basket_file:\n",
        "    basket=set(freq_sing(freq_it_sets,basket))\n",
        "              \n",
        "    for kuple in it.combinations(basket,r=k):\n",
        "        #sort tuple in order to avoid duplication caused by taking the same itemset ordered in a different way\n",
        "        kuple=tuple(sorted(kuple))\n",
        "\n",
        "        if check_mono_prop(kuple,k,freq_it_sets):\n",
        "          if kuple not in candidates.keys(): candidates[kuple]=1\n",
        "          else: candidates[kuple]+=1\n",
        "\n",
        "  return candidates\n",
        "\n",
        "\"\"\"\n",
        "  First pass of apriori. Build set of candidates singletons\n",
        "\"\"\"\n",
        "def first_pass(basket_file):\n",
        "  basket_count=0\n",
        "  candidates=dict()\n",
        "\n",
        "  for basket in basket_file:\n",
        "    basket_count+=1\n",
        "\n",
        "    for item in set(basket):\n",
        "      if (item,) not in candidates.keys(): candidates[(item,)]=1\n",
        "      else: candidates[(item,)]+=1\n",
        "  \n",
        "  return candidates,basket_count\n",
        "\n",
        "\"\"\"\n",
        "  Second pass of apriori. Build set of candidates couples\n",
        "\"\"\"\n",
        "def second_pass(basket_file,freq_it_sets):\n",
        "  basket_count=0\n",
        "  candidates=dict()\n",
        "\n",
        "  for basket in basket_file:\n",
        "    basket=set(freq_sing(freq_it_sets,basket))\n",
        "\n",
        "    for couple in it.combinations(basket,r=2):\n",
        "      couple=tuple(sorted(couple))\n",
        "      if couple not in candidates.keys(): candidates[couple]=1\n",
        "      else: candidates[couple]+=1\n",
        "  \n",
        "  return candidates\n",
        "\n",
        "\"\"\"\n",
        "  Loop for counting k-itemsets with k>2\n",
        "\"\"\"\n",
        "def main_loop(basket_file,freq_it_sets,s,max_k=-1):\n",
        "  k=3\n",
        "  #stop when no more frequent itemsets are found or k>max_k\n",
        "  while freq_it_sets[-1] and (max_k<0 or k<=max_k):\n",
        "\n",
        "    #duplicate generator for multiple iterations\n",
        "    basket_file,_bf=it.tee(basket_file,2)\n",
        "    ck=get_ck(basket_file,k,freq_it_sets)\n",
        "    freq_it_sets.append(filter_ck(ck,s))\n",
        "    k+=1\n",
        "    basket_file=_bf\n",
        "\n",
        "\"\"\"\n",
        "  Apriori algorithm iteration\n",
        "\"\"\"\n",
        "@time_it\n",
        "def apriori(basket_file,s=-1,max_k=-1,log=False):\n",
        "\n",
        "    freq_it_sets=[{tuple():1}]\n",
        "    basket_file,_bf=it.tee(basket_file,2)\n",
        "    ck,basket_count=first_pass(_bf)\n",
        "    \n",
        "    #set threshold to the 1% of the total number of baskets\n",
        "    if s<0: s=basket_count//100\n",
        "    freq_it_sets.append(filter_ck(ck,s))\n",
        "\n",
        "    basket_file,_bf=it.tee(basket_file,2)\n",
        "    freq_it_sets.append(filter_ck(second_pass(_bf,freq_it_sets),s))\n",
        "\n",
        "    main_loop(basket_file,freq_it_sets,s,max_k)\n",
        "    freq_it_sets=clean_output(freq_it_sets)\n",
        "    if log:dump_result(\"apriori\",s,basket_count,freq_it_sets)\n",
        "\n",
        "    return freq_it_sets"
      ],
      "id": "bYkGZQBq4t9f",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPimayhEg_Ue"
      },
      "source": [
        "PCY implementation"
      ],
      "id": "wPimayhEg_Ue"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJotqhVRg-sG"
      },
      "source": [
        "!pip install -q bitmap\n",
        "from bitmap import BitMap\n",
        "\n",
        "\"\"\"\n",
        "  PCY first pass. Count items' occurence and build buckets table\n",
        "\"\"\"\n",
        "def pcy_first_pass(basket_file,bm_size):\n",
        "  basket_count=0\n",
        "  candidates=dict()\n",
        "  buckets=[0 for i in range(bm_size)]\n",
        "  \n",
        "  for basket in basket_file:\n",
        "    basket_count+=1\n",
        "\n",
        "    for item in set(basket):\n",
        "      if (item,) not in candidates.keys(): candidates[(item,)]=1\n",
        "      else: candidates[(item,)]+=1\n",
        "\n",
        "    #PCY variant: during the first pass hash couples to buckets\n",
        "    for couple in it.combinations(set(basket),r=2):\n",
        "      buckets[( hash(tuple(sorted(couple))) ) %bm_size]+=1\n",
        "  \n",
        "  return candidates,basket_count,buckets\n",
        "\n",
        "\n",
        "def pcy_second_pass(basket_file,freq_it_sets,bm):\n",
        "  basket_count=0\n",
        "  candidates=dict()\n",
        "\n",
        "  for basket in basket_file:\n",
        "    basket=set(freq_sing(freq_it_sets,basket))\n",
        "\n",
        "    for couple in it.combinations(basket,r=2):\n",
        "      couple=tuple(sorted(couple))\n",
        "      #PCY variant: added constraint for couple -> must hash to a frequent bucket\n",
        "      if bm[( hash(tuple(sorted(couple))) ) %bm.size()]:\n",
        "        if couple not in candidates.keys(): candidates[couple]=1\n",
        "        else: candidates[couple]+=1\n",
        "  \n",
        "  return candidates\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "  PCY algorithm iteration\n",
        "\"\"\"\n",
        "@time_it\n",
        "def pcy(basket_file,s=-1,max_k=-1,bm_size=256,log=False):\n",
        "    freq_it_sets=[{tuple():1}]\n",
        "    bm=BitMap(bm_size)\n",
        "\n",
        "    freq_it_sets=[{tuple():1}]\n",
        "    basket_file,_bf=it.tee(basket_file,2)\n",
        "    ck,basket_count,buckets=pcy_first_pass(_bf,bm.size())\n",
        "    #set threshold to the 1% of the total number of baskets\n",
        "    if s<0: s=basket_count//100\n",
        "    freq_it_sets.append(filter_ck(ck,s))\n",
        "    \n",
        "    #PCY variant:set bit of frequent buckets in the bitmap for couples\n",
        "    for i in range(len(buckets)):\n",
        "      if buckets[i]>=s: bm.set(i)\n",
        "    \n",
        "    basket_file,_bf=it.tee(basket_file,2)\n",
        "    freq_it_sets.append(filter_ck(pcy_second_pass(_bf,freq_it_sets,bm),s))\n",
        "\n",
        "    main_loop(basket_file,freq_it_sets,s,max_k)\n",
        "    freq_it_sets=clean_output(freq_it_sets)\n",
        "    if log:dump_result(\"apriori\",s,basket_count,freq_it_sets)\n",
        "\n",
        "    return freq_it_sets"
      ],
      "id": "rJotqhVRg-sG",
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__dNse1upw90",
        "outputId": "9e6e1345-82d7-4a19-9e79-779f2edad584",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!sudo apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://www-eu.apache.org/dist/spark/spark-2.4.8/spark-2.4.8-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.8-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark"
      ],
      "id": "__dNse1upw90",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 2.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XudEqySGpqJA"
      },
      "source": [
        "import findspark\n",
        "\n",
        "os.environ['JAVA_HOME']='/usr/lib/jvm/java-8-openjdk-amd64'\n",
        "os.environ['SPARK_HOME']='spark-2.4.8-bin-hadoop2.7'\n",
        "\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "import pyspark\n",
        "\n",
        "spark=SparkSession.builder.master(\"local[*]\")..config(\"spark.driver.memory\", \"15g\").getOrCreate()\n",
        "sc=spark.sparkContext"
      ],
      "id": "XudEqySGpqJA",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rfaSeNjpybP"
      },
      "source": [
        "\"\"\"\n",
        "  Utilities to convert from dict{k:v,...} to list[(k,v)] and viceversa for pyspark compliance\n",
        "\"\"\"\n",
        "def dict_to_list(itemsets_dict):\n",
        "  return [(k,v) for k,v in itemsets_dict.items()]\n",
        "\n",
        "def list_to_dict(itemsets_list):\n",
        "  itemsets_dict=dict()\n",
        "  for itemset in itemsets_list:\n",
        "    if itemset[0] not in itemsets_dict: \n",
        "      itemsets_dict[itemset[0]]=itemset[1]\n",
        "\n",
        "  return itemsets_dict\n",
        "\n",
        "\"\"\"\n",
        "  This is the second map function: count occurence of candidates in the chunks \n",
        "\"\"\"\n",
        "def count_occurence(chunk,candidates):\n",
        "  ck=dict()\n",
        "  basket_count=0\n",
        "\n",
        "  for basket in chunk:\n",
        "    basket=set(basket)\n",
        "    basket_count+=1\n",
        "    for candidate in candidates:\n",
        "      if set(candidate[0]).issubset(basket):\n",
        "        if candidate[0] not in ck: ck[candidate[0]]=1\n",
        "        else: ck[candidate[0]]+=1\n",
        "\n",
        "  return dict_to_list(ck)\n",
        "\n",
        "@time_it\n",
        "def SON(sc,basket_file,s,num_p=2,max_k=-1):\n",
        "  \n",
        "  basket_file_rdd=sc.parallelize(basket_file,num_p)\n",
        "  freq_in_chunks=basket_file_rdd.mapPartitions(lambda chunk: dict_to_list(apriori(chunk,int((1/num_p)*s),max_k)))\n",
        "\n",
        "  first_map=freq_in_chunks.map(lambda fis: (fis[0],1))\n",
        "  first_reduce=first_map.reduceByKey(lambda el1,el2: 1)\n",
        "  candidates=first_reduce.collect()\n",
        "\n",
        "  second_map=basket_file_rdd.mapPartitions(lambda chunk:count_occurence(chunk,candidates))\n",
        "\n",
        "  return list_to_dict(second_map.reduceByKey(lambda el1,el2: el1+el2).filter(lambda el: el[1]>=s).collect())\n"
      ],
      "id": "7rfaSeNjpybP",
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfCOEYFIht9c"
      },
      "source": [
        "Toivonen algorithm implementation"
      ],
      "id": "qfCOEYFIht9c"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3ZvvKieh0WL"
      },
      "source": [
        "from math import ceil,floor\n",
        "\n",
        "\"\"\"\n",
        "  Utility to build negative border efficiently.\n",
        "  Return all k-itemsets made from k-1 frequent itemsets found in sample (only for k>=2)\n",
        "\"\"\"\n",
        "def nb_candidates(freq_in_sample,k):\n",
        "  immediate_subs=[itemset for itemset in freq_in_sample if len(itemset)==k-1]\n",
        "  self_join=[set(kuple[0])|set(kuple[1]) for kuple in it.product(immediate_subs,immediate_subs)]\n",
        "\n",
        "  return {tuple(sorted(itemset)) for itemset in self_join if len(itemset)==k}\n",
        "\n",
        "\"\"\"\n",
        "  Function that builds the negative border\n",
        "\"\"\"\n",
        "def build_neg_border(freq_in_sample,all_items):\n",
        "  neg_border=set()\n",
        "  max_k=len(max(freq_in_sample,key=lambda el:len(el)))+1 if freq_in_sample else 1\n",
        "\n",
        "  for k in range(1,max_k+1):\n",
        "    #add singletons not found in sample\n",
        "    if k==1:\n",
        "      for singleton in all_items:\n",
        "        if (singleton,) not in freq_in_sample:\n",
        "          neg_border|={(singleton,)}\n",
        "    else:\n",
        "      for itemset in nb_candidates(freq_in_sample,k):\n",
        "        if itemset not in freq_in_sample:\n",
        "          neg_border|={itemset}\n",
        "\n",
        "\n",
        "  return neg_border\n",
        "\n",
        "\n",
        "@time_it\n",
        "def toivonen(basket_file,s=-1,p=0.25,scaling=0.9,max_k=-1,log=False):\n",
        "  negative_border=set()\n",
        "  #freq_it_sets:the k-1-th element is the set of frequent itemsets made of k elements\n",
        "  freq_it_sets=dict()\n",
        "  basket_file,_bf=it.tee(basket_file,2)\n",
        "\n",
        "  sample,basket_count,all_items=get_rand_sample(_bf,p)\n",
        "  if s<0: s=basket_count//100\n",
        "  \n",
        "  ps=floor(scaling*p*s)\n",
        "\n",
        "  #keeping only itemsets and not their counters\n",
        "  freq_in_sample={k for k in apriori(sample,s=ps,max_k=max_k).keys()}\n",
        "\n",
        "  neg_border=build_neg_border(freq_in_sample,all_items)\n",
        "\n",
        "  max_k=len(max(neg_border,key=lambda el:len(el))) if max_k<0 else max_k\n",
        "\n",
        "  #add all the items not present in the sample to build the complete negative border\n",
        "  ck=freq_in_sample | neg_border\n",
        "\n",
        "  #total pass of toivonen algorithm\n",
        "  for basket in basket_file:\n",
        "    basket=set(basket)\n",
        "    for k in range(1,max_k+1):\n",
        "      for kuple in it.combinations(basket,r=k):\n",
        "        kuple=tuple(sorted(kuple))\n",
        "\n",
        "        if kuple in ck:\n",
        "          if kuple not in freq_it_sets:\n",
        "            freq_it_sets[kuple]=1\n",
        "          else:\n",
        "            freq_it_sets[kuple]+=1\n",
        "\n",
        "  #filter according to threshold\n",
        "  freq_it_sets=filter_ck(freq_it_sets,s)\n",
        "  #check that no elements of the negative border are frequent in the sample\n",
        "  if not [itemset for itemset in neg_border if itemset in freq_it_sets]:\n",
        "    if log:dump_result(\"toivonen\",s,basket_count,freq_it_sets)\n",
        "    return freq_it_sets\n",
        "  else:\n",
        "    return None\n",
        "\n"
      ],
      "id": "b3ZvvKieh0WL",
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOAV6G-5LO2X"
      },
      "source": [
        "Validate implementation by comparing it with apyori's implementation on a small size dataset"
      ],
      "id": "BOAV6G-5LO2X"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCZd-SdJTOhb",
        "outputId": "0c35aeaa-896b-4da7-d3bc-ed3904d1efe2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "create_txt_dataset(['Italian'],300)"
      ],
      "id": "jCZd-SdJTOhb",
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset italian300.txt was created with 300 baskets with 54 items on average\n",
            "------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtL-veBk1moc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "917b5475-a3ed-4a3f-b996-c6d8a0bb7bbb"
      },
      "source": [
        "!pip install apyori\n",
        "import apyori as ap\n",
        "from functools import partial\n",
        "from random import randint\n",
        "\n",
        "s_thresh=70\n",
        "apriori_res=apriori(iter_baskets_from_txt('italian300.txt'),s=s_thresh,max_k=3)\n",
        "pcy_res=pcy(iter_baskets_from_txt('italian300.txt'),s=s_thresh,max_k=3,bm_size=10000)\n",
        "son_res=SON(sc,iter_baskets_from_txt('italian300.txt'),s=s_thresh,max_k=3)\n",
        "toivonen_res=toivonen(iter_baskets_from_txt('italian300.txt'),s=s_thresh,p=0.5,scaling=0.8,max_k=3)\n",
        "\n",
        "bf=[i for i in iter_baskets_from_txt('italian300.txt')]\n",
        "basket_count=len(bf)\n",
        "\n",
        "start=time.time()\n",
        "apyori_res=list(ap.apriori(bf,min_support=s_thresh/basket_count,max_length=3))\n",
        "stop=time.time()\n",
        "\n",
        "print('\\n'+'-'*30)\n",
        "print('Function apyori.apriori executed in {0} seconds'.format(stop-start))\n",
        "print('-'*30+'\\n')\n",
        "\n",
        "#TEST\n",
        "def test(target,tested,basket_count):\n",
        "  failed=False\n",
        "\n",
        "  if len(target)!=len(tested): failed=True\n",
        "  test_itemsets={tuple(sorted(i.items)):i.support for i in target}\n",
        "\n",
        "  for k,v in tested.items():\n",
        "    if k not in test_itemsets:\n",
        "      failed=True\n",
        "      break\n",
        "    #apyori lib compute support as itemset_count/basket_count\n",
        "    elif test_itemsets[k]!=v/basket_count:\n",
        "      failed=True\n",
        "      break\n",
        "\n",
        "  if failed:\n",
        "    print(\"-\"*30+\"TEST FAILED\"+\"-\"*30)\n",
        "  else:\n",
        "    print(\"-\"*30+\"TEST PASSED\"+\"-\"*30)\n",
        "\n",
        "test(apyori_res,apriori_res,basket_count)\n",
        "test(apyori_res,pcy_res,basket_count)\n",
        "test(apyori_res,son_res,basket_count)\n",
        "if toivonen_res is not None:\n",
        "  test(apyori_res,toivonen_res,basket_count)\n",
        "else:\n",
        "  print('Toivonen didn\\'t find any frequent itemset')\n"
      ],
      "id": "mtL-veBk1moc",
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: apyori in /usr/local/lib/python3.7/dist-packages (1.1.2)\n",
            "Total number of candidate 1-itemsets: 6439\n",
            "Size in GB: 0.000295\n",
            "Number of frequent 1-itemsets: 24\n",
            "Size in GB: 0.000001\n",
            "------------------------------\n",
            "\n",
            "Total number of candidate 2-itemsets: 276\n",
            "Size in GB: 0.000009\n",
            "Number of frequent 2-itemsets: 130\n",
            "Size in GB: 0.000005\n",
            "------------------------------\n",
            "\n",
            "Total number of candidate 3-itemsets: 419\n",
            "Size in GB: 0.000019\n",
            "Number of frequent 3-itemsets: 249\n",
            "Size in GB: 0.000009\n",
            "------------------------------\n",
            "\n",
            "\n",
            "------------------------------\n",
            "Function apriori executed in 0.3157198429107666 seconds\n",
            "------------------------------\n",
            "\n",
            "Total number of candidate 1-itemsets: 6439\n",
            "Size in GB: 0.000295\n",
            "Number of frequent 1-itemsets: 24\n",
            "Size in GB: 0.000001\n",
            "------------------------------\n",
            "\n",
            "Total number of candidate 2-itemsets: 276\n",
            "Size in GB: 0.000009\n",
            "Number of frequent 2-itemsets: 130\n",
            "Size in GB: 0.000005\n",
            "------------------------------\n",
            "\n",
            "Total number of candidate 3-itemsets: 419\n",
            "Size in GB: 0.000019\n",
            "Number of frequent 3-itemsets: 249\n",
            "Size in GB: 0.000009\n",
            "------------------------------\n",
            "\n",
            "\n",
            "------------------------------\n",
            "Function pcy executed in 0.8081908226013184 seconds\n",
            "------------------------------\n",
            "\n",
            "\n",
            "------------------------------\n",
            "Function SON executed in 0.7664666175842285 seconds\n",
            "------------------------------\n",
            "\n",
            "Total number of candidate 1-itemsets: 4251\n",
            "Size in GB: 0.000148\n",
            "Number of frequent 1-itemsets: 32\n",
            "Size in GB: 0.000001\n",
            "------------------------------\n",
            "\n",
            "Total number of candidate 2-itemsets: 496\n",
            "Size in GB: 0.000019\n",
            "Number of frequent 2-itemsets: 247\n",
            "Size in GB: 0.000009\n",
            "------------------------------\n",
            "\n",
            "Total number of candidate 3-itemsets: 1216\n",
            "Size in GB: 0.000037\n",
            "Number of frequent 3-itemsets: 823\n",
            "Size in GB: 0.000037\n",
            "------------------------------\n",
            "\n",
            "\n",
            "------------------------------\n",
            "Function apriori executed in 0.37915921211242676 seconds\n",
            "------------------------------\n",
            "\n",
            "Total number of candidate 1-itemsets: 9303\n",
            "Size in GB: 0.000295\n",
            "Number of frequent 1-itemsets: 403\n",
            "Size in GB: 0.000019\n",
            "------------------------------\n",
            "\n",
            "\n",
            "------------------------------\n",
            "Function toivonen executed in 12.762188196182251 seconds\n",
            "------------------------------\n",
            "\n",
            "\n",
            "------------------------------\n",
            "Function apyori.apriori executed in 0.057694435119628906 seconds\n",
            "------------------------------\n",
            "\n",
            "------------------------------TEST PASSED------------------------------\n",
            "------------------------------TEST PASSED------------------------------\n",
            "------------------------------TEST PASSED------------------------------\n",
            "------------------------------TEST PASSED------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlgMk0-1sZVi"
      },
      "source": [
        "Experiment: \n",
        "\n",
        "*   build a dataset with english newspapers (baskets)\n",
        "*   consider 3 different orders of magnitude (10^3,10^5,10^6) for the number of baskets to analyze\n",
        "*   Evaluate performance of the 3 algorithms"
      ],
      "id": "nlgMk0-1sZVi"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86af2QLusWUd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42d31c20-1baa-43ef-9777-f6949ec25e58"
      },
      "source": [
        "create_txt_dataset(['English'],1000,remove_stopwords=True,max_items_per_basket=15)\n",
        "create_txt_dataset(['English'],100000,remove_stopwords=True,max_items_per_basket=15)\n",
        "create_txt_dataset(['English'],1000000,remove_stopwords=True,max_items_per_basket=15)"
      ],
      "id": "86af2QLusWUd",
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset english999.txt was created with 999 baskets with 11 items on average\n",
            "------------------------------\n",
            "Dataset english99944.txt was created with 99944 baskets with 11 items on average\n",
            "------------------------------\n",
            "Dataset english999318.txt was created with 999318 baskets with 11 items on average\n",
            "------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "fvqYt_987RXv",
        "outputId": "a823106a-4d82-4923-f8a8-d5ec83d88fa5"
      },
      "source": [
        "basket_files=['english999.txt','english99944.txt','english999318.txt']\n",
        "\n",
        "for basket_file in basket_files:\n",
        "\n",
        "  bf=[i for i in iter_baskets_from_txt(basket_file)]\n",
        "  basket_count=len(bf)\n",
        "  s_thresh=basket_count//100\n",
        "\n",
        "  apriori_res=apriori(iter_baskets_from_txt(basket_file),s=s_thresh,max_k=2)\n",
        "  pcy_res=pcy(iter_baskets_from_txt(basket_file),s=s_thresh,bm_size=4096,max_k=2)\n",
        "  son_res=SON(sc,iter_baskets_from_txt(basket_file),s=s_thresh,max_k=2)\n",
        "  toivonen_res=toivonen(iter_baskets_from_txt(basket_file),s=s_thresh,p=0.4,scaling=0.8,max_k=2)\n",
        "\n",
        "  start=time.time()\n",
        "  apyori_res=list(ap.apriori(bf,min_support=s_thresh/basket_count,max_length=2))\n",
        "  stop=time.time()\n",
        "  print('\\n'+'-'*30)\n",
        "  print('Function apyori.apriori executed in {0} seconds'.format(stop-start))\n",
        "  print('-'*30+'\\n')\n",
        "\n",
        "  print('\\n'+'*'*30)\n",
        "  print('Test on basket file {0}'.format(basket_file))\n",
        "  print('*'*30+'\\n')\n",
        "\n",
        "  print('Apriori results')\n",
        "  test(apyori_res,apriori_res,basket_count)\n",
        "  print('PCY results')\n",
        "  test(apyori_res,pcy_res,basket_count)\n",
        "  print('SON results')\n",
        "  test(apyori_res,son_res,basket_count)\n",
        "  print('Toivonen results')\n",
        "  if toivonen_res is not None:\n",
        "    test(apyori_res,toivonen_res,basket_count)\n",
        "  else:\n",
        "    print('Toivonen didn\\'t find any frequent itemset')"
      ],
      "id": "fvqYt_987RXv",
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of candidate 1-itemsets: 5592\n",
            "Size in GB: 0.000295\n",
            "Number of frequent 1-itemsets: 176\n",
            "Size in GB: 0.000009\n",
            "------------------------------\n",
            "\n",
            "Total number of candidate 2-itemsets: 3390\n",
            "Size in GB: 0.000148\n",
            "Number of frequent 2-itemsets: 6\n",
            "Size in GB: 0.000000\n",
            "------------------------------\n",
            "\n",
            "\n",
            "------------------------------\n",
            "Function apriori executed in 0.022962331771850586 seconds\n",
            "------------------------------\n",
            "\n",
            "Total number of candidate 1-itemsets: 5592\n",
            "Size in GB: 0.000295\n",
            "Number of frequent 1-itemsets: 176\n",
            "Size in GB: 0.000009\n",
            "------------------------------\n",
            "\n",
            "Total number of candidate 2-itemsets: 3367\n",
            "Size in GB: 0.000148\n",
            "Number of frequent 2-itemsets: 6\n",
            "Size in GB: 0.000000\n",
            "------------------------------\n",
            "\n",
            "\n",
            "------------------------------\n",
            "Function pcy executed in 0.08972811698913574 seconds\n",
            "------------------------------\n",
            "\n",
            "\n",
            "------------------------------\n",
            "Function SON executed in 0.46269965171813965 seconds\n",
            "------------------------------\n",
            "\n",
            "Total number of candidate 1-itemsets: 2963\n",
            "Size in GB: 0.000148\n",
            "Number of frequent 1-itemsets: 787\n",
            "Size in GB: 0.000037\n",
            "------------------------------\n",
            "\n",
            "Total number of candidate 2-itemsets: 9038\n",
            "Size in GB: 0.000295\n",
            "Number of frequent 2-itemsets: 299\n",
            "Size in GB: 0.000009\n",
            "------------------------------\n",
            "\n",
            "\n",
            "------------------------------\n",
            "Function apriori executed in 0.017335176467895508 seconds\n",
            "------------------------------\n",
            "\n",
            "Total number of candidate 1-itemsets: 20211\n",
            "Size in GB: 0.000590\n",
            "Number of frequent 1-itemsets: 182\n",
            "Size in GB: 0.000009\n",
            "------------------------------\n",
            "\n",
            "\n",
            "------------------------------\n",
            "Function toivonen executed in 3.1649179458618164 seconds\n",
            "------------------------------\n",
            "\n",
            "\n",
            "------------------------------\n",
            "Function apyori.apriori executed in 0.06492853164672852 seconds\n",
            "------------------------------\n",
            "\n",
            "\n",
            "******************************\n",
            "Test on basket file english999.txt\n",
            "******************************\n",
            "\n",
            "Apriori results\n",
            "------------------------------TEST PASSED------------------------------\n",
            "PCY results\n",
            "------------------------------TEST PASSED------------------------------\n",
            "SON results\n",
            "------------------------------TEST PASSED------------------------------\n",
            "Toivonen results\n",
            "Toivonen didn't find any frequent itemset\n",
            "Total number of candidate 1-itemsets: 66684\n",
            "Size in GB: 0.002622\n",
            "Number of frequent 1-itemsets: 104\n",
            "Size in GB: 0.000005\n",
            "------------------------------\n",
            "\n",
            "Total number of candidate 2-itemsets: 5353\n",
            "Size in GB: 0.000148\n",
            "Number of frequent 2-itemsets: 1\n",
            "Size in GB: 0.000000\n",
            "------------------------------\n",
            "\n",
            "\n",
            "------------------------------\n",
            "Function apriori executed in 2.0694563388824463 seconds\n",
            "------------------------------\n",
            "\n",
            "Total number of candidate 1-itemsets: 66684\n",
            "Size in GB: 0.002622\n",
            "Number of frequent 1-itemsets: 104\n",
            "Size in GB: 0.000005\n",
            "------------------------------\n",
            "\n",
            "Total number of candidate 2-itemsets: 5353\n",
            "Size in GB: 0.000148\n",
            "Number of frequent 2-itemsets: 1\n",
            "Size in GB: 0.000000\n",
            "------------------------------\n",
            "\n",
            "\n",
            "------------------------------\n",
            "Function pcy executed in 8.068054676055908 seconds\n",
            "------------------------------\n",
            "\n",
            "\n",
            "------------------------------\n",
            "Function SON executed in 6.053183317184448 seconds\n",
            "------------------------------\n",
            "\n",
            "Total number of candidate 1-itemsets: 43766\n",
            "Size in GB: 0.002622\n",
            "Number of frequent 1-itemsets: 168\n",
            "Size in GB: 0.000005\n",
            "------------------------------\n",
            "\n",
            "Total number of candidate 2-itemsets: 13638\n",
            "Size in GB: 0.000590\n",
            "Number of frequent 2-itemsets: 5\n",
            "Size in GB: 0.000000\n",
            "------------------------------\n",
            "\n",
            "\n",
            "------------------------------\n",
            "Function apriori executed in 0.7590091228485107 seconds\n",
            "------------------------------\n",
            "\n",
            "Total number of candidate 1-itemsets: 80682\n",
            "Size in GB: 0.002622\n",
            "Number of frequent 1-itemsets: 105\n",
            "Size in GB: 0.000005\n",
            "------------------------------\n",
            "\n",
            "\n",
            "------------------------------\n",
            "Function toivonen executed in 8.838477611541748 seconds\n",
            "------------------------------\n",
            "\n",
            "\n",
            "------------------------------\n",
            "Function apyori.apriori executed in 2.3234658241271973 seconds\n",
            "------------------------------\n",
            "\n",
            "\n",
            "******************************\n",
            "Test on basket file english99944.txt\n",
            "******************************\n",
            "\n",
            "Apriori results\n",
            "------------------------------TEST PASSED------------------------------\n",
            "PCY results\n",
            "------------------------------TEST PASSED------------------------------\n",
            "SON results\n",
            "------------------------------TEST PASSED------------------------------\n",
            "Toivonen results\n",
            "------------------------------TEST PASSED------------------------------\n",
            "Total number of candidate 1-itemsets: 175628\n",
            "Size in GB: 0.010486\n",
            "Number of frequent 1-itemsets: 105\n",
            "Size in GB: 0.000005\n",
            "------------------------------\n",
            "\n",
            "Total number of candidate 2-itemsets: 5460\n",
            "Size in GB: 0.000148\n",
            "Number of frequent 2-itemsets: 1\n",
            "Size in GB: 0.000000\n",
            "------------------------------\n",
            "\n",
            "\n",
            "------------------------------\n",
            "Function apriori executed in 22.689648866653442 seconds\n",
            "------------------------------\n",
            "\n",
            "Total number of candidate 1-itemsets: 175628\n",
            "Size in GB: 0.010486\n",
            "Number of frequent 1-itemsets: 105\n",
            "Size in GB: 0.000005\n",
            "------------------------------\n",
            "\n",
            "Total number of candidate 2-itemsets: 5460\n",
            "Size in GB: 0.000148\n",
            "Number of frequent 2-itemsets: 1\n",
            "Size in GB: 0.000000\n",
            "------------------------------\n",
            "\n",
            "\n",
            "------------------------------\n",
            "Function pcy executed in 84.08617162704468 seconds\n",
            "------------------------------\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "Py4JJavaError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-92-2e9c8d060c59>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0mapriori_res\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mapriori\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter_baskets_from_txt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbasket_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ms_thresh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0mpcy_res\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpcy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter_baskets_from_txt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbasket_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ms_thresh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbm_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4096\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m   \u001b[0mson_res\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSON\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0miter_baskets_from_txt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbasket_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ms_thresh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m   \u001b[0mtoivonen_res\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoivonen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter_baskets_from_txt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbasket_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ms_thresh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscaling\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-c999fd35cb6d>\u001b[0m in \u001b[0;36m_wrap\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_wrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mres\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-82-eceb71fed496>\u001b[0m in \u001b[0;36mSON\u001b[0;34m(sc, basket_file, s, num_p, max_k)\u001b[0m\n\u001b[1;32m     32\u001b[0m   \u001b[0mfirst_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfreq_in_chunks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mfis\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfis\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m   \u001b[0mfirst_reduce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfirst_map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduceByKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mel1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mel2\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m   \u001b[0mcandidates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfirst_reduce\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m   \u001b[0msecond_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbasket_file_rdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mcount_occurence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcandidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-2.4.8-bin-hadoop2.7/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    814\u001b[0m         \"\"\"\n\u001b[1;32m    815\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 816\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    817\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mspark-2.4.8-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-2.4.8-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mspark-2.4.8-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 116.0 failed 1 times, most recent failure: Lost task 0.0 in stage 116.0 (TID 248, localhost, executor driver): java.lang.OutOfMemoryError: Java heap space\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1925)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1913)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1912)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1912)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:948)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:948)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:948)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2146)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2095)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2084)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:759)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2067)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2088)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2107)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2132)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:990)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:989)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.OutOfMemoryError: Java heap space\n"
          ]
        }
      ]
    }
  ]
}