{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "frequent_itemsets.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/turatig/frequent_itemsets/blob/master/frequent_itemsets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J00vwdMle-Nx"
      },
      "source": [
        "**MARKET BASKET ANALYSIS NOTEBOOK**"
      ],
      "id": "J00vwdMle-Nx"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOVdeW-efZ3z"
      },
      "source": [
        "Dowload and preprocess dataset"
      ],
      "id": "LOVdeW-efZ3z"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22ba4125",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51b1c085-293e-4e3f-98ae-c3f4e8f4bbd3"
      },
      "source": [
        "!pip install kaggle\n",
        "\n",
        "\n",
        "import os,sys,time,zipfile,json,re\n",
        "import functools as ft\n",
        "import itertools as it\n",
        "from datetime import datetime as dt\n",
        "from random import uniform\n",
        "\n",
        "os.environ['KAGGLE_USERNAME']='giacomoturati1'\n",
        "os.environ['KAGGLE_KEY']='7d34a1aefc3558065164b70c24ce27ed'\n",
        "\n",
        "from kaggle.api.kaggle_api_extended import KaggleApi\n",
        "\n",
        "def get_dataset():\n",
        "\n",
        "  #execute only if the dataset was not already downloaded\n",
        "  if 'old-newspaper.tsv' not in os.listdir():\n",
        "    api=KaggleApi()\n",
        "    api.authenticate()\n",
        "\n",
        "    api.dataset_download_file('alvations/old-newspapers','old-newspaper.tsv')\n",
        "\n",
        "    with zipfile.ZipFile('old-newspaper.tsv.zip','r') as _zip:\n",
        "      _zip.extractall()\n",
        "\n",
        "#Yield baskets (list of lists) reading from tsv\n",
        "#languages: subset of languages to be considered during the market-basket analysis\n",
        "def iter_baskets_from_tsv(languages=None,max_basket=-1,skip=0):\n",
        "  count=0\n",
        "  with open('old-newspaper.tsv','r') as f:\n",
        "    #skip header line\n",
        "    next(f)\n",
        "    for line in f:\n",
        "      l=line.split('\\t')\n",
        "      if languages is not None and l[0] not in languages: continue\n",
        "\n",
        "      #get a list of words as basket skipping any sequence of non-alphabetical characters\n",
        "      basket=re.split(r'[^a-zA-Z]+',l[3])\n",
        "      #remove any empty string\n",
        "      basket=[word.lower() for word in basket if word!='']\n",
        "      if basket:\n",
        "        count+=1\n",
        "        if count>skip: yield basket\n",
        "        if max_basket>0 and count-skip>=max_basket: break \n",
        "      \n",
        "    f.close()\n",
        "\n",
        "#Create txt dataset. Structure: every line (basket) is a sequence of words (items) separated from commas\n",
        "def create_txt_dataset(languages=None,max_basket=-1,skip=0):\n",
        "  baskets=[]\n",
        "\n",
        "  #execute only if the dataset was not already created\n",
        "  for line in iter_baskets_from_tsv(languages,max_basket,skip=0):\n",
        "    baskets.append(ft.reduce(lambda w1,w2: w1+','+w2,line))\n",
        "\n",
        "  filename=ft.reduce(lambda i,j:i+'_'+j,languages).lower() if languages is not None else 'all_languages' \n",
        "  filename+=str(len(baskets))+'.txt'\n",
        "\n",
        "  with open(filename,'w') as f:\n",
        "    for basket in baskets:\n",
        "      f.write(basket+'\\n')\n",
        "\n",
        "    f.close()\n",
        "\n",
        "#Yield baskets from json structures as array of arrays\n",
        "def iter_baskets_from_txt(filename,max_basket=-1,skip=0):\n",
        "  theres_next=True\n",
        "  basket=[]\n",
        "  count=0\n",
        "\n",
        "  with open(filename,'r') as f:\n",
        "    eol=''\n",
        "    for line in f:\n",
        "      line=line.split(',')\n",
        "      count+=1\n",
        "      #trim \\n\n",
        "      if count>skip: yield line[:-1]+[line[-1][:-1]]\n",
        "      if max_basket>0 and count-skip>=max_basket: break \n",
        "    \n",
        "    f.close()\n",
        "\n",
        "#Scan the basket file and extract a basket with fixed probability p\n",
        "def get_rand_sample(basket_file,p):\n",
        "  basket_count=0\n",
        "  sample=[]\n",
        "  all_items=set()\n",
        "\n",
        "  for basket in basket_file:\n",
        "    basket_count+=1\n",
        "    if uniform(0,1)<=p:\n",
        "      sample.append(basket)\n",
        "\n",
        "    all_items|=set(basket)\n",
        "  \n",
        "  return sample,basket_count,all_items\n",
        "\n",
        "\n",
        "\n",
        "get_dataset()"
      ],
      "id": "22ba4125",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (5.0.2)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2020.12.5)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.41.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (2.10)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle) (1.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TqnHG1Eppe9F"
      },
      "source": [
        "Utilities to log the execution"
      ],
      "id": "TqnHG1Eppe9F"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDz4lLPYfJMc"
      },
      "source": [
        "def sizeof_GB(obj): return \"%f\"%(sys.getsizeof(obj)/1000000000)\n",
        "\n",
        "#decorator to log time execution of function/method\n",
        "def time_it(f):\n",
        "  def _wrap(*args,**kwargs):\n",
        "    start=time.time()\n",
        "    res=f(*args,**kwargs)\n",
        "    stop=time.time()\n",
        "    \n",
        "    print('\\n'+'-'*30)\n",
        "    print('Function {0} executed in {1} seconds'.format(f.__name__,stop-start))\n",
        "    print('-'*30+'\\n')\n",
        "    return res\n",
        "  return _wrap\n",
        "\n",
        "#decorator to log the memory space used before and after a candidate itemset filtering operation\n",
        "def log_filter(f):\n",
        "  def _wrap(*args,**kwargs):\n",
        "    if len(args)>0:\n",
        "      if args[0]:\n",
        "        print('Total number of candidate {0}-itemsets: {1}\\nSize in GB: {2}'.\\\n",
        "            format(len(list(args[0].keys())[0]),len(args[0]),sizeof_GB(args[0])))\n",
        "      else:\n",
        "        print('No more candidates were found')\n",
        "      \n",
        "    #argument was given by key=value\n",
        "    else:\n",
        "      if kwargs['candidates']:\n",
        "        print('Total number of candidate {0}-itemsets: {1}\\nSize in GB: {2}'.\\\n",
        "            format(len(list(kwargs['candidates'].keys())[0]),len(kwargs['candidates']),sizeof_GB(kwargs['candidates'])))\n",
        "      else:\n",
        "        print('No more candidates were found')\n",
        "    \n",
        "    res=f(*args,**kwargs)\n",
        "\n",
        "    if res:\n",
        "      print('Number of frequent {0}-itemsets: {1}\\nSize in GB: {2}'.\\\n",
        "              format(len(list(res.keys())[0]),len(res),sizeof_GB(res)))\n",
        "    else:\n",
        "      print('No frequent itemset were found')\n",
        "    print('-'*30+'\\n')\n",
        "    return res\n",
        "\n",
        "  return _wrap\n",
        "\n",
        "#Dump on json file the result of an algorithm run\n",
        "def dump_result(algo,s,basket_count,freq_it_sets):\n",
        "  def remap(dic):\n",
        "    return {str(k):v for k,v in dic.items()}\n",
        "\n",
        "  header_info={'support_threshold':s,'total_n_baskets':basket_count}\n",
        "\n",
        "  filename=algo+'_market_basket_analysis_'+str(dt.today())[:10]+'_'+str(dt.today())[11:]+'.json'\n",
        "              \n",
        "  with open(filename,'w') as f:\n",
        "    f.write(json.dumps({'header':header_info,'frequent itemsets':remap(freq_it_sets)},indent='\\t'))\n",
        "    f.close()"
      ],
      "id": "wDz4lLPYfJMc",
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ixo8_SLkdPOd"
      },
      "source": [
        "A-priori algorithm implementation"
      ],
      "id": "Ixo8_SLkdPOd"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-0BQUe4z29F"
      },
      "source": [
        "\"\"\"\n",
        "  Filter candidate set of itemsets according to suppport threshold \n",
        "\"\"\"\n",
        "@log_filter\n",
        "def filter_ck(candidates,s):\n",
        "  return {k:v for k,v in candidates.items() if v>=s}\n",
        "\n",
        "\"\"\"\n",
        "  Discard unfrequent singletons from a basket\n",
        "\"\"\"\n",
        "def freq_sing(freq_it_sets,basket):\n",
        "  return [word for word in basket if (word,) in freq_it_sets[1]]\n",
        "\n",
        "\"\"\"\n",
        "  Check monotonicity property\n",
        "  kuple is a possible k-itemset -> all immediate subsets (k-1 itemsets) are frequent itemsets.\n",
        "\"\"\" \n",
        "def check_mono_prop(kuple,k,freq_it_sets):\n",
        "  return all([tuple(sorted(el)) in freq_it_sets[k-1] for el in it.combinations(kuple,r=k-1)])\n",
        "\n",
        "\"\"\"\n",
        "  Utility to clean output of analysis. Take [{freq_0_it_set},{freq_1_it_set}...] and return {freq_non_empty_it_set}\n",
        "\"\"\"\n",
        "def clean_output(freq_it_sets):\n",
        "  if not freq_it_sets[-1]:\n",
        "    #remove the last element if empty\n",
        "    freq_it_sets=freq_it_sets[:-1]\n",
        "  #remove empty itemset set\n",
        "  freq_it_sets=freq_it_sets[1:]\n",
        "  fitsets=dict()\n",
        "\n",
        "  #create a single dict with frequent itemsets and their occurences\n",
        "  for fis in freq_it_sets:\n",
        "    for k,v in fis.items():\n",
        "      fitsets[k]=v\n",
        "\n",
        "  return fitsets\n",
        "\n",
        "\"\"\"\n",
        "  Return candidate k-itemsets found after a basket_file pass\n",
        "\"\"\"\n",
        "def apr_get_ck(basket_file,k,freq_it_sets):\n",
        "  basket_count=0\n",
        "  candidates=dict()\n",
        "\n",
        "  for basket in basket_file:\n",
        "    basket_count+=1\n",
        "    if k>2:\n",
        "      basket=freq_sing(freq_it_sets,basket)\n",
        "    \n",
        "    #discard duplicates\n",
        "    basket=set(basket)\n",
        "              \n",
        "    for kuple in it.combinations(basket,r=k):\n",
        "        #sort tuple in order to avoid duplication caused by taking the same itemset ordered in a different way\n",
        "        kuple=tuple(sorted(kuple))\n",
        "\n",
        "        if check_mono_prop(kuple,k,freq_it_sets):\n",
        "          if kuple not in candidates.keys(): candidates[kuple]=1\n",
        "          else: candidates[kuple]+=1\n",
        "\n",
        "  return candidates,basket_count\n",
        "\n",
        "\"\"\"\n",
        "  Apriori algorithm iteration\n",
        "\"\"\"\n",
        "@time_it\n",
        "def apriori(basket_file,s=-1,max_k=-1,log=False):\n",
        "    freq_it_sets=[{tuple():1}]\n",
        "    k=1\n",
        "\n",
        "    #stop when no more frequent itemsets are found or k>max_k\n",
        "    while freq_it_sets[-1] and (max_k<0 or k<=max_k):\n",
        "\n",
        "      #duplicate generator for multiple iterations\n",
        "      basket_file,_bf=it.tee(basket_file,2)\n",
        "      ck,basket_count=apr_get_ck(basket_file,k,freq_it_sets)\n",
        "      if s<0:\n",
        "        #set threshold to the 1% of the total number of baskets\n",
        "        s=basket_count//100\n",
        "      freq_it_sets.append(filter_ck(ck,s))\n",
        "      k+=1\n",
        "      basket_file=_bf\n",
        "\n",
        "    freq_it_sets=clean_output(freq_it_sets)\n",
        "    if log:dump_result(\"apriori\",s,basket_count,freq_it_sets)\n",
        "\n",
        "    return freq_it_sets\n",
        "    "
      ],
      "id": "P-0BQUe4z29F",
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPimayhEg_Ue"
      },
      "source": [
        "PCY implementation"
      ],
      "id": "wPimayhEg_Ue"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8YqFThkXhL4l"
      },
      "source": [
        "!pip install -q bitmap\n",
        "from bitmap import BitMap\n",
        "\n",
        "#map a tuple to a bucket of a table of size=s\n",
        "def hash_tuple(t,s): return hash(t)%s\n",
        "def set_all(bm):\n",
        "  for i in bm.size():\n",
        "    bm.set(i)\n"
      ],
      "id": "8YqFThkXhL4l",
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJotqhVRg-sG"
      },
      "source": [
        "\"\"\"\n",
        "  Return candidate k-itemsets found after a basket_file pass.\n",
        "  bm: bitmap of frequent buckets of couples\n",
        "\"\"\" \n",
        "def pcy_get_ck(basket_file,k,freq_it_sets,bm,max_basket=-1,skip=0):\n",
        "  basket_count=0\n",
        "  candidates=dict()\n",
        "  buckets=[0 for i in range(bm.size())]\n",
        "\n",
        "  for basket in basket_file:\n",
        "    basket_count+=1\n",
        "    if k>2:\n",
        "      basket=freq_sing(freq_it_sets,basket)\n",
        "    \n",
        "    basket=set(basket)\n",
        "              \n",
        "    for kuple in it.combinations(basket,r=k):\n",
        "        kuple=tuple(sorted(kuple))\n",
        "\n",
        "        #PCY variant: added constraint for couple -> must hash to a frequent bucket\n",
        "        if check_mono_prop(kuple,k,freq_it_sets) and (k!=2 or bm[hash_tuple(kuple,bm.size())]):\n",
        "          if kuple not in candidates.keys(): candidates[kuple]=1\n",
        "          else: candidates[kuple]+=1\n",
        "        \n",
        "    if k==1:\n",
        "      #PCY variant: during the first pass hash couples to buckets\n",
        "      for couple in it.combinations(basket,r=2):\n",
        "          buckets[hash_tuple(tuple(sorted(couple)),bm.size())]+=1\n",
        "\n",
        "  return candidates,basket_count,buckets\n",
        "\n",
        "\"\"\"\n",
        "  PCY algorithm iteration\n",
        "\"\"\"\n",
        "@time_it\n",
        "def pcy(basket_file,s=-1,max_k=-1,bm_size=256,log=False):\n",
        "    freq_it_sets=[{tuple():1}]\n",
        "    bm=BitMap(bm_size)\n",
        "    k=1\n",
        "\n",
        "    while freq_it_sets[-1] and (max_k<0 or k<=max_k):\n",
        "      \n",
        "      basket_file,_bf=it.tee(basket_file,2)\n",
        "      ck,basket_count,buckets=pcy_get_ck(basket_file,k,freq_it_sets,bm)\n",
        "      if s<0: s=basket_count//100\n",
        "      freq_it_sets.append(filter_ck(ck,s))\n",
        "\n",
        "      if k==1:\n",
        "        #PCY variant:set bit of frequent buckets in the bitmap for couples\n",
        "        for i in range(len(buckets)):\n",
        "          if buckets[i]>=s: bm.set(i)\n",
        "      k+=1\n",
        "      basket_file=_bf\n",
        "    \n",
        "    freq_it_sets=clean_output(freq_it_sets)\n",
        "    if log:dump_result(\"pcy\",s,basket_count,freq_it_sets)\n",
        "    \n",
        "    return freq_it_sets\n"
      ],
      "id": "rJotqhVRg-sG",
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfCOEYFIht9c"
      },
      "source": [
        "Toivonen algorithm implementation"
      ],
      "id": "qfCOEYFIht9c"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3ZvvKieh0WL"
      },
      "source": [
        "from math import ceil,floor\n",
        "\n",
        "\"\"\"\n",
        "  Function that builds the negative border\n",
        "\"\"\"\n",
        "def build_neg_border(freq_it_sets,all_items,max_k):\n",
        "  neg_border=set()\n",
        "\n",
        "  for k in range(1,max_k+1):\n",
        "    for kuple in it.combinations(all_items,r=k):\n",
        "      kuple=tuple(sorted(kuple))\n",
        "\n",
        "      #check if itemset is frequent in the sample\n",
        "      if kuple not in freq_it_sets:\n",
        "        #check if all immediate subsets are frequent in the sample\n",
        "        if k==1 or all([tuple(sorted(itemset)) in freq_it_sets for itemset in it.combinations(kuple,r=k-1)]):\n",
        "          neg_border|={kuple}\n",
        "\n",
        "    return neg_border\n",
        "\n",
        "@time_it\n",
        "def toivonen(basket_file,s=-1,p=0.25,scaling=0.9,max_k=-1,log=False):\n",
        "  negative_border=set()\n",
        "  #freq_it_sets:the k-1-th element is the set of frequent itemsets made of k elements\n",
        "  freq_it_sets=dict()\n",
        "  basket_file,_bf=it.tee(basket_file,2)\n",
        "\n",
        "  sample,basket_count,all_items=get_rand_sample(_bf,p)\n",
        "  if s<0: s=basket_count//100\n",
        "  \n",
        "  ps=floor(scaling*p*s)\n",
        "\n",
        "  #keeping only itemsets and not their counters\n",
        "  freq_in_sample={k for k in apriori(sample,s=ps,max_k=max_k).keys()}\n",
        "  #max itemset size to check\n",
        "  #if k is the max itemset size found in sample, there's no point in checking itemset of size>k+1\n",
        "  max_its_size=len(max(freq_in_sample,key=lambda itemset: len(itemset)))\n",
        "  max_its_size=max_its_size if max_its_size==max_k else max_its_size+1\n",
        "\n",
        "  neg_border=build_neg_border(freq_in_sample,all_items,max_its_size)\n",
        "\n",
        "  #add all the items not present in the sample to build the complete negative border\n",
        "  ck=freq_in_sample | neg_border\n",
        "\n",
        "  for basket in basket_file:\n",
        "    basket=set(basket)\n",
        "    for k in range(1,max_its_size+1):\n",
        "      for kuple in it.combinations(basket,r=k):\n",
        "        kuple=tuple(sorted(kuple))\n",
        "\n",
        "        if kuple in ck:\n",
        "          if kuple not in freq_it_sets:\n",
        "            freq_it_sets[kuple]=1\n",
        "          else:\n",
        "            freq_it_sets[kuple]+=1\n",
        "\n",
        "  #filter according to threshold\n",
        "  freq_it_sets=filter_ck(freq_it_sets,s)\n",
        "  #check that no elements of the negative border are frequent in the sample\n",
        "  if not [itemset for itemset in neg_border if itemset in freq_it_sets]:\n",
        "    if log:dump_result(\"toivonen\",s,basket_count,freq_it_sets)\n",
        "    return freq_it_sets\n",
        "  else:\n",
        "    return None\n",
        "\n"
      ],
      "id": "b3ZvvKieh0WL",
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOAV6G-5LO2X"
      },
      "source": [
        "Validate implementation by comparing it with apyori's implementation on a small size dataset"
      ],
      "id": "BOAV6G-5LO2X"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtL-veBk1moc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62a5f354-27bf-4bd4-8245-1d7decdb4b13"
      },
      "source": [
        "!pip install apyori\n",
        "import apyori as ap\n",
        "from functools import partial\n",
        "from random import randint\n",
        "\n",
        "create_txt_dataset(['Italian'],300)\n",
        "\n",
        "s_thresh=randint(30,100)\n",
        "apriori_res=apriori(iter_baskets_from_txt('italian300.txt'),s=s_thresh,max_k=3)\n",
        "pcy_res=pcy(iter_baskets_from_txt('italian300.txt'),s=s_thresh,max_k=3)\n",
        "toivonen_res=toivonen(iter_baskets_from_txt('italian300.txt'),s=s_thresh,p=uniform(0.4,0.7),scaling=uniform(0.7,0.9),max_k=3)\n",
        "\n",
        "bf=[i for i in iter_baskets_from_txt('italian300.txt')]\n",
        "basket_count=len(bf)\n",
        "\n",
        "start=time.time()\n",
        "#here support is computed as itemset_count/basket_count\n",
        "apyori_res=list(ap.apriori(bf,min_support=s_thresh/basket_count,max_length=3))\n",
        "stop=time.time()\n",
        "\n",
        "print('\\n'+'-'*30)\n",
        "print('Function apyori.apriori executed in {0} seconds'.format(stop-start))\n",
        "print('-'*30+'\\n')\n",
        "\n",
        "#TEST\n",
        "def test(target,tested,basket_count):\n",
        "  failed=False\n",
        "\n",
        "  if len(target)!=len(tested): failed=True\n",
        "  test_itemsets={tuple(sorted(i.items)):i.support for i in target}\n",
        "\n",
        "  for k,v in tested.items():\n",
        "    if k not in test_itemsets:\n",
        "      failed=True\n",
        "      break\n",
        "    elif test_itemsets[k]!=v/basket_count:\n",
        "      failed=True\n",
        "      break\n",
        "\n",
        "  if failed:\n",
        "    print(\"-\"*30+\"TEST FAILED\"+\"-\"*30)\n",
        "  else:\n",
        "    print(\"-\"*30+\"TEST PASSED\"+\"-\"*30)\n",
        "\n",
        "test(apyori_res,apriori_res,basket_count)\n",
        "test(apyori_res,pcy_res,basket_count)\n",
        "if toivonen_res is not None:\n",
        "  test(apyori_res,toivonen_res,basket_count)\n"
      ],
      "id": "mtL-veBk1moc",
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: apyori in /usr/local/lib/python3.7/dist-packages (1.1.2)\n",
            "Total number of candidate 1-itemsets: 6439\n",
            "Size in GB: 0.000295\n",
            "\n",
            "------------------------------\n",
            "Function filter_ck executed in 0.0003409385681152344 seconds\n",
            "------------------------------\n",
            "\n",
            "Number of frequent 1-itemsets: 30\n",
            "Size in GB: 0.000001\n",
            "------------------------------\n",
            "\n",
            "Total number of candidate 2-itemsets: 435\n",
            "Size in GB: 0.000019\n",
            "\n",
            "------------------------------\n",
            "Function filter_ck executed in 5.1975250244140625e-05 seconds\n",
            "------------------------------\n",
            "\n",
            "Number of frequent 2-itemsets: 220\n",
            "Size in GB: 0.000009\n",
            "------------------------------\n",
            "\n",
            "Total number of candidate 3-itemsets: 992\n",
            "Size in GB: 0.000037\n",
            "\n",
            "------------------------------\n",
            "Function filter_ck executed in 0.00011897087097167969 seconds\n",
            "------------------------------\n",
            "\n",
            "Number of frequent 3-itemsets: 611\n",
            "Size in GB: 0.000019\n",
            "------------------------------\n",
            "\n",
            "\n",
            "------------------------------\n",
            "Function apriori executed in 1.4765605926513672 seconds\n",
            "------------------------------\n",
            "\n",
            "Total number of candidate 1-itemsets: 6439\n",
            "Size in GB: 0.000295\n",
            "\n",
            "------------------------------\n",
            "Function filter_ck executed in 0.0003139972686767578 seconds\n",
            "------------------------------\n",
            "\n",
            "Number of frequent 1-itemsets: 30\n",
            "Size in GB: 0.000001\n",
            "------------------------------\n",
            "\n",
            "Total number of candidate 2-itemsets: 435\n",
            "Size in GB: 0.000019\n",
            "\n",
            "------------------------------\n",
            "Function filter_ck executed in 4.935264587402344e-05 seconds\n",
            "------------------------------\n",
            "\n",
            "Number of frequent 2-itemsets: 220\n",
            "Size in GB: 0.000009\n",
            "------------------------------\n",
            "\n",
            "Total number of candidate 3-itemsets: 992\n",
            "Size in GB: 0.000037\n",
            "\n",
            "------------------------------\n",
            "Function filter_ck executed in 0.00011444091796875 seconds\n",
            "------------------------------\n",
            "\n",
            "Number of frequent 3-itemsets: 611\n",
            "Size in GB: 0.000019\n",
            "------------------------------\n",
            "\n",
            "\n",
            "------------------------------\n",
            "Function pcy executed in 2.042693614959717 seconds\n",
            "------------------------------\n",
            "\n",
            "Total number of candidate 1-itemsets: 4008\n",
            "Size in GB: 0.000148\n",
            "\n",
            "------------------------------\n",
            "Function filter_ck executed in 0.00019979476928710938 seconds\n",
            "------------------------------\n",
            "\n",
            "Number of frequent 1-itemsets: 40\n",
            "Size in GB: 0.000001\n",
            "------------------------------\n",
            "\n",
            "Total number of candidate 2-itemsets: 780\n",
            "Size in GB: 0.000037\n",
            "\n",
            "------------------------------\n",
            "Function filter_ck executed in 7.987022399902344e-05 seconds\n",
            "------------------------------\n",
            "\n",
            "Number of frequent 2-itemsets: 313\n",
            "Size in GB: 0.000009\n",
            "------------------------------\n",
            "\n",
            "Total number of candidate 3-itemsets: 1632\n",
            "Size in GB: 0.000074\n",
            "\n",
            "------------------------------\n",
            "Function filter_ck executed in 0.0001983642578125 seconds\n",
            "------------------------------\n",
            "\n",
            "Number of frequent 3-itemsets: 1085\n",
            "Size in GB: 0.000037\n",
            "------------------------------\n",
            "\n",
            "\n",
            "------------------------------\n",
            "Function apriori executed in 0.9989378452301025 seconds\n",
            "------------------------------\n",
            "\n",
            "Total number of candidate 1-itemsets: 7837\n",
            "Size in GB: 0.000295\n",
            "\n",
            "------------------------------\n",
            "Function filter_ck executed in 0.0005865097045898438 seconds\n",
            "------------------------------\n",
            "\n",
            "Number of frequent 1-itemsets: 859\n",
            "Size in GB: 0.000037\n",
            "------------------------------\n",
            "\n",
            "\n",
            "------------------------------\n",
            "Function toivonen executed in 8.508574724197388 seconds\n",
            "------------------------------\n",
            "\n",
            "\n",
            "------------------------------\n",
            "Function apyori.apriori executed in 0.07523703575134277 seconds\n",
            "------------------------------\n",
            "\n",
            "------------------------------TEST PASSED------------------------------\n",
            "------------------------------TEST PASSED------------------------------\n",
            "------------------------------TEST FAILED------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlgMk0-1sZVi"
      },
      "source": [
        "Experiment: \n",
        "\n",
        "*   build a dataset with english newspapers (baskets)\n",
        "*   consider 3 different orders of magnitude (10^3,10^5,10^6) for the number of baskets to analyze\n",
        "*   Evaluate performance of the 3 algorithms"
      ],
      "id": "nlgMk0-1sZVi"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86af2QLusWUd"
      },
      "source": [
        "create_txt_dataset(['English'],1000)\n",
        "create_txt_dataset(['English'],100000)\n",
        "create_txt_dataset(['English'],1000000)"
      ],
      "id": "86af2QLusWUd",
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "fvqYt_987RXv",
        "outputId": "5a3569cd-f7d5-41e1-a65f-e054a83cffa0"
      },
      "source": [
        "basket_files=['english1000.txt','english100000.txt','english1000000.txt']\n",
        "apriori_res=list()\n",
        "pcy_res=list()\n",
        "toivonen_res=list()\n",
        "apyori_res=list()\n",
        "\n",
        "for basket_file in basket_files:\n",
        "\n",
        "  bf=[i for i in iter_baskets_from_txt(basket_file)]\n",
        "  basket_count=len(bf)\n",
        "  s_thresh=10*(basket_count//100)\n",
        "\n",
        "  apriori_res=apriori(iter_baskets_from_txt(basket_file),s=s_thresh)\n",
        "  pcy_res=pcy(iter_baskets_from_txt(basket_file),s=s_thresh)\n",
        "  toivonen_res=toivonen(iter_baskets_from_txt(basket_file),s=s_thresh,p=0.6,scaling=0.8)\n",
        "\n",
        "  start=time.time()\n",
        "  apyori_res=list(ap.apriori(bf,min_support=s_thresh/basket_count))\n",
        "  stop=time.time()\n",
        "  print('\\n'+'-'*30)\n",
        "  print('Function apyori.apriori executed in {0} seconds'.format(stop-start))\n",
        "  print('-'*30+'\\n')\n",
        "\n",
        "  print('\\n'+'*'*30)\n",
        "  print('Test on basket file {0}'.format(basket_file))\n",
        "  print('*'*30+'\\n')\n",
        "\n",
        "  print('Apriori results')\n",
        "  test(apyori_res,apriori_res,basket_count)\n",
        "  print('PCY results')\n",
        "  test(apyori_res,pcy_res,basket_count)\n",
        "  print('Toivonen results')\n",
        "  if toivonen_res is not None:\n",
        "    test(apyori_res,toivonen_res,basket_count)\n",
        "  else:\n",
        "    print('Toivonen didn\\'t find any frequent itemset')"
      ],
      "id": "fvqYt_987RXv",
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of candidate 1-itemsets: 7484\n",
            "Size in GB: 0.000295\n",
            "\n",
            "------------------------------\n",
            "Function filter_ck executed in 0.0006594657897949219 seconds\n",
            "------------------------------\n",
            "\n",
            "Number of frequent 1-itemsets: 29\n",
            "Size in GB: 0.000001\n",
            "------------------------------\n",
            "\n",
            "Total number of candidate 2-itemsets: 406\n",
            "Size in GB: 0.000019\n",
            "\n",
            "------------------------------\n",
            "Function filter_ck executed in 4.172325134277344e-05 seconds\n",
            "------------------------------\n",
            "\n",
            "Number of frequent 2-itemsets: 82\n",
            "Size in GB: 0.000002\n",
            "------------------------------\n",
            "\n",
            "Total number of candidate 3-itemsets: 176\n",
            "Size in GB: 0.000009\n",
            "\n",
            "------------------------------\n",
            "Function filter_ck executed in 3.2901763916015625e-05 seconds\n",
            "------------------------------\n",
            "\n",
            "Number of frequent 3-itemsets: 96\n",
            "Size in GB: 0.000005\n",
            "------------------------------\n",
            "\n",
            "Total number of candidate 4-itemsets: 73\n",
            "Size in GB: 0.000002\n",
            "\n",
            "------------------------------\n",
            "Function filter_ck executed in 1.71661376953125e-05 seconds\n",
            "------------------------------\n",
            "\n",
            "Number of frequent 4-itemsets: 38\n",
            "Size in GB: 0.000001\n",
            "------------------------------\n",
            "\n",
            "Total number of candidate 5-itemsets: 6\n",
            "Size in GB: 0.000000\n",
            "\n",
            "------------------------------\n",
            "Function filter_ck executed in 6.4373016357421875e-06 seconds\n",
            "------------------------------\n",
            "\n",
            "Number of frequent 5-itemsets: 5\n",
            "Size in GB: 0.000000\n",
            "------------------------------\n",
            "\n",
            "No more candidates were found\n",
            "\n",
            "------------------------------\n",
            "Function filter_ck executed in 4.291534423828125e-06 seconds\n",
            "------------------------------\n",
            "\n",
            "Number of frequent 0-itemsets: 0\n",
            "Size in GB: 0\n",
            "------------------------------\n",
            "\n",
            "\n",
            "------------------------------\n",
            "Function apriori executed in 5.079097509384155 seconds\n",
            "------------------------------\n",
            "\n",
            "Total number of candidate 1-itemsets: 7484\n",
            "Size in GB: 0.000295\n",
            "\n",
            "------------------------------\n",
            "Function filter_ck executed in 0.0006029605865478516 seconds\n",
            "------------------------------\n",
            "\n",
            "Number of frequent 1-itemsets: 29\n",
            "Size in GB: 0.000001\n",
            "------------------------------\n",
            "\n",
            "Total number of candidate 2-itemsets: 406\n",
            "Size in GB: 0.000019\n",
            "\n",
            "------------------------------\n",
            "Function filter_ck executed in 5.4836273193359375e-05 seconds\n",
            "------------------------------\n",
            "\n",
            "Number of frequent 2-itemsets: 82\n",
            "Size in GB: 0.000002\n",
            "------------------------------\n",
            "\n",
            "Total number of candidate 3-itemsets: 176\n",
            "Size in GB: 0.000009\n",
            "\n",
            "------------------------------\n",
            "Function filter_ck executed in 2.6941299438476562e-05 seconds\n",
            "------------------------------\n",
            "\n",
            "Number of frequent 3-itemsets: 96\n",
            "Size in GB: 0.000005\n",
            "------------------------------\n",
            "\n",
            "Total number of candidate 4-itemsets: 73\n",
            "Size in GB: 0.000002\n",
            "\n",
            "------------------------------\n",
            "Function filter_ck executed in 1.3589859008789062e-05 seconds\n",
            "------------------------------\n",
            "\n",
            "Number of frequent 4-itemsets: 38\n",
            "Size in GB: 0.000001\n",
            "------------------------------\n",
            "\n",
            "Total number of candidate 5-itemsets: 6\n",
            "Size in GB: 0.000000\n",
            "\n",
            "------------------------------\n",
            "Function filter_ck executed in 4.76837158203125e-06 seconds\n",
            "------------------------------\n",
            "\n",
            "Number of frequent 5-itemsets: 5\n",
            "Size in GB: 0.000000\n",
            "------------------------------\n",
            "\n",
            "No more candidates were found\n",
            "\n",
            "------------------------------\n",
            "Function filter_ck executed in 4.76837158203125e-06 seconds\n",
            "------------------------------\n",
            "\n",
            "Number of frequent 0-itemsets: 0\n",
            "Size in GB: 0\n",
            "------------------------------\n",
            "\n",
            "\n",
            "------------------------------\n",
            "Function pcy executed in 5.498939037322998 seconds\n",
            "------------------------------\n",
            "\n",
            "Total number of candidate 1-itemsets: 5522\n",
            "Size in GB: 0.000295\n",
            "\n",
            "------------------------------\n",
            "Function filter_ck executed in 0.00030612945556640625 seconds\n",
            "------------------------------\n",
            "\n",
            "Number of frequent 1-itemsets: 39\n",
            "Size in GB: 0.000001\n",
            "------------------------------\n",
            "\n",
            "Total number of candidate 2-itemsets: 741\n",
            "Size in GB: 0.000037\n",
            "\n",
            "------------------------------\n",
            "Function filter_ck executed in 7.319450378417969e-05 seconds\n",
            "------------------------------\n",
            "\n",
            "Number of frequent 2-itemsets: 132\n",
            "Size in GB: 0.000005\n",
            "------------------------------\n",
            "\n",
            "Total number of candidate 3-itemsets: 330\n",
            "Size in GB: 0.000009\n",
            "\n",
            "------------------------------\n",
            "Function filter_ck executed in 4.2438507080078125e-05 seconds\n",
            "------------------------------\n",
            "\n",
            "Number of frequent 3-itemsets: 179\n",
            "Size in GB: 0.000009\n",
            "------------------------------\n",
            "\n",
            "Total number of candidate 4-itemsets: 195\n",
            "Size in GB: 0.000009\n",
            "\n",
            "------------------------------\n",
            "Function filter_ck executed in 3.695487976074219e-05 seconds\n",
            "------------------------------\n",
            "\n",
            "Number of frequent 4-itemsets: 111\n",
            "Size in GB: 0.000005\n",
            "------------------------------\n",
            "\n",
            "Total number of candidate 5-itemsets: 38\n",
            "Size in GB: 0.000001\n",
            "\n",
            "------------------------------\n",
            "Function filter_ck executed in 1.0013580322265625e-05 seconds\n",
            "------------------------------\n",
            "\n",
            "Number of frequent 5-itemsets: 26\n",
            "Size in GB: 0.000001\n",
            "------------------------------\n",
            "\n",
            "Total number of candidate 6-itemsets: 1\n",
            "Size in GB: 0.000000\n",
            "\n",
            "------------------------------\n",
            "Function filter_ck executed in 4.0531158447265625e-06 seconds\n",
            "------------------------------\n",
            "\n",
            "Number of frequent 6-itemsets: 1\n",
            "Size in GB: 0.000000\n",
            "------------------------------\n",
            "\n",
            "No more candidates were found\n",
            "\n",
            "------------------------------\n",
            "Function filter_ck executed in 3.337860107421875e-06 seconds\n",
            "------------------------------\n",
            "\n",
            "Number of frequent 0-itemsets: 0\n",
            "Size in GB: 0\n",
            "------------------------------\n",
            "\n",
            "\n",
            "------------------------------\n",
            "Function apriori executed in 12.432908773422241 seconds\n",
            "------------------------------\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-e5936f06c053>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0mapriori_res\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mapriori\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter_baskets_from_txt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbasket_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ms_thresh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m   \u001b[0mpcy_res\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpcy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter_baskets_from_txt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbasket_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ms_thresh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m   \u001b[0mtoivonen_res\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoivonen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter_baskets_from_txt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbasket_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ms_thresh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscaling\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m   \u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-28-c92590165cfc>\u001b[0m in \u001b[0;36m_wrap\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_wrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mres\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-32-8399ec7f0507>\u001b[0m in \u001b[0;36mtoivonen\u001b[0;34m(basket_file, s, p, scaling, max_k, log)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mbasket\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbasket\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_its_size\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m       \u001b[0;32mfor\u001b[0m \u001b[0mkuple\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcombinations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbasket\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0mkuple\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}