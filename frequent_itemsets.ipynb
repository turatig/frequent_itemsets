{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Copia di frequent_itemsets.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/turatig/frequent_itemsets/blob/master/frequent_itemsets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J00vwdMle-Nx"
      },
      "source": [
        "**MARKET BASKET ANALYSIS NOTEBOOK**"
      ],
      "id": "J00vwdMle-Nx"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOVdeW-efZ3z"
      },
      "source": [
        "Dowload and preprocess dataset"
      ],
      "id": "LOVdeW-efZ3z"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22ba4125",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": "OK"
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "9cc781db-07d7-4159-fcf6-414314b887f9"
      },
      "source": [
        "!pip install kaggle\n",
        "\n",
        "\n",
        "import os,sys,time,zipfile,json,re\n",
        "import functools as ft\n",
        "import itertools as it\n",
        "from datetime import datetime as dt\n",
        "from random import uniform,shuffle\n",
        "from nltk.corpus import stopwords\n",
        "import nltk \n",
        "nltk.download('stopwords')\n",
        "\n",
        "#add kaggle crediantials json\n",
        "from google.colab import files\n",
        "\n",
        "files.upload()\n",
        "os.environ['KAGGLE_CONFIG_DIR']='.'\n",
        "\n",
        "from kaggle.api.kaggle_api_extended import KaggleApi\n",
        "\n",
        "def get_dataset():\n",
        "\n",
        "  #execute only if the dataset was not already downloaded\n",
        "  if 'old-newspaper.tsv' not in os.listdir():\n",
        "    api=KaggleApi()\n",
        "    api.authenticate()\n",
        "\n",
        "    api.dataset_download_file('alvations/old-newspapers','old-newspaper.tsv')\n",
        "\n",
        "    with zipfile.ZipFile('old-newspaper.tsv.zip','r') as _zip:\n",
        "      _zip.extractall()\n",
        "\n",
        "#Yield baskets (list of lists) reading from tsv\n",
        "#languages: subset of languages to be considered during the market-basket analysis\n",
        "def iter_baskets_from_tsv(languages=None,max_basket=-1,skip=0):\n",
        "  count=0\n",
        "  with open('old-newspaper.tsv','r') as f:\n",
        "    #skip header line\n",
        "    next(f)\n",
        "    for line in f:\n",
        "      l=line.split('\\t')\n",
        "      if languages is not None and l[0] not in languages: continue\n",
        "\n",
        "      #get a list of words as basket skipping any sequence of non-alphabetical characters\n",
        "      basket=re.split(r'[^a-zA-Z]+',l[3])\n",
        "      #remove any empty string\n",
        "      basket=[word.lower() for word in basket if word!='']\n",
        "      if basket:\n",
        "        count+=1\n",
        "        if count>skip: yield basket\n",
        "        if max_basket>0 and count-skip>=max_basket: break \n",
        "      \n",
        "    f.close()\n",
        "\n",
        "#Create txt dataset. Structure: every line (basket) is a sequence of words (items) separated from commas\n",
        "def create_txt_dataset(languages=None,max_basket=-1,skip=0,remove_stopwords=False,max_items_per_basket=-1):\n",
        "  baskets=[]\n",
        "  stopw=set()\n",
        "  #compute the mean of the number of items per basket\n",
        "  n_items=0\n",
        "\n",
        "  if remove_stopwords and languages:\n",
        "    for lang in languages: stopw|=set(stopwords.words(lang.lower()))\n",
        "\n",
        "  #execute only if the dataset was not already created\n",
        "  for line in iter_baskets_from_tsv(languages,max_basket,skip=0):\n",
        "    if stopw: line=[word for word in line if word not in stopw]\n",
        "    if line:\n",
        "      if max_items_per_basket>0:\n",
        "        #avoid bias on order during the analysis\n",
        "        shuffle(line)\n",
        "        line=line[:max_items_per_basket]\n",
        "      line=set(line)\n",
        "      baskets.append(ft.reduce(lambda w1,w2: w1+','+w2,line))\n",
        "      n_items+=len(line)\n",
        "\n",
        "  filename=ft.reduce(lambda i,j:i+'_'+j,languages).lower() if languages is not None else 'all_languages' \n",
        "  filename+=str(len(baskets))+'.txt'\n",
        "\n",
        "  with open(filename,'w') as f:\n",
        "    for basket in baskets:\n",
        "      f.write(basket+'\\n')\n",
        "\n",
        "    f.close()\n",
        "\n",
        "  print(\"Dataset {0} was created with {1} baskets with {2} items on average\".format(filename,len(baskets),n_items//len(baskets)))\n",
        "  print(\"-\"*30)\n",
        "\n",
        "#Yield baskets from txt files where every line is a basket and every basket is a sequence item1,item2...\n",
        "def iter_baskets_from_txt(filename,max_basket=-1,skip=0):\n",
        "  theres_next=True\n",
        "  basket=[]\n",
        "  count=0\n",
        "\n",
        "  with open(filename,'r') as f:\n",
        "    eol=''\n",
        "    for line in f:\n",
        "      line=line.split(',')\n",
        "      count+=1\n",
        "      #trim \\n\n",
        "      if count>skip: yield line[:-1]+[line[-1][:-1]]\n",
        "      if max_basket>0 and count-skip>=max_basket: break \n",
        "    \n",
        "    f.close()\n",
        "\n",
        "#Scan the basket file and extract a basket with fixed probability p\n",
        "def get_rand_sample(basket_file,p):\n",
        "  basket_count=0\n",
        "  sample=[]\n",
        "  all_items=set()\n",
        "\n",
        "  for basket in basket_file:\n",
        "    basket_count+=1\n",
        "    if uniform(0,1)<=p:\n",
        "      sample.append(basket)\n",
        "\n",
        "    all_items|=set(basket)\n",
        "  \n",
        "  return sample,basket_count,all_items\n",
        "\n",
        "\n",
        "\n",
        "get_dataset()"
      ],
      "id": "22ba4125",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.12)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2020.12.5)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (5.0.2)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.41.1)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (3.0.4)\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-51a7e06f-2a66-4c4b-9fef-4ded6146493f\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-51a7e06f-2a66-4c4b-9fef-4ded6146493f\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle (1).json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TqnHG1Eppe9F"
      },
      "source": [
        "Utilities to log the execution"
      ],
      "id": "TqnHG1Eppe9F"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDz4lLPYfJMc"
      },
      "source": [
        "def sizeof_GB(obj): return \"%f\"%(sys.getsizeof(obj)/1000000000)\n",
        "\n",
        "#decorator to log time execution of function/method\n",
        "def time_it(f):\n",
        "  def _wrap(*args,**kwargs):\n",
        "    start=time.time()\n",
        "    res=f(*args,**kwargs)\n",
        "    stop=time.time()\n",
        "    \n",
        "    print('\\n'+'-'*30)\n",
        "    print('Function {0} executed in {1} seconds'.format(f.__name__,stop-start))\n",
        "    print('-'*30+'\\n')\n",
        "    return res\n",
        "  return _wrap\n",
        "\n",
        "#decorator to log the memory space used before and after a candidate itemset filtering operation\n",
        "def log_filter(f):\n",
        "  def _wrap(*args,**kwargs):\n",
        "    if len(args)>0:\n",
        "      if args[0]:\n",
        "        print('Total number of candidate {0}-itemsets: {1}\\nSize in GB: {2}'.\\\n",
        "            format(len(list(args[0].keys())[0]),len(args[0]),sizeof_GB(args[0])))\n",
        "      else:\n",
        "        print('No more candidates were found')\n",
        "      \n",
        "    #argument was given by key=value\n",
        "    else:\n",
        "      if kwargs['candidates']:\n",
        "        print('Total number of candidate {0}-itemsets: {1}\\nSize in GB: {2}'.\\\n",
        "            format(len(list(kwargs['candidates'].keys())[0]),len(kwargs['candidates']),sizeof_GB(kwargs['candidates'])))\n",
        "      else:\n",
        "        print('No more candidates were found')\n",
        "    \n",
        "    res=f(*args,**kwargs)\n",
        "\n",
        "    if res:\n",
        "      print('Number of frequent {0}-itemsets: {1}\\nSize in GB: {2}'.\\\n",
        "              format(len(list(res.keys())[0]),len(res),sizeof_GB(res)))\n",
        "    else:\n",
        "      print('No frequent itemset were found')\n",
        "    print('-'*30+'\\n')\n",
        "    return res\n",
        "\n",
        "  return _wrap\n",
        "\n",
        "#Dump on json file the result of an algorithm run\n",
        "def dump_result(algo,s,basket_count,freq_it_sets):\n",
        "  def remap(dic):\n",
        "    return {str(k):v for k,v in dic.items()}\n",
        "\n",
        "  header_info={'support_threshold':s,'total_n_baskets':basket_count}\n",
        "\n",
        "  filename=algo+'_market_basket_analysis_'+str(dt.today())[:10]+'_'+str(dt.today())[11:]+'.json'\n",
        "              \n",
        "  with open(filename,'w') as f:\n",
        "    f.write(json.dumps({'header':header_info,'frequent itemsets':remap(freq_it_sets)},indent='\\t'))\n",
        "    f.close()\n",
        "\n",
        "\"\"\"\n",
        "  Utility to clean output of analysis. Take [{freq_0_it_set},{freq_1_it_set}...] and return {freq_non_empty_it_set}\n",
        "\"\"\"\n",
        "def clean_output(freq_it_sets):\n",
        "  if not freq_it_sets[-1]:\n",
        "    #remove the last element if empty\n",
        "    freq_it_sets=freq_it_sets[:-1]\n",
        "  #remove empty itemset set\n",
        "  freq_it_sets=freq_it_sets[1:]\n",
        "  fitsets=dict()\n",
        "\n",
        "  #create a single dict with frequent itemsets and their occurences\n",
        "  for fis in freq_it_sets:\n",
        "    for k,v in fis.items():\n",
        "      fitsets[k]=v\n",
        "\n",
        "  return fitsets"
      ],
      "id": "wDz4lLPYfJMc",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ixo8_SLkdPOd"
      },
      "source": [
        "A-priori algorithm implementation"
      ],
      "id": "Ixo8_SLkdPOd"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYkGZQBq4t9f"
      },
      "source": [
        "\"\"\"\n",
        "  Filter candidate set of itemsets according to suppport threshold \n",
        "\"\"\"\n",
        "@log_filter\n",
        "def filter_ck(candidates,s):\n",
        "  return {k:v for k,v in candidates.items() if v>=s}\n",
        "\n",
        "\"\"\"\n",
        "  Discard unfrequent singletons from a basket\n",
        "\"\"\"\n",
        "def freq_sing(freq_it_sets,basket):\n",
        "  return [word for word in basket if (word,) in freq_it_sets[1]]\n",
        "\n",
        "\"\"\"\n",
        "  Check monotonicity property\n",
        "  kuple is a possible k-itemset -> all immediate subsets (k-1 itemsets) are frequent itemsets.\n",
        "\"\"\" \n",
        "def check_mono_prop(kuple,k,freq_it_sets):\n",
        "  return all([tuple(sorted(el)) in freq_it_sets[k-1] for el in it.combinations(kuple,r=k-1)])\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "  Return candidate k-itemsets found after a basket_file pass\n",
        "\"\"\"\n",
        "def get_ck(basket_file,k,freq_it_sets):\n",
        "  candidates=dict()\n",
        "\n",
        "  for basket in basket_file:\n",
        "    basket=set(freq_sing(freq_it_sets,basket))\n",
        "              \n",
        "    for kuple in it.combinations(basket,r=k):\n",
        "        #sort tuple in order to avoid duplication caused by taking the same itemset ordered in a different way\n",
        "        kuple=tuple(sorted(kuple))\n",
        "\n",
        "        if check_mono_prop(kuple,k,freq_it_sets):\n",
        "          if kuple not in candidates.keys(): candidates[kuple]=1\n",
        "          else: candidates[kuple]+=1\n",
        "\n",
        "  return candidates\n",
        "\n",
        "\"\"\"\n",
        "  First pass of apriori. Build set of candidates singletons\n",
        "\"\"\"\n",
        "def first_pass(basket_file):\n",
        "  basket_count=0\n",
        "  candidates=dict()\n",
        "\n",
        "  for basket in basket_file:\n",
        "    basket_count+=1\n",
        "\n",
        "    for item in set(basket):\n",
        "      if (item,) not in candidates.keys(): candidates[(item,)]=1\n",
        "      else: candidates[(item,)]+=1\n",
        "  \n",
        "  return candidates,basket_count\n",
        "\n",
        "\"\"\"\n",
        "  Second pass of apriori. Build set of candidates couples\n",
        "\"\"\"\n",
        "def second_pass(basket_file,freq_it_sets):\n",
        "  basket_count=0\n",
        "  candidates=dict()\n",
        "\n",
        "  for basket in basket_file:\n",
        "    basket=set(freq_sing(freq_it_sets,basket))\n",
        "\n",
        "    for couple in it.combinations(basket,r=2):\n",
        "      couple=tuple(sorted(couple))\n",
        "      if couple not in candidates.keys(): candidates[couple]=1\n",
        "      else: candidates[couple]+=1\n",
        "  \n",
        "  return candidates\n",
        "\n",
        "\"\"\"\n",
        "  Loop for counting k-itemsets with k>2\n",
        "\"\"\"\n",
        "def main_loop(basket_file,freq_it_sets,s,max_k=-1):\n",
        "  k=3\n",
        "  #stop when no more frequent itemsets are found or k>max_k\n",
        "  while freq_it_sets[-1] and (max_k<0 or k<=max_k):\n",
        "\n",
        "    #duplicate generator for multiple iterations\n",
        "    basket_file,_bf=it.tee(basket_file,2)\n",
        "    ck=get_ck(basket_file,k,freq_it_sets)\n",
        "    freq_it_sets.append(filter_ck(ck,s))\n",
        "    k+=1\n",
        "    basket_file=_bf\n",
        "\n",
        "\"\"\"\n",
        "  Apriori algorithm iteration\n",
        "\"\"\"\n",
        "@time_it\n",
        "def apriori(basket_file,s=-1,max_k=-1,log=False):\n",
        "\n",
        "    freq_it_sets=[{tuple():1}]\n",
        "    basket_file,_bf=it.tee(basket_file,2)\n",
        "    ck,basket_count=first_pass(_bf)\n",
        "    \n",
        "    #set threshold to the 1% of the total number of baskets\n",
        "    if s<0: s=basket_count//100\n",
        "    freq_it_sets.append(filter_ck(ck,s))\n",
        "\n",
        "    basket_file,_bf=it.tee(basket_file,2)\n",
        "    freq_it_sets.append(filter_ck(second_pass(_bf,freq_it_sets),s))\n",
        "\n",
        "    main_loop(basket_file,freq_it_sets,s,max_k)\n",
        "    freq_it_sets=clean_output(freq_it_sets)\n",
        "    if log:dump_result(\"apriori\",s,basket_count,freq_it_sets)\n",
        "\n",
        "    return freq_it_sets"
      ],
      "id": "bYkGZQBq4t9f",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPimayhEg_Ue"
      },
      "source": [
        "PCY implementation"
      ],
      "id": "wPimayhEg_Ue"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJotqhVRg-sG"
      },
      "source": [
        "!pip install -q bitmap\n",
        "from bitmap import BitMap\n",
        "\n",
        "\"\"\"\n",
        "  PCY first pass. Count items' occurence and build buckets table\n",
        "\"\"\"\n",
        "def pcy_first_pass(basket_file,bm_size):\n",
        "  basket_count=0\n",
        "  candidates=dict()\n",
        "  buckets=[0 for i in range(bm_size)]\n",
        "  \n",
        "  for basket in basket_file:\n",
        "    basket_count+=1\n",
        "\n",
        "    for item in set(basket):\n",
        "      if (item,) not in candidates.keys(): candidates[(item,)]=1\n",
        "      else: candidates[(item,)]+=1\n",
        "\n",
        "    #PCY variant: during the first pass hash couples to buckets\n",
        "    for couple in it.combinations(set(basket),r=2):\n",
        "      buckets[( hash(tuple(sorted(couple))) ) %bm_size]+=1\n",
        "  \n",
        "  return candidates,basket_count,buckets\n",
        "\n",
        "\n",
        "def pcy_second_pass(basket_file,freq_it_sets,bm):\n",
        "  basket_count=0\n",
        "  candidates=dict()\n",
        "\n",
        "  for basket in basket_file:\n",
        "    basket=set(freq_sing(freq_it_sets,basket))\n",
        "\n",
        "    for couple in it.combinations(basket,r=2):\n",
        "      couple=tuple(sorted(couple))\n",
        "      #PCY variant: added constraint for couple -> must hash to a frequent bucket\n",
        "      if bm[( hash(tuple(sorted(couple))) ) %bm.size()]:\n",
        "        if couple not in candidates.keys(): candidates[couple]=1\n",
        "        else: candidates[couple]+=1\n",
        "  \n",
        "  return candidates\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "  PCY algorithm iteration\n",
        "\"\"\"\n",
        "@time_it\n",
        "def pcy(basket_file,s=-1,max_k=-1,bm_size=256,log=False):\n",
        "    freq_it_sets=[{tuple():1}]\n",
        "    bm=BitMap(bm_size)\n",
        "\n",
        "    freq_it_sets=[{tuple():1}]\n",
        "    basket_file,_bf=it.tee(basket_file,2)\n",
        "    ck,basket_count,buckets=pcy_first_pass(_bf,bm.size())\n",
        "    #set threshold to the 1% of the total number of baskets\n",
        "    if s<0: s=basket_count//100\n",
        "    freq_it_sets.append(filter_ck(ck,s))\n",
        "    \n",
        "    #PCY variant:set bit of frequent buckets in the bitmap for couples\n",
        "    for i in range(len(buckets)):\n",
        "      if buckets[i]>=s: bm.set(i)\n",
        "    \n",
        "    basket_file,_bf=it.tee(basket_file,2)\n",
        "    freq_it_sets.append(filter_ck(pcy_second_pass(_bf,freq_it_sets,bm),s))\n",
        "\n",
        "    main_loop(basket_file,freq_it_sets,s,max_k)\n",
        "    freq_it_sets=clean_output(freq_it_sets)\n",
        "    if log:dump_result(\"apriori\",s,basket_count,freq_it_sets)\n",
        "\n",
        "    return freq_it_sets"
      ],
      "id": "rJotqhVRg-sG",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_Yn4TgAVQsO"
      },
      "source": [
        "SON algorithm implementation"
      ],
      "id": "o_Yn4TgAVQsO"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__dNse1upw90"
      },
      "source": [
        "!sudo apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://www-eu.apache.org/dist/spark/spark-2.4.8/spark-2.4.8-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.8-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark"
      ],
      "id": "__dNse1upw90",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XudEqySGpqJA"
      },
      "source": [
        "import findspark\n",
        "\n",
        "os.environ['JAVA_HOME']='/usr/lib/jvm/java-8-openjdk-amd64'\n",
        "os.environ['SPARK_HOME']='spark-2.4.8-bin-hadoop2.7'\n",
        "\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "import pyspark\n",
        "\n",
        "spark=SparkSession.builder.master(\"local[*]\").config(\"spark.driver.memory\", \"15g\").getOrCreate()\n",
        "sc=spark.sparkContext"
      ],
      "id": "XudEqySGpqJA",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rfaSeNjpybP"
      },
      "source": [
        "\"\"\"\n",
        "  Utilities to convert from dict{k:v,...} to list[(k,v)] and viceversa for pyspark compliance\n",
        "\"\"\"\n",
        "def dict_to_list(itemsets_dict):\n",
        "  return [(k,v) for k,v in itemsets_dict.items()]\n",
        "\n",
        "def list_to_dict(itemsets_list):\n",
        "  itemsets_dict=dict()\n",
        "  for itemset in itemsets_list:\n",
        "    if itemset[0] not in itemsets_dict: \n",
        "      itemsets_dict[itemset[0]]=itemset[1]\n",
        "\n",
        "  return itemsets_dict\n",
        "\n",
        "\"\"\"\n",
        "  This is the second map function: count occurence of candidates in the chunks \n",
        "\"\"\"\n",
        "def count_occurence(chunk,candidates):\n",
        "  ck=dict()\n",
        "\n",
        "  for basket in chunk:\n",
        "    basket=set(basket)\n",
        "    for candidate in candidates:\n",
        "      if set(candidate[0]).issubset(basket):\n",
        "        if candidate[0] not in ck: ck[candidate[0]]=1\n",
        "        else: ck[candidate[0]]+=1\n",
        "\n",
        "  return dict_to_list(ck)\n",
        "\n",
        "@time_it\n",
        "def SON(sc,basket_file,s,num_p=2,max_k=-1):\n",
        "  \n",
        "  basket_file_rdd=sc.parallelize(basket_file,num_p)\n",
        "  freq_in_chunks=basket_file_rdd.mapPartitions(lambda chunk: dict_to_list(apriori(chunk,int((1/num_p)*s),max_k)))\n",
        "\n",
        "  first_map=freq_in_chunks.map(lambda fis: (fis[0],1))\n",
        "  first_reduce=first_map.reduceByKey(lambda el1,el2: 1)\n",
        "  candidates=first_reduce.collect()\n",
        "\n",
        "  second_map=basket_file_rdd.mapPartitions(lambda chunk:count_occurence(chunk,candidates))\n",
        "\n",
        "  return list_to_dict(second_map.reduceByKey(lambda el1,el2: el1+el2).filter(lambda el: el[1]>=s).collect())\n"
      ],
      "id": "7rfaSeNjpybP",
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfCOEYFIht9c"
      },
      "source": [
        "Toivonen algorithm implementation"
      ],
      "id": "qfCOEYFIht9c"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3ZvvKieh0WL"
      },
      "source": [
        "from math import ceil,floor\n",
        "\n",
        "\"\"\"\n",
        "  Utility to build negative border efficiently.\n",
        "  Return all k-itemsets made from k-1 frequent itemsets found in sample (only for k>=2)\n",
        "\"\"\"\n",
        "def nb_candidates(freq_in_sample,k):\n",
        "  immediate_subs=[itemset for itemset in freq_in_sample if len(itemset)==k-1]\n",
        "  self_join=[set(kuple[0])|set(kuple[1]) for kuple in it.product(immediate_subs,immediate_subs)]\n",
        "\n",
        "  return {tuple(sorted(itemset)) for itemset in self_join if len(itemset)==k}\n",
        "\n",
        "\"\"\"\n",
        "  Function that builds the negative border\n",
        "\"\"\"\n",
        "def build_neg_border(freq_in_sample,all_items):\n",
        "  neg_border=set()\n",
        "  max_k=len(max(freq_in_sample,key=lambda el:len(el)))+1 if freq_in_sample else 1\n",
        "\n",
        "  for k in range(1,max_k+1):\n",
        "    #add singletons not found in sample\n",
        "    if k==1:\n",
        "      for singleton in all_items:\n",
        "        if (singleton,) not in freq_in_sample:\n",
        "          neg_border|={(singleton,)}\n",
        "    else:\n",
        "      for itemset in nb_candidates(freq_in_sample,k):\n",
        "        if itemset not in freq_in_sample:\n",
        "          neg_border|={itemset}\n",
        "\n",
        "\n",
        "  return neg_border\n",
        "\n",
        "\n",
        "@time_it\n",
        "def toivonen(basket_file,s=-1,p=0.25,scaling=0.9,max_k=-1,log=False):\n",
        "  negative_border=set()\n",
        "  #freq_it_sets:the k-1-th element is the set of frequent itemsets made of k elements\n",
        "  freq_it_sets=dict()\n",
        "  basket_file,_bf=it.tee(basket_file,2)\n",
        "\n",
        "  sample,basket_count,all_items=get_rand_sample(_bf,p)\n",
        "  if s<0: s=basket_count//100\n",
        "  \n",
        "  ps=floor(scaling*p*s)\n",
        "\n",
        "  #keeping only itemsets and not their counters\n",
        "  freq_in_sample={k for k in apriori(sample,s=ps,max_k=max_k).keys()}\n",
        "\n",
        "  neg_border=build_neg_border(freq_in_sample,all_items)\n",
        "\n",
        "  max_k=len(max(neg_border,key=lambda el:len(el))) if max_k<0 else max_k\n",
        "\n",
        "  #add all the items not present in the sample to build the complete negative border\n",
        "  ck=freq_in_sample | neg_border\n",
        "\n",
        "  #total pass of toivonen algorithm\n",
        "  for basket in basket_file:\n",
        "    basket=set(basket)\n",
        "    for k in range(1,max_k+1):\n",
        "      for kuple in it.combinations(basket,r=k):\n",
        "        kuple=tuple(sorted(kuple))\n",
        "\n",
        "        if kuple in ck:\n",
        "          if kuple not in freq_it_sets:\n",
        "            freq_it_sets[kuple]=1\n",
        "          else:\n",
        "            freq_it_sets[kuple]+=1\n",
        "\n",
        "  #filter according to threshold\n",
        "  freq_it_sets=filter_ck(freq_it_sets,s)\n",
        "  #check that no elements of the negative border are frequent in the sample\n",
        "  if not [itemset for itemset in neg_border if itemset in freq_it_sets]:\n",
        "    if log:dump_result(\"toivonen\",s,basket_count,freq_it_sets)\n",
        "    return freq_it_sets\n",
        "  else:\n",
        "    return None\n",
        "\n"
      ],
      "id": "b3ZvvKieh0WL",
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOAV6G-5LO2X"
      },
      "source": [
        "Validate implementation by comparing it with apyori's implementation on a small size dataset"
      ],
      "id": "BOAV6G-5LO2X"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCZd-SdJTOhb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20b53c61-383a-494a-e150-37f5bef1c09b"
      },
      "source": [
        "create_txt_dataset(['Italian'],300)"
      ],
      "id": "jCZd-SdJTOhb",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset italian300.txt was created with 300 baskets with 54 items on average\n",
            "------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtL-veBk1moc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5c79088-aa1f-4964-9a69-a3cc4e65b9fc"
      },
      "source": [
        "!pip install apyori\n",
        "import apyori as ap\n",
        "from functools import partial\n",
        "from random import randint\n",
        "\n",
        "s_thresh=70\n",
        "apriori_res=apriori(iter_baskets_from_txt('italian300.txt'),s=s_thresh,max_k=3)\n",
        "pcy_res=pcy(iter_baskets_from_txt('italian300.txt'),s=s_thresh,max_k=3,bm_size=10000)\n",
        "son_res=SON(sc,iter_baskets_from_txt('italian300.txt'),s=s_thresh,max_k=3)\n",
        "toivonen_res=toivonen(iter_baskets_from_txt('italian300.txt'),s=s_thresh,p=0.5,scaling=0.8,max_k=3)\n",
        "\n",
        "bf=[i for i in iter_baskets_from_txt('italian300.txt')]\n",
        "basket_count=len(bf)\n",
        "\n",
        "start=time.time()\n",
        "apyori_res=list(ap.apriori(bf,min_support=s_thresh/basket_count,max_length=3))\n",
        "stop=time.time()\n",
        "\n",
        "print('\\n'+'-'*30)\n",
        "print('Function apyori.apriori executed in {0} seconds'.format(stop-start))\n",
        "print('-'*30+'\\n')\n",
        "\n",
        "#TEST\n",
        "def test(target,tested,basket_count):\n",
        "  failed=False\n",
        "\n",
        "  if len(target)!=len(tested): failed=True\n",
        "  test_itemsets={tuple(sorted(i.items)):i.support for i in target}\n",
        "\n",
        "  for k,v in tested.items():\n",
        "    if k not in test_itemsets:\n",
        "      failed=True\n",
        "      break\n",
        "    #apyori lib compute support as itemset_count/basket_count\n",
        "    elif test_itemsets[k]!=v/basket_count:\n",
        "      failed=True\n",
        "      break\n",
        "\n",
        "  if failed:\n",
        "    print(\"-\"*30+\"TEST FAILED\"+\"-\"*30)\n",
        "  else:\n",
        "    print(\"-\"*30+\"TEST PASSED\"+\"-\"*30)\n",
        "\n",
        "test(apyori_res,apriori_res,basket_count)\n",
        "test(apyori_res,pcy_res,basket_count)\n",
        "test(apyori_res,son_res,basket_count)\n",
        "if toivonen_res is not None:\n",
        "  test(apyori_res,toivonen_res,basket_count)\n",
        "else:\n",
        "  print('Toivonen didn\\'t find any frequent itemset')\n"
      ],
      "id": "mtL-veBk1moc",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: apyori in /usr/local/lib/python3.7/dist-packages (1.1.2)\n",
            "Total number of candidate 1-itemsets: 6439\n",
            "Size in GB: 0.000295\n",
            "Number of frequent 1-itemsets: 24\n",
            "Size in GB: 0.000001\n",
            "------------------------------\n",
            "\n",
            "Total number of candidate 2-itemsets: 276\n",
            "Size in GB: 0.000009\n",
            "Number of frequent 2-itemsets: 130\n",
            "Size in GB: 0.000005\n",
            "------------------------------\n",
            "\n",
            "Total number of candidate 3-itemsets: 419\n",
            "Size in GB: 0.000019\n",
            "Number of frequent 3-itemsets: 249\n",
            "Size in GB: 0.000009\n",
            "------------------------------\n",
            "\n",
            "\n",
            "------------------------------\n",
            "Function apriori executed in 0.8207533359527588 seconds\n",
            "------------------------------\n",
            "\n",
            "Total number of candidate 1-itemsets: 6439\n",
            "Size in GB: 0.000295\n",
            "Number of frequent 1-itemsets: 24\n",
            "Size in GB: 0.000001\n",
            "------------------------------\n",
            "\n",
            "Total number of candidate 2-itemsets: 276\n",
            "Size in GB: 0.000009\n",
            "Number of frequent 2-itemsets: 130\n",
            "Size in GB: 0.000005\n",
            "------------------------------\n",
            "\n",
            "Total number of candidate 3-itemsets: 419\n",
            "Size in GB: 0.000019\n",
            "Number of frequent 3-itemsets: 249\n",
            "Size in GB: 0.000009\n",
            "------------------------------\n",
            "\n",
            "\n",
            "------------------------------\n",
            "Function pcy executed in 2.1628618240356445 seconds\n",
            "------------------------------\n",
            "\n",
            "\n",
            "------------------------------\n",
            "Function SON executed in 230.29452538490295 seconds\n",
            "------------------------------\n",
            "\n",
            "Total number of candidate 1-itemsets: 3985\n",
            "Size in GB: 0.000148\n",
            "Number of frequent 1-itemsets: 33\n",
            "Size in GB: 0.000001\n",
            "------------------------------\n",
            "\n",
            "Total number of candidate 2-itemsets: 528\n",
            "Size in GB: 0.000019\n",
            "Number of frequent 2-itemsets: 219\n",
            "Size in GB: 0.000009\n",
            "------------------------------\n",
            "\n",
            "Total number of candidate 3-itemsets: 967\n",
            "Size in GB: 0.000037\n",
            "Number of frequent 3-itemsets: 586\n",
            "Size in GB: 0.000019\n",
            "------------------------------\n",
            "\n",
            "\n",
            "------------------------------\n",
            "Function apriori executed in 0.5662119388580322 seconds\n",
            "------------------------------\n",
            "\n",
            "Total number of candidate 1-itemsets: 9010\n",
            "Size in GB: 0.000295\n",
            "Number of frequent 1-itemsets: 403\n",
            "Size in GB: 0.000019\n",
            "------------------------------\n",
            "\n",
            "\n",
            "------------------------------\n",
            "Function toivonen executed in 20.192709922790527 seconds\n",
            "------------------------------\n",
            "\n",
            "\n",
            "------------------------------\n",
            "Function apyori.apriori executed in 0.12058448791503906 seconds\n",
            "------------------------------\n",
            "\n",
            "------------------------------TEST PASSED------------------------------\n",
            "------------------------------TEST PASSED------------------------------\n",
            "------------------------------TEST PASSED------------------------------\n",
            "------------------------------TEST PASSED------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlgMk0-1sZVi"
      },
      "source": [
        "Experiment: \n",
        "\n",
        "*   build a dataset with english newspapers (baskets)\n",
        "*   consider 3 different orders of magnitude (10^3,10^5,10^6) for the number of baskets to analyze\n",
        "*   Evaluate performance of the 3 algorithms"
      ],
      "id": "nlgMk0-1sZVi"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86af2QLusWUd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf4a93bf-3254-4b52-a480-7ae76bc92b9a"
      },
      "source": [
        "create_txt_dataset(['English'],1000,remove_stopwords=True,max_items_per_basket=15)\n",
        "create_txt_dataset(['English'],100000,remove_stopwords=True,max_items_per_basket=15)\n",
        "create_txt_dataset(['English'],1000000,remove_stopwords=True,max_items_per_basket=15)"
      ],
      "id": "86af2QLusWUd",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset english999.txt was created with 999 baskets with 11 items on average\n",
            "------------------------------\n",
            "Dataset english99944.txt was created with 99944 baskets with 11 items on average\n",
            "------------------------------\n",
            "Dataset english999318.txt was created with 999318 baskets with 11 items on average\n",
            "------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvqYt_987RXv"
      },
      "source": [
        "#sc is the spark context, support threshold here is given as percentage of basket file size\n",
        "def experiment(basket_files,sc,s=1):\n",
        "  for basket_file in basket_files:\n",
        "\n",
        "    bf=[i for i in iter_baskets_from_txt(basket_file)]\n",
        "    basket_count=len(bf)\n",
        "    s_thresh=int(s*(basket_count//100))\n",
        "\n",
        "    print(\"Support threshold: {0}\".format(s_thresh))\n",
        "    print(\"-\"*30)\n",
        "\n",
        "    apriori_res=apriori(iter_baskets_from_txt(basket_file),s=s_thresh,max_k=2)\n",
        "    pcy_res=pcy(iter_baskets_from_txt(basket_file),s=s_thresh,bm_size=40000,max_k=2)\n",
        "    son_res=SON(sc,iter_baskets_from_txt(basket_file),s=s_thresh,max_k=2)\n",
        "    toivonen_res=toivonen(iter_baskets_from_txt(basket_file),s=s_thresh,p=0.4,scaling=0.8,max_k=2)\n",
        "\n",
        "    start=time.time()\n",
        "    apyori_res=list(ap.apriori(bf,min_support=s_thresh/basket_count,max_length=2))\n",
        "    stop=time.time()\n",
        "    print('\\n'+'-'*30)\n",
        "    print('Function apyori.apriori executed in {0} seconds'.format(stop-start))\n",
        "    print('-'*30+'\\n')\n",
        "\n",
        "    print('\\n'+'*'*30)\n",
        "    print('Test on basket file {0}'.format(basket_file))\n",
        "    print('*'*30+'\\n')\n",
        "\n",
        "    print('Apriori results')\n",
        "    test(apyori_res,apriori_res,basket_count)\n",
        "    print('PCY results')\n",
        "    test(apyori_res,pcy_res,basket_count)\n",
        "    print('SON results')\n",
        "    test(apyori_res,son_res,basket_count)\n",
        "    print('Toivonen results')\n",
        "    if toivonen_res is not None:\n",
        "      test(apyori_res,toivonen_res,basket_count)\n",
        "    else:\n",
        "      print('Toivonen didn\\'t find any frequent itemset')\n",
        "    \n",
        "  return apriori_res"
      ],
      "id": "fvqYt_987RXv",
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJBPVCo1pz0F"
      },
      "source": [
        "#given a set of frequent itemsets, the total number of baskets, find rules of type itemset -> item with confidence>min_conf\n",
        "def find_rules(freq_it_sets,basket_count,k=2,min_conf=0):\n",
        "  itemsets={key:v for key,v in freq_it_sets.items() if len(key)==k}\n",
        "  rules=dict()\n",
        "\n",
        "  for key,v in itemsets.items():\n",
        "    for item in key:\n",
        "      i_minus_item=tuple(sorted(set(key)-{item}))\n",
        "      if freq_it_sets[key]/freq_it_sets[i_minus_item]>=min_conf:\n",
        "        rules[(i_minus_item,item)]=freq_it_sets[key]/freq_it_sets[i_minus_item]\n",
        "  return rules"
      ],
      "id": "bJBPVCo1pz0F",
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EL-tLnDeDdBM",
        "outputId": "994085f6-f16e-4938-fb04-594a7dac9f3d"
      },
      "source": [
        "basket_files=['english999.txt','english99944.txt','english999318.txt']\n",
        "\n",
        "freq_it_sets=experiment(basket_files,sc)"
      ],
      "id": "EL-tLnDeDdBM",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Support threshold: 9\n",
            "------------------------------\n",
            "Total number of candidate 1-itemsets: 5566\n",
            "Size in GB: 0.000295\n",
            "Number of frequent 1-itemsets: 166\n",
            "Size in GB: 0.000005\n",
            "------------------------------\n",
            "\n",
            "Total number of candidate 2-itemsets: 3100\n",
            "Size in GB: 0.000148\n",
            "Number of frequent 2-itemsets: 6\n",
            "Size in GB: 0.000000\n",
            "------------------------------\n",
            "\n",
            "\n",
            "------------------------------\n",
            "Function apriori executed in 0.019451618194580078 seconds\n",
            "------------------------------\n",
            "\n",
            "Total number of candidate 1-itemsets: 5566\n",
            "Size in GB: 0.000295\n",
            "Number of frequent 1-itemsets: 166\n",
            "Size in GB: 0.000005\n",
            "------------------------------\n",
            "\n",
            "Total number of candidate 2-itemsets: 29\n",
            "Size in GB: 0.000001\n",
            "Number of frequent 2-itemsets: 6\n",
            "Size in GB: 0.000000\n",
            "------------------------------\n",
            "\n",
            "\n",
            "------------------------------\n",
            "Function pcy executed in 0.07381987571716309 seconds\n",
            "------------------------------\n",
            "\n",
            "\n",
            "------------------------------\n",
            "Function SON executed in 0.5472507476806641 seconds\n",
            "------------------------------\n",
            "\n",
            "Total number of candidate 1-itemsets: 2896\n",
            "Size in GB: 0.000148\n",
            "Number of frequent 1-itemsets: 746\n",
            "Size in GB: 0.000037\n",
            "------------------------------\n",
            "\n",
            "Total number of candidate 2-itemsets: 8260\n",
            "Size in GB: 0.000295\n",
            "Number of frequent 2-itemsets: 320\n",
            "Size in GB: 0.000009\n",
            "------------------------------\n",
            "\n",
            "\n",
            "------------------------------\n",
            "Function apriori executed in 0.023201704025268555 seconds\n",
            "------------------------------\n",
            "\n",
            "Total number of candidate 1-itemsets: 19048\n",
            "Size in GB: 0.000590\n",
            "Number of frequent 1-itemsets: 172\n",
            "Size in GB: 0.000009\n",
            "------------------------------\n",
            "\n",
            "\n",
            "------------------------------\n",
            "Function toivonen executed in 2.1982834339141846 seconds\n",
            "------------------------------\n",
            "\n",
            "\n",
            "------------------------------\n",
            "Function apyori.apriori executed in 0.04420351982116699 seconds\n",
            "------------------------------\n",
            "\n",
            "\n",
            "******************************\n",
            "Test on basket file english999.txt\n",
            "******************************\n",
            "\n",
            "Apriori results\n",
            "------------------------------TEST PASSED------------------------------\n",
            "PCY results\n",
            "------------------------------TEST PASSED------------------------------\n",
            "SON results\n",
            "------------------------------TEST PASSED------------------------------\n",
            "Toivonen results\n",
            "Toivonen didn't find any frequent itemset\n",
            "Support threshold: 999\n",
            "------------------------------\n",
            "Total number of candidate 1-itemsets: 66804\n",
            "Size in GB: 0.002622\n",
            "Number of frequent 1-itemsets: 104\n",
            "Size in GB: 0.000005\n",
            "------------------------------\n",
            "\n",
            "Total number of candidate 2-itemsets: 5354\n",
            "Size in GB: 0.000148\n",
            "Number of frequent 2-itemsets: 1\n",
            "Size in GB: 0.000000\n",
            "------------------------------\n",
            "\n",
            "\n",
            "------------------------------\n",
            "Function apriori executed in 1.7493677139282227 seconds\n",
            "------------------------------\n",
            "\n",
            "Total number of candidate 1-itemsets: 66804\n",
            "Size in GB: 0.002622\n",
            "Number of frequent 1-itemsets: 104\n",
            "Size in GB: 0.000005\n",
            "------------------------------\n",
            "\n",
            "Total number of candidate 2-itemsets: 6\n",
            "Size in GB: 0.000000\n",
            "Number of frequent 2-itemsets: 1\n",
            "Size in GB: 0.000000\n",
            "------------------------------\n",
            "\n",
            "\n",
            "------------------------------\n",
            "Function pcy executed in 6.548019886016846 seconds\n",
            "------------------------------\n",
            "\n",
            "\n",
            "------------------------------\n",
            "Function SON executed in 5.3525004386901855 seconds\n",
            "------------------------------\n",
            "\n",
            "Total number of candidate 1-itemsets: 43772\n",
            "Size in GB: 0.002622\n",
            "Number of frequent 1-itemsets: 171\n",
            "Size in GB: 0.000009\n",
            "------------------------------\n",
            "\n",
            "Total number of candidate 2-itemsets: 14154\n",
            "Size in GB: 0.000590\n",
            "Number of frequent 2-itemsets: 6\n",
            "Size in GB: 0.000000\n",
            "------------------------------\n",
            "\n",
            "\n",
            "------------------------------\n",
            "Function apriori executed in 0.5441324710845947 seconds\n",
            "------------------------------\n",
            "\n",
            "Total number of candidate 1-itemsets: 81310\n",
            "Size in GB: 0.002622\n",
            "Number of frequent 1-itemsets: 105\n",
            "Size in GB: 0.000005\n",
            "------------------------------\n",
            "\n",
            "\n",
            "------------------------------\n",
            "Function toivonen executed in 6.0935378074646 seconds\n",
            "------------------------------\n",
            "\n",
            "\n",
            "------------------------------\n",
            "Function apyori.apriori executed in 1.5337281227111816 seconds\n",
            "------------------------------\n",
            "\n",
            "\n",
            "******************************\n",
            "Test on basket file english99944.txt\n",
            "******************************\n",
            "\n",
            "Apriori results\n",
            "------------------------------TEST PASSED------------------------------\n",
            "PCY results\n",
            "------------------------------TEST PASSED------------------------------\n",
            "SON results\n",
            "------------------------------TEST PASSED------------------------------\n",
            "Toivonen results\n",
            "------------------------------TEST PASSED------------------------------\n",
            "Support threshold: 9993\n",
            "------------------------------\n",
            "Total number of candidate 1-itemsets: 175224\n",
            "Size in GB: 0.010486\n",
            "Number of frequent 1-itemsets: 106\n",
            "Size in GB: 0.000005\n",
            "------------------------------\n",
            "\n",
            "Total number of candidate 2-itemsets: 5565\n",
            "Size in GB: 0.000295\n",
            "Number of frequent 2-itemsets: 1\n",
            "Size in GB: 0.000000\n",
            "------------------------------\n",
            "\n",
            "\n",
            "------------------------------\n",
            "Function apriori executed in 16.48541784286499 seconds\n",
            "------------------------------\n",
            "\n",
            "Total number of candidate 1-itemsets: 175224\n",
            "Size in GB: 0.010486\n",
            "Number of frequent 1-itemsets: 106\n",
            "Size in GB: 0.000005\n",
            "------------------------------\n",
            "\n",
            "Total number of candidate 2-itemsets: 6\n",
            "Size in GB: 0.000000\n",
            "Number of frequent 2-itemsets: 1\n",
            "Size in GB: 0.000000\n",
            "------------------------------\n",
            "\n",
            "\n",
            "------------------------------\n",
            "Function pcy executed in 70.80847406387329 seconds\n",
            "------------------------------\n",
            "\n",
            "\n",
            "------------------------------\n",
            "Function SON executed in 50.04983878135681 seconds\n",
            "------------------------------\n",
            "\n",
            "Total number of candidate 1-itemsets: 120413\n",
            "Size in GB: 0.005243\n",
            "Number of frequent 1-itemsets: 169\n",
            "Size in GB: 0.000005\n",
            "------------------------------\n",
            "\n",
            "Total number of candidate 2-itemsets: 14195\n",
            "Size in GB: 0.000590\n",
            "Number of frequent 2-itemsets: 6\n",
            "Size in GB: 0.000000\n",
            "------------------------------\n",
            "\n",
            "\n",
            "------------------------------\n",
            "Function apriori executed in 5.609654664993286 seconds\n",
            "------------------------------\n",
            "\n",
            "Total number of candidate 1-itemsets: 189420\n",
            "Size in GB: 0.010486\n",
            "Number of frequent 1-itemsets: 107\n",
            "Size in GB: 0.000005\n",
            "------------------------------\n",
            "\n",
            "\n",
            "------------------------------\n",
            "Function toivonen executed in 64.75103902816772 seconds\n",
            "------------------------------\n",
            "\n",
            "\n",
            "------------------------------\n",
            "Function apyori.apriori executed in 13.20380687713623 seconds\n",
            "------------------------------\n",
            "\n",
            "\n",
            "******************************\n",
            "Test on basket file english999318.txt\n",
            "******************************\n",
            "\n",
            "Apriori results\n",
            "------------------------------TEST PASSED------------------------------\n",
            "PCY results\n",
            "------------------------------TEST PASSED------------------------------\n",
            "SON results\n",
            "------------------------------TEST PASSED------------------------------\n",
            "Toivonen results\n",
            "------------------------------TEST PASSED------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-FJOeEfVssPu",
        "outputId": "9629c4e9-6286-441b-be8b-25de9c65e504"
      },
      "source": [
        "for k,v in find_rules(freq_it_sets,basket_count).items():\n",
        "  print(\"Rules {0} -> {1} has confidence {2}\".format(k[0],k[1],v))"
      ],
      "id": "-FJOeEfVssPu",
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Rules ('would',) -> said has confidence 0.27342027039984657\n",
            "Rules ('said',) -> would has confidence 0.06353857380802505\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0STpfE_Maq3j",
        "outputId": "2640b7b1-91b1-47d7-da2d-471193a587af",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "create_txt_dataset(['English'],1000000,remove_stopwords=True)"
      ],
      "id": "0STpfE_Maq3j",
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset english999318.txt was created with 999318 baskets with 18 items on average\n",
            "------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3bAn-LzbdFLG",
        "outputId": "b7eb9f52-a8dd-406b-82d3-965d1213530c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "freq_it_sets=apriori(iter_baskets_from_txt('english999318.txt'),s=int(0.8*(999318//100)),max_k=2)\n",
        "\n",
        "for k,v in find_rules(freq_it_sets,basket_count).items():\n",
        "  print(\"Rules {0} -> {1} has confidence {2}\".format(k[0],k[1],v))"
      ],
      "id": "3bAn-LzbdFLG",
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of candidate 1-itemsets: 211748\n",
            "Size in GB: 0.010486\n",
            "Number of frequent 1-itemsets: 311\n",
            "Size in GB: 0.000009\n",
            "------------------------------\n",
            "\n",
            "Total number of candidate 2-itemsets: 48205\n",
            "Size in GB: 0.002622\n",
            "Number of frequent 2-itemsets: 35\n",
            "Size in GB: 0.000001\n",
            "------------------------------\n",
            "\n",
            "\n",
            "------------------------------\n",
            "Function apriori executed in 60.58247208595276 seconds\n",
            "------------------------------\n",
            "\n",
            "Rules ('st',) -> louis has confidence 0.403933507296801\n",
            "Rules ('louis',) -> st has confidence 0.9085869565217392\n",
            "Rules ('new',) -> jersey has confidence 0.1289073794454975\n",
            "Rules ('jersey',) -> new has confidence 0.7658173489836817\n",
            "Rules ('said',) -> city has confidence 0.03721278499968986\n",
            "Rules ('city',) -> said has confidence 0.23596673596673598\n",
            "Rules ('said',) -> new has confidence 0.056308761109781925\n",
            "Rules ('new',) -> said has confidence 0.20414752465705016\n",
            "Rules ('said',) -> good has confidence 0.04432393155576823\n",
            "Rules ('good',) -> said has confidence 0.3653895321231601\n",
            "Rules ('said',) -> make has confidence 0.03910909074797742\n",
            "Rules ('make',) -> said has confidence 0.28837923486556244\n",
            "Rules ('would',) -> said has confidence 0.34091246596815444\n",
            "Rules ('said',) -> would has confidence 0.09154105856394715\n",
            "Rules ('year',) -> one has confidence 0.11401816517637119\n",
            "Rules ('one',) -> year has confidence 0.10114928169893816\n",
            "Rules ('said',) -> really has confidence 0.037855225031235874\n",
            "Rules ('really',) -> said has confidence 0.5040707964601769\n",
            "Rules ('school',) -> high has confidence 0.29192380825299397\n",
            "Rules ('high',) -> school has confidence 0.3375115705029312\n",
            "Rules ('said',) -> could has confidence 0.0503229922641359\n",
            "Rules ('could',) -> said has confidence 0.3040882439559851\n",
            "Rules ('two',) -> one has confidence 0.1655303685758782\n",
            "Rules ('one',) -> two has confidence 0.1195003123048095\n",
            "Rules ('time',) -> said has confidence 0.2630572029845035\n",
            "Rules ('said',) -> time has confidence 0.060921037474191635\n",
            "Rules ('said',) -> going has confidence 0.06200653959645905\n",
            "Rules ('going',) -> said has confidence 0.5135402906208718\n",
            "Rules ('state',) -> said has confidence 0.23774002799891347\n",
            "Rules ('said',) -> state has confidence 0.050411604682280176\n",
            "Rules ('year',) -> said has confidence 0.22033373231007533\n",
            "Rules ('said',) -> year has confidence 0.06932592533517647\n",
            "Rules ('said',) -> get has confidence 0.06401361086742696\n",
            "Rules ('get',) -> said has confidence 0.3659574468085106\n",
            "Rules ('said',) -> one has confidence 0.08011005662333519\n",
            "Rules ('one',) -> said has confidence 0.22587133041848845\n",
            "Rules ('two',) -> said has confidence 0.18447828344004152\n",
            "Rules ('said',) -> two has confidence 0.047234849491807784\n",
            "Rules ('think',) -> said has confidence 0.5039897861474625\n",
            "Rules ('said',) -> think has confidence 0.04897165288743564\n",
            "Rules ('york',) -> new has confidence 0.9755256518082422\n",
            "Rules ('new',) -> york has confidence 0.1863173450701963\n",
            "Rules ('said',) -> back has confidence 0.03909579888525578\n",
            "Rules ('back',) -> said has confidence 0.27147428008860447\n",
            "Rules ('said',) -> like has confidence 0.06058431028524337\n",
            "Rules ('like',) -> said has confidence 0.300798521744869\n",
            "Rules ('said',) -> know has confidence 0.040043951759399565\n",
            "Rules ('know',) -> said has confidence 0.42439894815927875\n",
            "Rules ('said',) -> first has confidence 0.04071740613729608\n",
            "Rules ('first',) -> said has confidence 0.17545534384665318\n",
            "Rules ('year',) -> old has confidence 0.17479405759346617\n",
            "Rules ('old',) -> year has confidence 0.554845342392276\n",
            "Rules ('said',) -> lot has confidence 0.03623361777919558\n",
            "Rules ('lot',) -> said has confidence 0.487453060737915\n",
            "Rules ('year',) -> last has confidence 0.24175174258959375\n",
            "Rules ('last',) -> year has confidence 0.3514431934493347\n",
            "Rules ('said',) -> last has confidence 0.04650379704211748\n",
            "Rules ('last',) -> said has confidence 0.2148618219037871\n",
            "Rules ('want',) -> said has confidence 0.43010863942058974\n",
            "Rules ('said',) -> want has confidence 0.036836182222576674\n",
            "Rules ('said',) -> go has confidence 0.035985503008391595\n",
            "Rules ('go',) -> said has confidence 0.33817712453678644\n",
            "Rules ('said',) -> people has confidence 0.0710804512144332\n",
            "Rules ('people',) -> said has confidence 0.372270565030746\n",
            "Rules ('said',) -> police has confidence 0.04156365473057394\n",
            "Rules ('police',) -> said has confidence 0.40788729944780205\n",
            "Rules ('years',) -> said has confidence 0.22302619527385967\n",
            "Rules ('said',) -> years has confidence 0.04315424763626374\n",
            "Rules ('said',) -> also has confidence 0.04990208327795057\n",
            "Rules ('also',) -> said has confidence 0.20051272008687757\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}