{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "frequent_itemsets.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/turatig/frequent_itemsets/blob/master/frequent_itemsets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J00vwdMle-Nx"
      },
      "source": [
        "**MARKET BASKET ANALYSIS NOTEBOOK**"
      ],
      "id": "J00vwdMle-Nx"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOVdeW-efZ3z"
      },
      "source": [
        "Dowload and preprocess dataset"
      ],
      "id": "LOVdeW-efZ3z"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22ba4125",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02cc30da-0a49-4242-e3e0-ea000c0a6400"
      },
      "source": [
        "!pip install kaggle\n",
        "\n",
        "\n",
        "import os,sys,time,zipfile,json,re\n",
        "import functools as ft\n",
        "import itertools as it\n",
        "from datetime import datetime as dt\n",
        "from random import uniform\n",
        "from nltk.corpus import stopwords\n",
        "import nltk \n",
        "nltk.download('stopwords')\n",
        "\n",
        "os.environ['KAGGLE_USERNAME']='giacomoturati1'\n",
        "os.environ['KAGGLE_KEY']='7d34a1aefc3558065164b70c24ce27ed'\n",
        "\n",
        "from kaggle.api.kaggle_api_extended import KaggleApi\n",
        "\n",
        "def get_dataset():\n",
        "\n",
        "  #execute only if the dataset was not already downloaded\n",
        "  if 'old-newspaper.tsv' not in os.listdir():\n",
        "    api=KaggleApi()\n",
        "    api.authenticate()\n",
        "\n",
        "    api.dataset_download_file('alvations/old-newspapers','old-newspaper.tsv')\n",
        "\n",
        "    with zipfile.ZipFile('old-newspaper.tsv.zip','r') as _zip:\n",
        "      _zip.extractall()\n",
        "\n",
        "#Yield baskets (list of lists) reading from tsv\n",
        "#languages: subset of languages to be considered during the market-basket analysis\n",
        "def iter_baskets_from_tsv(languages=None,max_basket=-1,skip=0):\n",
        "  count=0\n",
        "  with open('old-newspaper.tsv','r') as f:\n",
        "    #skip header line\n",
        "    next(f)\n",
        "    for line in f:\n",
        "      l=line.split('\\t')\n",
        "      if languages is not None and l[0] not in languages: continue\n",
        "\n",
        "      #get a list of words as basket skipping any sequence of non-alphabetical characters\n",
        "      basket=re.split(r'[^a-zA-Z]+',l[3])\n",
        "      #remove any empty string\n",
        "      basket=[word.lower() for word in basket if word!='']\n",
        "      if basket:\n",
        "        count+=1\n",
        "        if count>skip: yield basket\n",
        "        if max_basket>0 and count-skip>=max_basket: break \n",
        "      \n",
        "    f.close()\n",
        "\n",
        "#Create txt dataset. Structure: every line (basket) is a sequence of words (items) separated from commas\n",
        "def create_txt_dataset(languages=None,max_basket=-1,skip=0,remove_stopwords=False,max_items_per_basket=-1):\n",
        "  baskets=[]\n",
        "  stopw=set()\n",
        "  #compute the mean of the number of items per basket\n",
        "  n_items=0\n",
        "\n",
        "  if remove_stopwords and languages:\n",
        "    for lang in languages: stopw|=set(stopwords.words(lang.lower()))\n",
        "\n",
        "  #execute only if the dataset was not already created\n",
        "  for line in iter_baskets_from_tsv(languages,max_basket,skip=0):\n",
        "    if stopw: line=[word for word in line if word not in stopw]\n",
        "    if line:\n",
        "      if max_items_per_basket>0: line=line[:max_items_per_basket]\n",
        "      line=set(line)\n",
        "      baskets.append(ft.reduce(lambda w1,w2: w1+','+w2,line))\n",
        "      n_items+=len(line)\n",
        "\n",
        "  filename=ft.reduce(lambda i,j:i+'_'+j,languages).lower() if languages is not None else 'all_languages' \n",
        "  filename+=str(len(baskets))+'.txt'\n",
        "\n",
        "  with open(filename,'w') as f:\n",
        "    for basket in baskets:\n",
        "      f.write(basket+'\\n')\n",
        "\n",
        "    f.close()\n",
        "\n",
        "  print(\"Dataset {0} was created with {1} baskets with {2} items on average\".format(filename,len(baskets),n_items//len(baskets)))\n",
        "  print(\"-\"*30)\n",
        "\n",
        "#Yield baskets from txt files where every line is a basket and every basket is a sequence item1,item2...\n",
        "def iter_baskets_from_txt(filename,max_basket=-1,skip=0):\n",
        "  theres_next=True\n",
        "  basket=[]\n",
        "  count=0\n",
        "\n",
        "  with open(filename,'r') as f:\n",
        "    eol=''\n",
        "    for line in f:\n",
        "      line=line.split(',')\n",
        "      count+=1\n",
        "      #trim \\n\n",
        "      if count>skip: yield line[:-1]+[line[-1][:-1]]\n",
        "      if max_basket>0 and count-skip>=max_basket: break \n",
        "    \n",
        "    f.close()\n",
        "\n",
        "#Scan the basket file and extract a basket with fixed probability p\n",
        "def get_rand_sample(basket_file,p):\n",
        "  basket_count=0\n",
        "  sample=[]\n",
        "  all_items=set()\n",
        "\n",
        "  for basket in basket_file:\n",
        "    basket_count+=1\n",
        "    if uniform(0,1)<=p:\n",
        "      sample.append(basket)\n",
        "\n",
        "    all_items|=set(basket)\n",
        "  \n",
        "  return sample,basket_count,all_items\n",
        "\n",
        "\n",
        "\n",
        "get_dataset()"
      ],
      "id": "22ba4125",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.12)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2020.12.5)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (5.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.41.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (2.10)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TqnHG1Eppe9F"
      },
      "source": [
        "Utilities to log the execution"
      ],
      "id": "TqnHG1Eppe9F"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDz4lLPYfJMc"
      },
      "source": [
        "def sizeof_GB(obj): return \"%f\"%(sys.getsizeof(obj)/1000000000)\n",
        "\n",
        "#decorator to log time execution of function/method\n",
        "def time_it(f):\n",
        "  def _wrap(*args,**kwargs):\n",
        "    start=time.time()\n",
        "    res=f(*args,**kwargs)\n",
        "    stop=time.time()\n",
        "    \n",
        "    print('\\n'+'-'*30)\n",
        "    print('Function {0} executed in {1} seconds'.format(f.__name__,stop-start))\n",
        "    print('-'*30+'\\n')\n",
        "    return res\n",
        "  return _wrap\n",
        "\n",
        "#decorator to log the memory space used before and after a candidate itemset filtering operation\n",
        "def log_filter(f):\n",
        "  def _wrap(*args,**kwargs):\n",
        "    if len(args)>0:\n",
        "      if args[0]:\n",
        "        print('Total number of candidate {0}-itemsets: {1}\\nSize in GB: {2}'.\\\n",
        "            format(len(list(args[0].keys())[0]),len(args[0]),sizeof_GB(args[0])))\n",
        "      else:\n",
        "        print('No more candidates were found')\n",
        "      \n",
        "    #argument was given by key=value\n",
        "    else:\n",
        "      if kwargs['candidates']:\n",
        "        print('Total number of candidate {0}-itemsets: {1}\\nSize in GB: {2}'.\\\n",
        "            format(len(list(kwargs['candidates'].keys())[0]),len(kwargs['candidates']),sizeof_GB(kwargs['candidates'])))\n",
        "      else:\n",
        "        print('No more candidates were found')\n",
        "    \n",
        "    res=f(*args,**kwargs)\n",
        "\n",
        "    if res:\n",
        "      print('Number of frequent {0}-itemsets: {1}\\nSize in GB: {2}'.\\\n",
        "              format(len(list(res.keys())[0]),len(res),sizeof_GB(res)))\n",
        "    else:\n",
        "      print('No frequent itemset were found')\n",
        "    print('-'*30+'\\n')\n",
        "    return res\n",
        "\n",
        "  return _wrap\n",
        "\n",
        "#Dump on json file the result of an algorithm run\n",
        "def dump_result(algo,s,basket_count,freq_it_sets):\n",
        "  def remap(dic):\n",
        "    return {str(k):v for k,v in dic.items()}\n",
        "\n",
        "  header_info={'support_threshold':s,'total_n_baskets':basket_count}\n",
        "\n",
        "  filename=algo+'_market_basket_analysis_'+str(dt.today())[:10]+'_'+str(dt.today())[11:]+'.json'\n",
        "              \n",
        "  with open(filename,'w') as f:\n",
        "    f.write(json.dumps({'header':header_info,'frequent itemsets':remap(freq_it_sets)},indent='\\t'))\n",
        "    f.close()"
      ],
      "id": "wDz4lLPYfJMc",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ixo8_SLkdPOd"
      },
      "source": [
        "A-priori algorithm implementation"
      ],
      "id": "Ixo8_SLkdPOd"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYkGZQBq4t9f"
      },
      "source": [
        "\"\"\"\n",
        "  Filter candidate set of itemsets according to suppport threshold \n",
        "\"\"\"\n",
        "@log_filter\n",
        "def filter_ck(candidates,s):\n",
        "  return {k:v for k,v in candidates.items() if v>=s}\n",
        "\n",
        "\"\"\"\n",
        "  Discard unfrequent singletons from a basket\n",
        "\"\"\"\n",
        "def freq_sing(freq_it_sets,basket):\n",
        "  return [word for word in basket if (word,) in freq_it_sets[1]]\n",
        "\n",
        "\"\"\"\n",
        "  Check monotonicity property\n",
        "  kuple is a possible k-itemset -> all immediate subsets (k-1 itemsets) are frequent itemsets.\n",
        "\"\"\" \n",
        "def check_mono_prop(kuple,k,freq_it_sets):\n",
        "  return all([tuple(sorted(el)) in freq_it_sets[k-1] for el in it.combinations(kuple,r=k-1)])\n",
        "\n",
        "\"\"\"\n",
        "  Utility to clean output of analysis. Take [{freq_0_it_set},{freq_1_it_set}...] and return {freq_non_empty_it_set}\n",
        "\"\"\"\n",
        "def clean_output(freq_it_sets):\n",
        "  if not freq_it_sets[-1]:\n",
        "    #remove the last element if empty\n",
        "    freq_it_sets=freq_it_sets[:-1]\n",
        "  #remove empty itemset set\n",
        "  freq_it_sets=freq_it_sets[1:]\n",
        "  fitsets=dict()\n",
        "\n",
        "  #create a single dict with frequent itemsets and their occurences\n",
        "  for fis in freq_it_sets:\n",
        "    for k,v in fis.items():\n",
        "      fitsets[k]=v\n",
        "\n",
        "  return fitsets\n",
        "\n",
        "\"\"\"\n",
        "  Return candidate k-itemsets found after a basket_file pass\n",
        "\"\"\"\n",
        "def apr_get_ck(basket_file,k,freq_it_sets):\n",
        "  basket_count=0\n",
        "  candidates=dict()\n",
        "\n",
        "  for basket in basket_file:\n",
        "    basket_count+=1\n",
        "    if k>2:\n",
        "      basket=freq_sing(freq_it_sets,basket)\n",
        "              \n",
        "    for kuple in it.combinations(basket,r=k):\n",
        "        #sort tuple in order to avoid duplication caused by taking the same itemset ordered in a different way\n",
        "        kuple=tuple(sorted(kuple))\n",
        "\n",
        "        if check_mono_prop(kuple,k,freq_it_sets):\n",
        "          if kuple not in candidates.keys(): candidates[kuple]=1\n",
        "          else: candidates[kuple]+=1\n",
        "\n",
        "  return candidates,basket_count\n",
        "\n",
        "\"\"\"\n",
        "  Apriori algorithm iteration\n",
        "\"\"\"\n",
        "@time_it\n",
        "def apriori(basket_file,s=-1,max_k=-1,log=False):\n",
        "    freq_it_sets=[{tuple():1}]\n",
        "    k=1\n",
        "\n",
        "    #stop when no more frequent itemsets are found or k>max_k\n",
        "    while freq_it_sets[-1] and (max_k<0 or k<=max_k):\n",
        "\n",
        "      #duplicate generator for multiple iterations\n",
        "      basket_file,_bf=it.tee(basket_file,2)\n",
        "      ck,basket_count=apr_get_ck(basket_file,k,freq_it_sets)\n",
        "      if s<0:\n",
        "        #set threshold to the 1% of the total number of baskets\n",
        "        s=basket_count//100\n",
        "      freq_it_sets.append(filter_ck(ck,s))\n",
        "      k+=1\n",
        "      basket_file=_bf\n",
        "\n",
        "    freq_it_sets=clean_output(freq_it_sets)\n",
        "    if log:dump_result(\"apriori\",s,basket_count,freq_it_sets)\n",
        "\n",
        "    return freq_it_sets"
      ],
      "id": "bYkGZQBq4t9f",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPimayhEg_Ue"
      },
      "source": [
        "PCY implementation"
      ],
      "id": "wPimayhEg_Ue"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8YqFThkXhL4l"
      },
      "source": [
        "!pip install -q bitmap\n",
        "from bitmap import BitMap\n",
        "\n",
        "#map a tuple to a bucket of a table of size=s\n",
        "def hash_tuple(t,s): return hash(t)%s\n",
        "def set_all(bm):\n",
        "  for i in bm.size():\n",
        "    bm.set(i)\n"
      ],
      "id": "8YqFThkXhL4l",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJotqhVRg-sG"
      },
      "source": [
        "\"\"\"\n",
        "  Return candidate k-itemsets found after a basket_file pass.\n",
        "  bm: bitmap of frequent buckets of couples\n",
        "\"\"\" \n",
        "def pcy_get_ck(basket_file,k,freq_it_sets,bm,max_basket=-1,skip=0):\n",
        "  basket_count=0\n",
        "  candidates=dict()\n",
        "  buckets=[0 for i in range(bm.size())]\n",
        "\n",
        "  for basket in basket_file:\n",
        "    basket_count+=1\n",
        "    if k>2:\n",
        "      basket=freq_sing(freq_it_sets,basket)\n",
        "              \n",
        "    for kuple in it.combinations(basket,r=k):\n",
        "        kuple=tuple(sorted(kuple))\n",
        "\n",
        "        #PCY variant: added constraint for couple -> must hash to a frequent bucket\n",
        "        if check_mono_prop(kuple,k,freq_it_sets) and (k!=2 or bm[hash_tuple(kuple,bm.size())]):\n",
        "          if kuple not in candidates.keys(): candidates[kuple]=1\n",
        "          else: candidates[kuple]+=1\n",
        "        \n",
        "    if k==1:\n",
        "      #PCY variant: during the first pass hash couples to buckets\n",
        "      for couple in it.combinations(basket,r=2):\n",
        "          buckets[hash_tuple(tuple(sorted(couple)),bm.size())]+=1\n",
        "\n",
        "  return candidates,basket_count,buckets\n",
        "\n",
        "\"\"\"\n",
        "  PCY algorithm iteration\n",
        "\"\"\"\n",
        "@time_it\n",
        "def pcy(basket_file,s=-1,max_k=-1,bm_size=256,log=False):\n",
        "    freq_it_sets=[{tuple():1}]\n",
        "    bm=BitMap(bm_size)\n",
        "    k=1\n",
        "\n",
        "    while freq_it_sets[-1] and (max_k<0 or k<=max_k):\n",
        "      \n",
        "      basket_file,_bf=it.tee(basket_file,2)\n",
        "      ck,basket_count,buckets=pcy_get_ck(basket_file,k,freq_it_sets,bm)\n",
        "      if s<0: s=basket_count//100\n",
        "      freq_it_sets.append(filter_ck(ck,s))\n",
        "\n",
        "      if k==1:\n",
        "        #PCY variant:set bit of frequent buckets in the bitmap for couples\n",
        "        for i in range(len(buckets)):\n",
        "          if buckets[i]>=s: bm.set(i)\n",
        "      k+=1\n",
        "      basket_file=_bf\n",
        "    \n",
        "    freq_it_sets=clean_output(freq_it_sets)\n",
        "    if log:dump_result(\"pcy\",s,basket_count,freq_it_sets)\n",
        "    \n",
        "    return freq_it_sets\n"
      ],
      "id": "rJotqhVRg-sG",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfCOEYFIht9c"
      },
      "source": [
        "Toivonen algorithm implementation"
      ],
      "id": "qfCOEYFIht9c"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3ZvvKieh0WL"
      },
      "source": [
        "from math import ceil,floor\n",
        "\n",
        "\"\"\"\n",
        "  Utility to build negative border efficiently.\n",
        "  Return all k-itemsets made from k-1 frequent itemsets found in sample (only for k>=2)\n",
        "\"\"\"\n",
        "def nb_candidates(freq_in_sample,k):\n",
        "  immediate_subs=[itemset for itemset in freq_in_sample if len(itemset)==k-1]\n",
        "  self_join=[set(kuple[0])|set(kuple[1]) for kuple in it.product(immediate_subs,immediate_subs)]\n",
        "\n",
        "  return {tuple(sorted(itemset)) for itemset in self_join if len(itemset)==k}\n",
        "\n",
        "\"\"\"\n",
        "  Function that builds the negative border\n",
        "\"\"\"\n",
        "def build_neg_border(freq_in_sample,all_items):\n",
        "  neg_border=set()\n",
        "  max_k=len(max(freq_in_sample,key=lambda el:len(el)))+1 if freq_in_sample else 1\n",
        "\n",
        "  for k in range(1,max_k+1):\n",
        "    #add singletons not found in sample\n",
        "    if k==1:\n",
        "      for singleton in all_items:\n",
        "        if (singleton,) not in freq_in_sample:\n",
        "          neg_border|={(singleton,)}\n",
        "    else:\n",
        "      for itemset in nb_candidates(freq_in_sample,k):\n",
        "        if itemset not in freq_in_sample:\n",
        "          neg_border|={itemset}\n",
        "\n",
        "\n",
        "  return neg_border\n",
        "\n",
        "\n",
        "@time_it\n",
        "def toivonen(basket_file,s=-1,p=0.25,scaling=0.9,max_k=-1,log=False):\n",
        "  negative_border=set()\n",
        "  #freq_it_sets:the k-1-th element is the set of frequent itemsets made of k elements\n",
        "  freq_it_sets=dict()\n",
        "  basket_file,_bf=it.tee(basket_file,2)\n",
        "\n",
        "  sample,basket_count,all_items=get_rand_sample(_bf,p)\n",
        "  if s<0: s=basket_count//100\n",
        "  \n",
        "  ps=floor(scaling*p*s)\n",
        "\n",
        "  #keeping only itemsets and not their counters\n",
        "  freq_in_sample={k for k in apriori(sample,s=ps,max_k=max_k).keys()}\n",
        "\n",
        "  neg_border=build_neg_border(freq_in_sample,all_items)\n",
        "\n",
        "  max_k=len(max(neg_border,key=lambda el:len(el))) if max_k<0 else max_k\n",
        "\n",
        "  #add all the items not present in the sample to build the complete negative border\n",
        "  ck=freq_in_sample | neg_border\n",
        "\n",
        "  #total pass of toivonen algorithm\n",
        "  for basket in basket_file:\n",
        "    basket=set(basket)\n",
        "    for k in range(1,max_k+1):\n",
        "      for kuple in it.combinations(basket,r=k):\n",
        "        kuple=tuple(sorted(kuple))\n",
        "\n",
        "        if kuple in ck:\n",
        "          if kuple not in freq_it_sets:\n",
        "            freq_it_sets[kuple]=1\n",
        "          else:\n",
        "            freq_it_sets[kuple]+=1\n",
        "\n",
        "  #filter according to threshold\n",
        "  freq_it_sets=filter_ck(freq_it_sets,s)\n",
        "  #check that no elements of the negative border are frequent in the sample\n",
        "  if not [itemset for itemset in neg_border if itemset in freq_it_sets]:\n",
        "    if log:dump_result(\"toivonen\",s,basket_count,freq_it_sets)\n",
        "    return freq_it_sets\n",
        "  else:\n",
        "    return None\n",
        "\n"
      ],
      "id": "b3ZvvKieh0WL",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOAV6G-5LO2X"
      },
      "source": [
        "Validate implementation by comparing it with apyori's implementation on a small size dataset"
      ],
      "id": "BOAV6G-5LO2X"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtL-veBk1moc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9395d441-e564-46f9-914e-141930855312"
      },
      "source": [
        "!pip install apyori\n",
        "import apyori as ap\n",
        "from functools import partial\n",
        "from random import randint\n",
        "\n",
        "create_txt_dataset(['Italian'],300)\n",
        "\n",
        "s_thresh=randint(30,100)\n",
        "apriori_res=apriori(iter_baskets_from_txt('italian300.txt'),s=s_thresh,max_k=3)\n",
        "pcy_res=pcy(iter_baskets_from_txt('italian300.txt'),s=s_thresh,max_k=3)\n",
        "toivonen_res=toivonen(iter_baskets_from_txt('italian300.txt'),s=s_thresh,p=uniform(0.4,0.7),scaling=uniform(0.7,0.9),max_k=3)\n",
        "\n",
        "bf=[i for i in iter_baskets_from_txt('italian300.txt')]\n",
        "basket_count=len(bf)\n",
        "\n",
        "start=time.time()\n",
        "#here support is computed as itemset_count/basket_count\n",
        "apyori_res=list(ap.apriori(bf,min_support=s_thresh/basket_count,max_length=3))\n",
        "stop=time.time()\n",
        "\n",
        "print('\\n'+'-'*30)\n",
        "print('Function apyori.apriori executed in {0} seconds'.format(stop-start))\n",
        "print('-'*30+'\\n')\n",
        "\n",
        "#TEST\n",
        "def test(target,tested,basket_count):\n",
        "  failed=False\n",
        "\n",
        "  if len(target)!=len(tested): failed=True\n",
        "  test_itemsets={tuple(sorted(i.items)):i.support for i in target}\n",
        "\n",
        "  for k,v in tested.items():\n",
        "    if k not in test_itemsets:\n",
        "      failed=True\n",
        "      break\n",
        "    elif test_itemsets[k]!=v/basket_count:\n",
        "      failed=True\n",
        "      break\n",
        "\n",
        "  if failed:\n",
        "    print(\"-\"*30+\"TEST FAILED\"+\"-\"*30)\n",
        "  else:\n",
        "    print(\"-\"*30+\"TEST PASSED\"+\"-\"*30)\n",
        "\n",
        "test(apyori_res,apriori_res,basket_count)\n",
        "test(apyori_res,pcy_res,basket_count)\n",
        "if toivonen_res is not None:\n",
        "  test(apyori_res,toivonen_res,basket_count)\n"
      ],
      "id": "mtL-veBk1moc",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting apyori\n",
            "  Downloading https://files.pythonhosted.org/packages/5e/62/5ffde5c473ea4b033490617ec5caa80d59804875ad3c3c57c0976533a21a/apyori-1.1.2.tar.gz\n",
            "Building wheels for collected packages: apyori\n",
            "  Building wheel for apyori (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for apyori: filename=apyori-1.1.2-cp37-none-any.whl size=5975 sha256=2ec6917aafb9e0b3b48b3e84164e1a044d03148336202a8ced28eb3def93d60c\n",
            "  Stored in directory: /root/.cache/pip/wheels/5d/92/bb/474bbadbc8c0062b9eb168f69982a0443263f8ab1711a8cad0\n",
            "Successfully built apyori\n",
            "Installing collected packages: apyori\n",
            "Successfully installed apyori-1.1.2\n",
            "Dataset italian300.txt was created with 300 baskets with 54 items on average\n",
            "------------------------------\n",
            "Total number of candidate 1-itemsets: 6439\n",
            "Size in GB: 0.000295\n",
            "Number of frequent 1-itemsets: 19\n",
            "Size in GB: 0.000001\n",
            "------------------------------\n",
            "\n",
            "Total number of candidate 2-itemsets: 171\n",
            "Size in GB: 0.000009\n",
            "Number of frequent 2-itemsets: 73\n",
            "Size in GB: 0.000002\n",
            "------------------------------\n",
            "\n",
            "Total number of candidate 3-itemsets: 180\n",
            "Size in GB: 0.000009\n",
            "Number of frequent 3-itemsets: 99\n",
            "Size in GB: 0.000005\n",
            "------------------------------\n",
            "\n",
            "\n",
            "------------------------------\n",
            "Function apriori executed in 1.5919721126556396 seconds\n",
            "------------------------------\n",
            "\n",
            "Total number of candidate 1-itemsets: 6439\n",
            "Size in GB: 0.000295\n",
            "Number of frequent 1-itemsets: 19\n",
            "Size in GB: 0.000001\n",
            "------------------------------\n",
            "\n",
            "Total number of candidate 2-itemsets: 171\n",
            "Size in GB: 0.000009\n",
            "Number of frequent 2-itemsets: 73\n",
            "Size in GB: 0.000002\n",
            "------------------------------\n",
            "\n",
            "Total number of candidate 3-itemsets: 180\n",
            "Size in GB: 0.000009\n",
            "Number of frequent 3-itemsets: 99\n",
            "Size in GB: 0.000005\n",
            "------------------------------\n",
            "\n",
            "\n",
            "------------------------------\n",
            "Function pcy executed in 2.2582833766937256 seconds\n",
            "------------------------------\n",
            "\n",
            "Total number of candidate 1-itemsets: 3994\n",
            "Size in GB: 0.000148\n",
            "Number of frequent 1-itemsets: 26\n",
            "Size in GB: 0.000001\n",
            "------------------------------\n",
            "\n",
            "Total number of candidate 2-itemsets: 325\n",
            "Size in GB: 0.000009\n",
            "Number of frequent 2-itemsets: 140\n",
            "Size in GB: 0.000005\n",
            "------------------------------\n",
            "\n",
            "Total number of candidate 3-itemsets: 474\n",
            "Size in GB: 0.000019\n",
            "Number of frequent 3-itemsets: 254\n",
            "Size in GB: 0.000009\n",
            "------------------------------\n",
            "\n",
            "\n",
            "------------------------------\n",
            "Function apriori executed in 0.9087390899658203 seconds\n",
            "------------------------------\n",
            "\n",
            "Total number of candidate 1-itemsets: 7814\n",
            "Size in GB: 0.000295\n",
            "Number of frequent 1-itemsets: 191\n",
            "Size in GB: 0.000009\n",
            "------------------------------\n",
            "\n",
            "\n",
            "------------------------------\n",
            "Function toivonen executed in 10.289072513580322 seconds\n",
            "------------------------------\n",
            "\n",
            "\n",
            "------------------------------\n",
            "Function apyori.apriori executed in 0.038794517517089844 seconds\n",
            "------------------------------\n",
            "\n",
            "------------------------------TEST PASSED------------------------------\n",
            "------------------------------TEST PASSED------------------------------\n",
            "------------------------------TEST PASSED------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlgMk0-1sZVi"
      },
      "source": [
        "Experiment: \n",
        "\n",
        "*   build a dataset with english newspapers (baskets)\n",
        "*   consider 3 different orders of magnitude (10^3,10^5,10^6) for the number of baskets to analyze\n",
        "*   Evaluate performance of the 3 algorithms"
      ],
      "id": "nlgMk0-1sZVi"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86af2QLusWUd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a4254a8-4763-4c01-8113-eeb13eca078d"
      },
      "source": [
        "create_txt_dataset(['English'],1000,remove_stopwords=True,max_items_per_basket=15)\n",
        "create_txt_dataset(['English'],100000,remove_stopwords=True,max_items_per_basket=15)\n",
        "create_txt_dataset(['English'],1000000,remove_stopwords=True,max_items_per_basket=15)"
      ],
      "id": "86af2QLusWUd",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset english999.txt was created with 999 baskets with 11 items on average\n",
            "------------------------------\n",
            "Dataset english99944.txt was created with 99944 baskets with 12 items on average\n",
            "------------------------------\n",
            "Dataset english999318.txt was created with 999318 baskets with 11 items on average\n",
            "------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fvqYt_987RXv",
        "outputId": "b3a1465d-b207-4929-ec0a-1bccc112d40f"
      },
      "source": [
        "basket_files=['english999.txt','english99944.txt','english999318.txt']\n",
        "\n",
        "for basket_file in basket_files:\n",
        "\n",
        "  bf=[i for i in iter_baskets_from_txt(basket_file)]\n",
        "  basket_count=len(bf)\n",
        "  s_thresh=basket_count//100\n",
        "\n",
        "  apriori_res=apriori(iter_baskets_from_txt(basket_file),s=s_thresh,max_k=2)\n",
        "  pcy_res=pcy(iter_baskets_from_txt(basket_file),s=s_thresh,bm_size=64,max_k=2)\n",
        "  toivonen_res=toivonen(iter_baskets_from_txt(basket_file),s=s_thresh,p=0.6,scaling=0.75,max_k=2)\n",
        "\n",
        "  start=time.time()\n",
        "  apyori_res=list(ap.apriori(bf,min_support=s_thresh/basket_count,max_length=2))\n",
        "  stop=time.time()\n",
        "  print('\\n'+'-'*30)\n",
        "  print('Function apyori.apriori executed in {0} seconds'.format(stop-start))\n",
        "  print('-'*30+'\\n')\n",
        "\n",
        "  print('\\n'+'*'*30)\n",
        "  print('Test on basket file {0}'.format(basket_file))\n",
        "  print('*'*30+'\\n')\n",
        "\n",
        "  print('Apriori results')\n",
        "  test(apyori_res,apriori_res,basket_count)\n",
        "  print('PCY results')\n",
        "  test(apyori_res,pcy_res,basket_count)\n",
        "  print('Toivonen results')\n",
        "  if toivonen_res is not None:\n",
        "    test(apyori_res,toivonen_res,basket_count)\n",
        "  else:\n",
        "    print('Toivonen didn\\'t find any frequent itemset')"
      ],
      "id": "fvqYt_987RXv",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of candidate 1-itemsets: 5615\n",
            "Size in GB: 0.000295\n",
            "Number of frequent 1-itemsets: 172\n",
            "Size in GB: 0.000009\n",
            "------------------------------\n",
            "\n",
            "Total number of candidate 2-itemsets: 3191\n",
            "Size in GB: 0.000148\n",
            "Number of frequent 2-itemsets: 13\n",
            "Size in GB: 0.000001\n",
            "------------------------------\n",
            "\n",
            "\n",
            "------------------------------\n",
            "Function apriori executed in 0.19497346878051758 seconds\n",
            "------------------------------\n",
            "\n",
            "Total number of candidate 1-itemsets: 5615\n",
            "Size in GB: 0.000295\n",
            "Number of frequent 1-itemsets: 172\n",
            "Size in GB: 0.000009\n",
            "------------------------------\n",
            "\n",
            "Total number of candidate 2-itemsets: 3191\n",
            "Size in GB: 0.000148\n",
            "Number of frequent 2-itemsets: 13\n",
            "Size in GB: 0.000001\n",
            "------------------------------\n",
            "\n",
            "\n",
            "------------------------------\n",
            "Function pcy executed in 0.28920841217041016 seconds\n",
            "------------------------------\n",
            "\n",
            "Total number of candidate 1-itemsets: 4146\n",
            "Size in GB: 0.000148\n",
            "Number of frequent 1-itemsets: 385\n",
            "Size in GB: 0.000019\n",
            "------------------------------\n",
            "\n",
            "Total number of candidate 2-itemsets: 5690\n",
            "Size in GB: 0.000295\n",
            "Number of frequent 2-itemsets: 45\n",
            "Size in GB: 0.000002\n",
            "------------------------------\n",
            "\n",
            "\n",
            "------------------------------\n",
            "Function apriori executed in 0.1403336524963379 seconds\n",
            "------------------------------\n",
            "\n",
            "Total number of candidate 1-itemsets: 13168\n",
            "Size in GB: 0.000590\n",
            "Number of frequent 1-itemsets: 185\n",
            "Size in GB: 0.000009\n",
            "------------------------------\n",
            "\n",
            "\n",
            "------------------------------\n",
            "Function toivonen executed in 0.7104532718658447 seconds\n",
            "------------------------------\n",
            "\n",
            "\n",
            "------------------------------\n",
            "Function apyori.apriori executed in 0.13147878646850586 seconds\n",
            "------------------------------\n",
            "\n",
            "\n",
            "******************************\n",
            "Test on basket file english999.txt\n",
            "******************************\n",
            "\n",
            "Apriori results\n",
            "------------------------------TEST PASSED------------------------------\n",
            "PCY results\n",
            "------------------------------TEST PASSED------------------------------\n",
            "Toivonen results\n",
            "Toivonen didn't find any frequent itemset\n",
            "Total number of candidate 1-itemsets: 67297\n",
            "Size in GB: 0.002622\n",
            "Number of frequent 1-itemsets: 107\n",
            "Size in GB: 0.000005\n",
            "------------------------------\n",
            "\n",
            "Total number of candidate 2-itemsets: 5665\n",
            "Size in GB: 0.000295\n",
            "Number of frequent 2-itemsets: 3\n",
            "Size in GB: 0.000000\n",
            "------------------------------\n",
            "\n",
            "\n",
            "------------------------------\n",
            "Function apriori executed in 19.294224977493286 seconds\n",
            "------------------------------\n",
            "\n",
            "Total number of candidate 1-itemsets: 67297\n",
            "Size in GB: 0.002622\n",
            "Number of frequent 1-itemsets: 107\n",
            "Size in GB: 0.000005\n",
            "------------------------------\n",
            "\n",
            "Total number of candidate 2-itemsets: 5665\n",
            "Size in GB: 0.000295\n",
            "Number of frequent 2-itemsets: 3\n",
            "Size in GB: 0.000000\n",
            "------------------------------\n",
            "\n",
            "\n",
            "------------------------------\n",
            "Function pcy executed in 27.491958379745483 seconds\n",
            "------------------------------\n",
            "\n",
            "Total number of candidate 1-itemsets: 53271\n",
            "Size in GB: 0.002622\n",
            "Number of frequent 1-itemsets: 186\n",
            "Size in GB: 0.000009\n",
            "------------------------------\n",
            "\n",
            "Total number of candidate 2-itemsets: 16957\n",
            "Size in GB: 0.000590\n",
            "Number of frequent 2-itemsets: 12\n",
            "Size in GB: 0.000001\n",
            "------------------------------\n",
            "\n",
            "\n",
            "------------------------------\n",
            "Function apriori executed in 11.425679683685303 seconds\n",
            "------------------------------\n",
            "\n",
            "Total number of candidate 1-itemsets: 84439\n",
            "Size in GB: 0.002622\n",
            "Number of frequent 1-itemsets: 110\n",
            "Size in GB: 0.000005\n",
            "------------------------------\n",
            "\n",
            "\n",
            "------------------------------\n",
            "Function toivonen executed in 18.24434518814087 seconds\n",
            "------------------------------\n",
            "\n",
            "\n",
            "------------------------------\n",
            "Function apyori.apriori executed in 1.84611177444458 seconds\n",
            "------------------------------\n",
            "\n",
            "\n",
            "******************************\n",
            "Test on basket file english99944.txt\n",
            "******************************\n",
            "\n",
            "Apriori results\n",
            "------------------------------TEST PASSED------------------------------\n",
            "PCY results\n",
            "------------------------------TEST PASSED------------------------------\n",
            "Toivonen results\n",
            "------------------------------TEST PASSED------------------------------\n",
            "Total number of candidate 1-itemsets: 175227\n",
            "Size in GB: 0.010486\n",
            "Number of frequent 1-itemsets: 108\n",
            "Size in GB: 0.000005\n",
            "------------------------------\n",
            "\n",
            "Total number of candidate 2-itemsets: 5778\n",
            "Size in GB: 0.000295\n",
            "Number of frequent 2-itemsets: 3\n",
            "Size in GB: 0.000000\n",
            "------------------------------\n",
            "\n",
            "\n",
            "------------------------------\n",
            "Function apriori executed in 195.52580070495605 seconds\n",
            "------------------------------\n",
            "\n",
            "Total number of candidate 1-itemsets: 175227\n",
            "Size in GB: 0.010486\n",
            "Number of frequent 1-itemsets: 108\n",
            "Size in GB: 0.000005\n",
            "------------------------------\n",
            "\n",
            "Total number of candidate 2-itemsets: 5778\n",
            "Size in GB: 0.000295\n",
            "Number of frequent 2-itemsets: 3\n",
            "Size in GB: 0.000000\n",
            "------------------------------\n",
            "\n",
            "\n",
            "------------------------------\n",
            "Function pcy executed in 276.80315041542053 seconds\n",
            "------------------------------\n",
            "\n",
            "Total number of candidate 1-itemsets: 142757\n",
            "Size in GB: 0.005243\n",
            "Number of frequent 1-itemsets: 184\n",
            "Size in GB: 0.000009\n",
            "------------------------------\n",
            "\n",
            "Total number of candidate 2-itemsets: 16836\n",
            "Size in GB: 0.000590\n",
            "Number of frequent 2-itemsets: 12\n",
            "Size in GB: 0.000001\n",
            "------------------------------\n",
            "\n",
            "\n",
            "------------------------------\n",
            "Function apriori executed in 113.56370496749878 seconds\n",
            "------------------------------\n",
            "\n",
            "Total number of candidate 1-itemsets: 192063\n",
            "Size in GB: 0.010486\n",
            "Number of frequent 1-itemsets: 111\n",
            "Size in GB: 0.000005\n",
            "------------------------------\n",
            "\n",
            "\n",
            "------------------------------\n",
            "Function toivonen executed in 186.0554940700531 seconds\n",
            "------------------------------\n",
            "\n",
            "\n",
            "------------------------------\n",
            "Function apyori.apriori executed in 17.825950384140015 seconds\n",
            "------------------------------\n",
            "\n",
            "\n",
            "******************************\n",
            "Test on basket file english999318.txt\n",
            "******************************\n",
            "\n",
            "Apriori results\n",
            "------------------------------TEST PASSED------------------------------\n",
            "PCY results\n",
            "------------------------------TEST PASSED------------------------------\n",
            "Toivonen results\n",
            "------------------------------TEST PASSED------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}